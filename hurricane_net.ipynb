{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Copy of hurricane-net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hammad93/hurricane-net/blob/master/hurricane_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibY3ir9MTEao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "759aeff8-91da-4b69-eeed-28a2072ea7a6"
      },
      "source": [
        "!rm -rf sample_data\n",
        "!git init\n",
        "!git remote add origin https://github.com/hammad93/hurricane-net.git\n",
        "!git pull origin master"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n",
            "fatal: remote origin already exists.\n",
            "From https://github.com/hammad93/hurricane-net\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa7Z-ReQTA5t",
        "colab_type": "text"
      },
      "source": [
        "# hurricane-net\n",
        "Hammad Usmani\n",
        "### A machine learning algorithm to forecast the intensity and trajectory of Atlantic tropical storms\n",
        "[https://github.com/hammad93/hurricane-net](https://github.com/hammad93/hurricane-net)\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "1. [Background](#Background)\n",
        "2. [Problem](#Problem)\n",
        "3. [Datasets](#Datasets)\n",
        "4. [Workflow Diagram](#Workflow)\n",
        "5. [Data Extraction](#Extract)\n",
        "6. [Data Transformation](#Transform)\n",
        "7. [Data Loading](#Load)\n",
        "8. [Feature Engineering](#FeatureEngineering)\n",
        "9. [Model Architecture](#ModelArchitecture)\n",
        "11. [Model Selection](#ModelSelection)\n",
        "12. [Paramater Optimization](#Optimization)\n",
        "13. [Model Evaluation & Benchmarks](#Benchmarks)\n",
        "14. [Visualizations](#Visualizations)\n",
        "\n",
        "![Hurricane Maria 2017](img/hurricane-maria.png \"Hurricane Maria. Source: NOAA\")\n",
        "\n",
        "## Background<a id=\"Background\"></a>\n",
        "\n",
        "The National Hurricane Center (NHC) and National Oceanic and Atmospheric Administration (NOAA) provide predictions for storms trajectories, intensity, and size. They create these predictions based on models that can be classified into 3 groups: dynamical, statistical, and ensemble [1]. The most accurate models are based on computational fluid dynamics and achieve more precision than their statistical and ensemble counterparts [1][4]. The current statistical models (OCD5) are based on multiple regression methods that can explain a significant amount of variance [1]. In this project, we research and implement the domain of machine learning and deep learning into predictive hurricane models for both trajectory and intensity and evaluate them against the NHC standards. \n",
        "Previous research into machine learning to forecast tropical Atlantic storms include a sparse recurrent neural network (Kordmahalleh, Sefidmazgi, & Homaifar, 2016) and an artificial neural network (Jung & Das, 2013); both achieved favorable results. The hurricane models created can be utilized to develop more precise emergency planning. There is a necessity for more accurate and timely models that can help reduce the amount of loss caused by hurricanes. \n",
        "\n",
        "## Problem<a id=\"Problem\"></a>\n",
        "\n",
        "The NOAA and NHC have several different classifications for Atlantic hurricane models that describe feature prediction and model architecture. The 3 main classifications for hurricane model architecture include dynamical, statistical, and ensemble. Classifications also include relative compute time required to create an output grouped as either early or late and forecast parameters such as trajectory, intensity, and wind radii. The most accurate models are late models that take upwards of 6 hours to produce an output whereas models that can produce an output in seconds to minutes are called early. Early models tend to be statistical which include the baseline model for trajectory named CLIPER5 Climatology and Persistence (CLP5) utilizing multivariate regression. The performance for these methods can be augmented by incorporating more advanced statistical methods from deep learning such as recurrent neural networks. Kordmahalleh et al., 2016 created a sparse recurrent neural network augmented by a genetic algorithm but there are factors requiring improvement. The training set utilized an older version of the NHC Hurricane Database format known as HURDAT while a new format has been released called HURDAT2 with additional information on wind radii. Kordmahalleh et al., 2016 also utilized benchmarks different from the standard applied within the NHC. Other than improving their methodology, we can expand the scope by creating separate models for both intensity and trajectory. These models can be used to predict the trajectory and intensity for future Atlantic storms.\n",
        "\n",
        "## Datasets<a id=\"Datasets\"></a>\n",
        "\n",
        "The following datasets and inputs including their sources will be used to create machine learning models:\n",
        "- NHC Hurricane Database (HURDAT2)\n",
        "    - http://www.nhc.noaa.gov/data/#hurdat\n",
        "    - https://www.kaggle.com/noaa/hurricane-database\n",
        "- NHC Forecast Error Database\n",
        "    - http://www.nhc.noaa.gov/verification/verify7.shtml\n",
        "- NHC GIS\n",
        "    - http://www.nhc.noaa.gov/gis/\n",
        "\n",
        "*In the future, the IBTrACS database will be used to extend the hurricane-ai to additional regions.*\n",
        "\n",
        "The NHC HURDAT2 database contains the tracking information for Atlantic tropical and subtropical cyclones which includes hurricanes and tropical storms from 1851 to 2016. The most updated version of the dataset is included on the noaa.gov site and includes 2 additional years of cyclone data compared to the data set available on Kaggle and is potentially more descriptive. To match the inputs of the baseline model used by the NHC, we are calculating the forward motion of the storm by applying a vector based on previous and current geographical location.\n",
        "\n",
        "*Table 1. This table contains the tentative features as input to the model*\n",
        "\n",
        "| **Name**         | **Data Type** | **Description**                                                     |\n",
        "|------------------|---------------|---------------------------------------------------------------------|\n",
        "| Time             | Date Time     | The date and time of the measurement.                               |\n",
        "| Latitude         | Float         | The geographical latitude of the storm eye to 1 decimal precision.  |\n",
        "| Longitude        | Float         | The geographical longitude of the storm eye to 1 decimal precision. |\n",
        "| Maximum Winds    | Integer       | The maximum sustained winds within the storm.                       |\n",
        "| Minimum Pressure | Integer       | The minimum barometric pressure within the storm.                   |\n",
        "| Forward Motion   | String        | Calculated vector of motion based on location in time series.       |\n",
        "\n",
        "The Forecast Error Database contains information on the accuracy of predicted models from the NHC. The two model forecast errors available are labeled OFCL and BCD5. The OFCL is the official NHC forecast and the BCD5 is the real track available. This data set can be used to benchmark and evaluate the deep learning model. \n",
        "The NOAA and NHC also hosts a geographical information system (GIS) that contains raw and processed data on hurricanes. The server hosting the GIS is publicly accessible and can be used to evaluate our model by comparing the 2017 Atlantic tropical season. The preliminary best tracks can be found here before they are finalized and available in the HURDAT2 data set. With the GIS, we can construct a final evaluation data set.\n",
        "\n",
        "*Diagram 1. This graphic describes the workflow for the deep learning models*.<a id=\"Workflow\"></a>\n",
        "![Data Pipeline](img/Deep Learning Workflow.png \"hurricane-net Data Pipeline\")\n",
        "\n",
        "## Extract Data<a id=\"Extract\"></a>\n",
        "\n",
        "*The following code uses the hurdat2 and models modules created to provide a class interface for the HURDAT2 and error forecast database located in the data and models folder. *\n",
        "\n",
        "We will begin our steps to perform extraction, transformation, and loading of our data for analysis or broadly known as ETL. Although we're dividing these steps into disctinct procedures, they are often more fluid and often have overlaps. The extraction phase consists of collecting and parsing the HURDAT2 and error forecast databases for analysis and benchmarking. The HURDAT2 database is our core foundation for creating the deep learning model. We store the database in its raw .txt format but it can be directly linked to the database hosted by the NHC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F02MSMmiTA5x",
        "colab_type": "code",
        "outputId": "be04a43c-a37f-4dc7-ebd7-d8692e08009e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# Import various libraries throughout the software\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import datetime\n",
        "import dateutil\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Import from hurdat2 class in data folder and models class from hurricane-models folder\n",
        "from data.hurdat2 import hurdat2\n",
        "from errors.models import models\n",
        "\n",
        "# Initialize Dataframe for hurricanes and error database\n",
        "dataset = hurdat2(\"data/hurdat2.txt\") # Note that this data includes up to and including 2016\n",
        "errors = models(\"errors/1989-present_OFCL_v_BCD5_ind_ATL_TI_errors.txt\")\n",
        "\n",
        "# Show the first 5 records from Hurricane Katrina 2005 (AL122005)\n",
        "dataset.hurricanes.query('storm_id == \"AL122005\"').head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>storm_id</th>\n",
              "      <th>storm_name</th>\n",
              "      <th>entry_time</th>\n",
              "      <th>entry_id</th>\n",
              "      <th>entry_status</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>max_wind</th>\n",
              "      <th>min_pressure</th>\n",
              "      <th>34kt_ne</th>\n",
              "      <th>34kt_se</th>\n",
              "      <th>34kt_sw</th>\n",
              "      <th>34kt_nw</th>\n",
              "      <th>50kt_ne</th>\n",
              "      <th>50kt_se</th>\n",
              "      <th>50kt_sw</th>\n",
              "      <th>50kt_nw</th>\n",
              "      <th>64kt_ne</th>\n",
              "      <th>64kt_se</th>\n",
              "      <th>64kt_sw</th>\n",
              "      <th>64kt_nw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14663</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-23 18:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TD</td>\n",
              "      <td>23.1N</td>\n",
              "      <td>75.1W</td>\n",
              "      <td>30</td>\n",
              "      <td>1008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14664</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-24 00:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TD</td>\n",
              "      <td>23.4N</td>\n",
              "      <td>75.7W</td>\n",
              "      <td>30</td>\n",
              "      <td>1007</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14665</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-24 06:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TD</td>\n",
              "      <td>23.8N</td>\n",
              "      <td>76.2W</td>\n",
              "      <td>30</td>\n",
              "      <td>1007</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14666</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-24 12:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TS</td>\n",
              "      <td>24.5N</td>\n",
              "      <td>76.5W</td>\n",
              "      <td>35</td>\n",
              "      <td>1006</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14667</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-24 18:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TS</td>\n",
              "      <td>25.4N</td>\n",
              "      <td>76.9W</td>\n",
              "      <td>40</td>\n",
              "      <td>1003</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       storm_id storm_name          entry_time  ... 64kt_se 64kt_sw 64kt_nw\n",
              "14663  AL122005    KATRINA 2005-08-23 18:00:00  ...       0       0       0\n",
              "14664  AL122005    KATRINA 2005-08-24 00:00:00  ...       0       0       0\n",
              "14665  AL122005    KATRINA 2005-08-24 06:00:00  ...       0       0       0\n",
              "14666  AL122005    KATRINA 2005-08-24 12:00:00  ...       0       0       0\n",
              "14667  AL122005    KATRINA 2005-08-24 18:00:00  ...       0       0       0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk5uZFS0TA51",
        "colab_type": "code",
        "outputId": "e683ab05-9391-497a-84e6-31c97ed5c5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "# Show the first 3 OFCL hurricane model errors for Hurricane Katrina 2005 on 28-08-2005/18:00:00\n",
        "pprint(errors.models['OFCL'].storm['AL122005'][datetime.datetime(2005, 8, 28, 18, 0)], indent = 8)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{       'intensity_forecast': {       datetime.datetime(2005, 8, 28, 18, 0): 0.0,\n",
            "                                      datetime.datetime(2005, 8, 29, 6, 0): 20.9,\n",
            "                                      datetime.datetime(2005, 8, 29, 18, 0): 93.6,\n",
            "                                      datetime.datetime(2005, 8, 30, 6, 0): 170.2,\n",
            "                                      datetime.datetime(2005, 8, 30, 18, 0): 248.6,\n",
            "                                      datetime.datetime(2005, 8, 31, 18, 0): None,\n",
            "                                      datetime.datetime(2005, 9, 1, 18, 0): None,\n",
            "                                      datetime.datetime(2005, 9, 2, 18, 0): None,\n",
            "                                      datetime.datetime(2005, 9, 3, 18, 0): None,\n",
            "                                      datetime.datetime(2005, 9, 4, 18, 0): None},\n",
            "        'lat': 26.3,\n",
            "        'long': 88.6,\n",
            "        'sample_sizes': {       'F012': 0.33,\n",
            "                                'F024': 0.33,\n",
            "                                'F036': 0.33,\n",
            "                                'F048': 0.33,\n",
            "                                'F072': 0.0,\n",
            "                                'F096': 0.0,\n",
            "                                'F120': 0.0,\n",
            "                                'F144': 0.0,\n",
            "                                'F168': 0.0},\n",
            "        'track_forecast': {       datetime.datetime(2005, 8, 28, 18, 0): 0.0,\n",
            "                                  datetime.datetime(2005, 8, 29, 6, 0): 28.0,\n",
            "                                  datetime.datetime(2005, 8, 29, 18, 0): 32.0,\n",
            "                                  datetime.datetime(2005, 8, 30, 6, 0): 17.0,\n",
            "                                  datetime.datetime(2005, 8, 30, 18, 0): 6.0,\n",
            "                                  datetime.datetime(2005, 8, 31, 18, 0): None,\n",
            "                                  datetime.datetime(2005, 9, 1, 18, 0): None,\n",
            "                                  datetime.datetime(2005, 9, 2, 18, 0): None,\n",
            "                                  datetime.datetime(2005, 9, 3, 18, 0): None,\n",
            "                                  datetime.datetime(2005, 9, 4, 18, 0): None},\n",
            "        'wind_speed': 150}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vtb4-c_TA55",
        "colab_type": "text"
      },
      "source": [
        "## Transform Data<a id=\"Transform\"></a>\n",
        "\n",
        "The following code will tranform the hurricane best path data into objects that can be better manipulated for processing. to match between datasets, we will also create a `storm_id` dictionary to store storm names matched with ID's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TXygtaGkTA56",
        "colab_type": "code",
        "outputId": "cd88ef38-a689-4796-8211-feac99a18370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Create hurricane class\n",
        "class hurricane(object) : \n",
        "    def __init__(self, name, id) :\n",
        "        # Set instance variables\n",
        "        self.name = name\n",
        "        self.id = id\n",
        "        self.entries = dict()\n",
        "        self.models = dict()\n",
        "        \n",
        "        return\n",
        "    # Add hurricane track entry based on standard HURDAT2 format\n",
        "    def add_entry(self, array) :\n",
        "        entry = {\n",
        "            array[0] : { # dateteime of entry\n",
        "                'entry_time' : array[0], \n",
        "                'entry_id' : array[1],\n",
        "                'entry_status' : array[2],\n",
        "                'lat' : float(array[3][:-1]), # Convert to number from format '#.#N'\n",
        "                'long' : float(array[4][:-1]), # Convert to number from format '#.#W'\n",
        "                'max_wind' : float(array[5]),\n",
        "                'min_pressure' : None if array[6] is None else float(array[6]), # Early records are -999 or None\n",
        "                'wind_radii' :  array[7:], # Array based on HURDAT2 format\n",
        "            }\n",
        "        }\n",
        "        self.entries.update(entry)\n",
        "        \n",
        "        return\n",
        "    # Add hurricane model errors\n",
        "    def add_model(self, name, model) :\n",
        "        self.models[name] = model\n",
        "        \n",
        "        return\n",
        "# Storm ID Key for matching between datasets\n",
        "storm_ids = dict()\n",
        "\n",
        "# Parse in hurricanes\n",
        "hurricanes = dict()\n",
        "print(\"Transforming HURDAT2 into objects . . .\")\n",
        "for index, entry in dataset.hurricanes.iterrows() :\n",
        "    print(\"Transforming {}/{} entries from HURDAT2\".format(index + 1, len(dataset.hurricanes)), end = \"\\r\")\n",
        "    # New hurricane\n",
        "    if entry['storm_id'] not in hurricanes :\n",
        "        hurricanes[entry['storm_id']] = hurricane(entry['storm_name'], entry['storm_id'])\n",
        "        storm_ids[entry['storm_id']] = entry['storm_name']\n",
        "    # Add entry to hurricane\n",
        "    hurricanes[entry['storm_id']].add_entry(entry[2:])\n",
        "print(\"\\nDone!\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transforming HURDAT2 into objects . . .\n",
            "Transforming 20291/20291 entries from HURDAT2\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC2DpnqwTA58",
        "colab_type": "text"
      },
      "source": [
        "## Load Data<a id=\"Load\"></a>\n",
        "\n",
        "The following will finalize our preliminary data preparation by loading some of the errors into each hurricane object. Note that models start from the year 1970 and any hurricane before that has no previous model data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKKXK1yXTA59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all available model errors\n",
        "models = errors.models.keys()\n",
        "# Load model errors into hurricanes\n",
        "for id in storm_ids :\n",
        "    for model in models :\n",
        "        # Skip if this hurricane does not have the model\n",
        "        if id not in errors.models[model].storm :\n",
        "            continue\n",
        "        hurricanes[id].add_model(model, errors.models[model].storm[id])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7baivH-TA6D",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering & Data Augmentation<a id=\"FeatureEngineering\"></a>\n",
        "\n",
        "The following section will extract the relevant features and engineer each data point so that we can fit it into the model. Because the type of inputs are important, the features will be transformed based on the model architecture. This will also include data augmentation methods. The higher level architecture will be a deep learning recurrent neural network with LSTM and time distributed layers.\n",
        "\n",
        "The current statistical baseline model using multivariate regression uses multiple predictors as input. According to Knaff 2013, the following predictors were calculated for their intensity model that were not included in the HURDAT2 database. These features can be calculated from the data loaded into our current object model.\n",
        "\n",
        "1. Date Information\n",
        "2. Zonal Speed Of The Storm (U) (kt)\n",
        "3. Meridional Speed Of The Storm (V) (kt)\n",
        "4. 12-h Change In Intensity (DVMX) (kt)\n",
        "\n",
        "The shape on the input to the LSTM will be in a 3D array with the format [samples, timestamps, features]. We will intitially begin with 1 time step and evaluate more can benefit our model. The output requires a 5 day forecast and observations without track data 5 days in the future will not be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GsKxUTh7TA6E",
        "colab_type": "code",
        "outputId": "b40aa23e-7de7-48c0-d409-ae678e848406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "def feature_extraction(timestep, previous) :\n",
        "    '''\n",
        "    PURPOSE: Calculate the features for a machine learning model within the context of hurricane-net\n",
        "    METHOD: Use the predictors and the calculation methodology defined in Knaff 2013\n",
        "    INPUT:  timestep - current dictionary of features in the hurricane object format\n",
        "            previous - previous timestep dictionary of features in the hurricane object format\n",
        "    OUTPUT: Dictionary of features\n",
        "    '''\n",
        "    features = {\n",
        "        'lat' : timestep['lat'],\n",
        "        'long' : timestep['long'],\n",
        "        'max_wind' : timestep['max_wind'],\n",
        "        'delta_wind' : (timestep['max_wind'] - previous['max_wind']) / # Calculated from track (12h)\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 43200),\n",
        "        'min_pressure' : timestep['min_pressure'], \n",
        "        'zonal_speed' : (timestep['lat'] - previous['lat'])/ # Calculated from track (per hour)\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 3600),\n",
        "        'meridonal_speed' : (timestep['long'] - previous['long'])/# Calculated from track (per hour)\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 3600),\n",
        "        'year' : timestep['entry_time'].year,\n",
        "        'month' : timestep['entry_time'].month,\n",
        "        'day' : timestep['entry_time'].day,\n",
        "        'hour' : timestep['entry_time'].hour,\n",
        "    }\n",
        "    return features\n",
        "    \n",
        "def storm_x_y(storm, timesteps = 1, lag = 24) :\n",
        "    '''\n",
        "    PURPOSE: Create independent and dependent samples for a machine learning model based on the timesteps\n",
        "    METHOD: Use the HURDAT2 database and a hurricane object as defined in hurricane-net for feature extraction\n",
        "    INPUT:  storm - hurricane object\n",
        "            timesteps - (default = 1) number of timesteps to calculate\n",
        "            include_none - (default = False) Boolean for including None in test data. Imputing function unavailable.\n",
        "            lag - (default = 24) lag in hours for the dependent variables up to 5 days\n",
        "    OUTPUT: Dictionary with independent (x) and dependent (y) values.\n",
        "    '''\n",
        "    x = []\n",
        "    # Create testing data structure with a dictionary\n",
        "    times = [time * lag for time in range(1, (120 // lag) + 1)] # Begin at lag hours with lag increments up to 120h inclusive\n",
        "    y = dict([(time,[]) for time in times])\n",
        "    \n",
        "    # Sort by entry time\n",
        "    entries = [entry[1] for entry in sorted(storm.entries.items())]\n",
        "    \n",
        "    for index in range(len(entries)) :\n",
        "        if index < timesteps : # Flag for insufficient initial time steps\n",
        "            continue\n",
        "\n",
        "        # If we're not including None values, check to see if there will be any\n",
        "        if None in [storm.entries.get(entries[index]['entry_time'] +\n",
        "                                         datetime.timedelta(hours = future)) for future in times] : break\n",
        "            \n",
        "        # Calculate time steps and their features for independent values\n",
        "        sample = []\n",
        "        for step in range(timesteps) :\n",
        "            # Training sample\n",
        "            timestep = entries[index - step]\n",
        "            previous = entries[index - step - 1]\n",
        "            sample.append([timestep['entry_time']] + [[feature_extraction(timestep, previous)]])\n",
        "        x.append(sample) # Add our constructed sample\n",
        "        \n",
        "        # Calculate time steps and their features for dependent values\n",
        "        for future in times :\n",
        "            timestep = storm.entries.get(entries[index]['entry_time'] + datetime.timedelta(hours = future))\n",
        "            previous = storm.entries.get(entries[index]['entry_time'] + datetime.timedelta(hours = future - lag))\n",
        "            \n",
        "            if timestep and previous: \n",
        "                y[future].append(feature_extraction(timestep, previous))\n",
        "            else :\n",
        "                y[future].append(None)\n",
        "    \n",
        "    # Return output, if there is no output, return None.\n",
        "    if len(x) is 0 :\n",
        "        return None\n",
        "    else:\n",
        "        return {'x': x, 'y': y}\n",
        "def shape(hurricanes, timesteps, remove_missing = True) :\n",
        "    '''\n",
        "    PURPOSE: Shape our data for input into machine learning models\n",
        "    METHOD: Use a numpy array to shape into (samples, timesteps, features)\n",
        "    INPUT:  hurricanes - dictionary of hurricane objects\n",
        "            timesteps - number of timesteps for the shape\n",
        "            remove_missing - boolean indicating whether the algorithm will disregard missing values\n",
        "    OUTPUT: numpy array of shape (samples, timesteps, 11) where 11 is the number of predictors in a hurricane object\n",
        "    '''\n",
        "    x = []\n",
        "    y = []\n",
        "    lag = 24 # lag time in hours\n",
        "    precision = np.float64 # defines the precision of our data type\n",
        "    times = [time * lag for time in range(1, (120 // lag) + 1)] # Begin at lag hours with lag increments up to 120h inclusive\n",
        "    count = 0\n",
        "    for hurricane in hurricanes.values() :\n",
        "        count += 1\n",
        "        result = storm_x_y(hurricane, timesteps, lag)\n",
        "        if result is None :\n",
        "            continue\n",
        "        # Extract only the values from the strom features using our specified precision\n",
        "        hurricane_x = np.array(\n",
        "            [[list(sample[1][0].values()) for sample in x] for x in result['x']],\n",
        "            dtype = precision)\n",
        "        hurricane_y = np.array(\n",
        "            [[list(result['y'][time][index].values()) for time in times] for index in range(len(result['y'][lag]))],\n",
        "            dtype = precision)\n",
        "        # Disregard if algorithm requires no missing values\n",
        "        if remove_missing :\n",
        "            if (len(np.where(np.isnan(hurricane_x))[0]) > 0) or (len(np.where(np.isnan(hurricane_y))[0]) > 0) :\n",
        "                continue\n",
        "        # Add to our results\n",
        "        x.extend(hurricane_x)\n",
        "        y.extend(hurricane_y)\n",
        "        print(\"Feature engineered {}/{} hurricanes for {} timestep(s)\".format(count, len(hurricanes), timesteps), end = \"\\r\")\n",
        "    print(\"\\nDone feature engineering hurricanes.\")\n",
        "    \n",
        "    return {'x': np.array(x), 'y': np.array(y)}\n",
        "def scaler(processed_data, hurricanes) :\n",
        "    '''\n",
        "    PURPOSE: Scale our data using the RobustScaler method from the sklearn library\n",
        "    METHOD: Generate data using 1 timesteps and then remove the NaN or None types to use the scaler methods\n",
        "    INPUT:  hurricanes - dictionary of hurricane objects\n",
        "            processed_data - dictionary of x and y values of data produced by shape() function with no missing values\n",
        "    OUTPUT: 1) Scaled processed_data using RobustScaler\n",
        "            2) RobustScaler object fit with appropriate data\n",
        "    '''\n",
        "    print(\"Scaling Data . . . (1 timestep for unqiue data)\")\n",
        "    # Create our scaler\n",
        "    unqiue_data = shape(hurricanes, timesteps = 1)\n",
        "    x = np.reshape(unqiue_data['x'], (unqiue_data['x'].shape[0], -1))\n",
        "    x = np.delete(x, np.where(np.isnan(x))[0], 0)\n",
        "    scaler = RobustScaler()\n",
        "    scaler.fit(x)\n",
        "    \n",
        "    # Scale our data\n",
        "    for index in range(len(processed_data['x'])) :\n",
        "        # Scale our x\n",
        "        processed_data['x'][index] = scaler.transform(processed_data['x'][index])\n",
        "        # Scale our y\n",
        "        processed_data['y'][index] = scaler.transform(processed_data['y'][index])\n",
        "    print(\"Done scaling.\")\n",
        "    return processed_data, scaler\n",
        "# Finalize and scale procesed data into a dictionary\n",
        "preprocessed_data = shape(hurricanes, timesteps = 5)\n",
        "processed_data, scaler = scaler(preprocessed_data, hurricanes)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature engineered 764/764 hurricanes for 5 timestep(s)\n",
            "Done feature engineering hurricanes.\n",
            "Scaling Data . . . (1 timestep for unqiue data)\n",
            "Feature engineered 764/764 hurricanes for 1 timestep(s)\n",
            "Done feature engineering hurricanes.\n",
            "Done scaling.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk1fyYBXTA6G",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture<a id=\"ModelArchitecture\"></a>\n",
        "\n",
        "Following feature engineering, we are now ready to input our data into a machine learning algorithm. The scope of this project will attempt a deep learning approach to forecasting Atlantic tropical cyclones. We will experiment with nunermous different architectures but we will focus around a Recurrent Neural Network utilizing LSTM cells.\n",
        "\n",
        "Notes:\n",
        "- We will use 500 epochs for wind intensity because the validation loss is not decresing\n",
        "- We will use 1,000 epochs for latitute and longitude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0ETjdPZJTA6H",
        "colab_type": "code",
        "outputId": "7047ee1a-08ac-4717-c9bd-9fab3e644d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import RepeatVector\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Create our cross validation data structure\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(processed_data['x'], processed_data['y'],\n",
        "                                                                    test_size = 0.2)\n",
        "\n",
        "# Train for wind intensity\n",
        "y_train_wind = np.array([[[features[2]] for features in y] for y in y_train], dtype = np.float64)\n",
        "y_test_wind = np.array([[[features[2]] for features in y] for y in y_test], dtype = np.float64)\n",
        "\n",
        "# Train for latitude and longitude location\n",
        "y_train_lat = np.array([[[features[0]] for features in y] for y in y_train], dtype = np.float64)\n",
        "y_test_lat = np.array([[[features[0]] for features in y] for y in y_test], dtype = np.float64)\n",
        "y_train_long = np.array([[[features[1]] for features in y] for y in y_train], dtype = np.float64)\n",
        "y_test_long = np.array([[[features[1]] for features in y] for y in y_test], dtype = np.float64)\n",
        "\n",
        "\n",
        "def bd_lstm_td(X_train, y_train, X_test, y_test, n_epochs = 500) :    \n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units = 512, return_sequences = True, dropout = 0.05),\n",
        "                            input_shape = (X_train.shape[1],X_train.shape[2])))\n",
        "    model.add(LSTM(units = 256, return_sequences = True, dropout = 0.05))\n",
        "    model.add(TimeDistributed(Dense(1)))\n",
        "    model.compile(loss='mse', optimizer='adadelta')\n",
        "    print(model.summary())\n",
        "    history = model.fit(X_train, y_train, batch_size = len(X_train), epochs = n_epochs,\n",
        "                        validation_data = (X_test, y_test))\n",
        "    return model, history\n",
        "\n",
        "def lstm_td(X_train, X_test, y_train, y_test) :\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units = 1024, input_shape = (5,8), return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(8)))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    print(model.summary())\n",
        "    model.fit(X_train, y_train, batch_size = len(X_train), epochs = 300)\n",
        "    \n",
        "    return model\n",
        "\n",
        "model_wind, model_wind_history = bd_lstm_td(X_train, y_train_wind, X_test, y_test_wind, n_epochs = 500)\n",
        "model_lat, model_lat_history = bd_lstm_td(X_train, y_train_lat, X_test, y_test_lat, n_epochs = 1000)\n",
        "model_long, model_long_history = bd_lstm_td(X_train, y_train_long, X_test, y_test_long, n_epochs = 1000)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_5 (Bidirection (None, 5, 1024)           2146304   \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 5, 256)            1311744   \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 5, 1)              257       \n",
            "=================================================================\n",
            "Total params: 3,458,305\n",
            "Trainable params: 3,458,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 3298 samples, validate on 825 samples\n",
            "Epoch 1/500\n",
            "3298/3298 [==============================] - 4s 1ms/step - loss: 0.8970 - val_loss: 0.6697\n",
            "Epoch 2/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.7094 - val_loss: 0.5812\n",
            "Epoch 3/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.6205 - val_loss: 0.5410\n",
            "Epoch 4/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.5816 - val_loss: 0.5210\n",
            "Epoch 5/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.5607 - val_loss: 0.5075\n",
            "Epoch 6/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.5433 - val_loss: 0.4966\n",
            "Epoch 7/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.5322 - val_loss: 0.4873\n",
            "Epoch 8/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.5200 - val_loss: 0.4785\n",
            "Epoch 9/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.5123 - val_loss: 0.4708\n",
            "Epoch 10/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.5047 - val_loss: 0.4638\n",
            "Epoch 11/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4967 - val_loss: 0.4574\n",
            "Epoch 12/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4895 - val_loss: 0.4511\n",
            "Epoch 13/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4822 - val_loss: 0.4459\n",
            "Epoch 14/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4769 - val_loss: 0.4410\n",
            "Epoch 15/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4717 - val_loss: 0.4369\n",
            "Epoch 16/500\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.4707 - val_loss: 0.4330\n",
            "Epoch 17/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4652 - val_loss: 0.4296\n",
            "Epoch 18/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4614 - val_loss: 0.4268\n",
            "Epoch 19/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4567 - val_loss: 0.4244\n",
            "Epoch 20/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4566 - val_loss: 0.4222\n",
            "Epoch 21/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4543 - val_loss: 0.4206\n",
            "Epoch 22/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4520 - val_loss: 0.4187\n",
            "Epoch 23/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4493 - val_loss: 0.4173\n",
            "Epoch 24/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4486 - val_loss: 0.4160\n",
            "Epoch 25/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4482 - val_loss: 0.4154\n",
            "Epoch 26/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4449 - val_loss: 0.4140\n",
            "Epoch 27/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4470 - val_loss: 0.4138\n",
            "Epoch 28/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4399 - val_loss: 0.4123\n",
            "Epoch 29/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4463 - val_loss: 0.4123\n",
            "Epoch 30/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4410 - val_loss: 0.4111\n",
            "Epoch 31/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4444 - val_loss: 0.4113\n",
            "Epoch 32/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4388 - val_loss: 0.4098\n",
            "Epoch 33/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4423 - val_loss: 0.4099\n",
            "Epoch 34/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4401 - val_loss: 0.4088\n",
            "Epoch 35/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4399 - val_loss: 0.4082\n",
            "Epoch 36/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4399 - val_loss: 0.4088\n",
            "Epoch 37/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4374 - val_loss: 0.4072\n",
            "Epoch 38/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4335 - val_loss: 0.4084\n",
            "Epoch 39/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4359 - val_loss: 0.4064\n",
            "Epoch 40/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4359 - val_loss: 0.4078\n",
            "Epoch 41/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4329 - val_loss: 0.4051\n",
            "Epoch 42/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4333 - val_loss: 0.4062\n",
            "Epoch 43/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4349 - val_loss: 0.4044\n",
            "Epoch 44/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4319 - val_loss: 0.4057\n",
            "Epoch 45/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4332 - val_loss: 0.4036\n",
            "Epoch 46/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4329 - val_loss: 0.4054\n",
            "Epoch 47/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4298 - val_loss: 0.4028\n",
            "Epoch 48/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4303 - val_loss: 0.4051\n",
            "Epoch 49/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4331 - val_loss: 0.4021\n",
            "Epoch 50/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4307 - val_loss: 0.4050\n",
            "Epoch 51/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4327 - val_loss: 0.4012\n",
            "Epoch 52/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4282 - val_loss: 0.4054\n",
            "Epoch 53/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4295 - val_loss: 0.4013\n",
            "Epoch 54/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4306 - val_loss: 0.4061\n",
            "Epoch 55/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4290 - val_loss: 0.4012\n",
            "Epoch 56/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4286 - val_loss: 0.4063\n",
            "Epoch 57/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4343 - val_loss: 0.4013\n",
            "Epoch 58/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4319 - val_loss: 0.4063\n",
            "Epoch 59/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4321 - val_loss: 0.4014\n",
            "Epoch 60/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4305 - val_loss: 0.4079\n",
            "Epoch 61/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4303 - val_loss: 0.4022\n",
            "Epoch 62/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4327 - val_loss: 0.4118\n",
            "Epoch 63/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4343 - val_loss: 0.4035\n",
            "Epoch 64/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4328 - val_loss: 0.4127\n",
            "Epoch 65/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4359 - val_loss: 0.4022\n",
            "Epoch 66/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4317 - val_loss: 0.4120\n",
            "Epoch 67/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4352 - val_loss: 0.4014\n",
            "Epoch 68/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4295 - val_loss: 0.4090\n",
            "Epoch 69/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4321 - val_loss: 0.4005\n",
            "Epoch 70/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4296 - val_loss: 0.4093\n",
            "Epoch 71/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4313 - val_loss: 0.4007\n",
            "Epoch 72/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4316 - val_loss: 0.4098\n",
            "Epoch 73/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4332 - val_loss: 0.3997\n",
            "Epoch 74/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4287 - val_loss: 0.4070\n",
            "Epoch 75/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4291 - val_loss: 0.3969\n",
            "Epoch 76/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4269 - val_loss: 0.4038\n",
            "Epoch 77/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4259 - val_loss: 0.3963\n",
            "Epoch 78/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4238 - val_loss: 0.4036\n",
            "Epoch 79/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4269 - val_loss: 0.3956\n",
            "Epoch 80/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4252 - val_loss: 0.4039\n",
            "Epoch 81/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4250 - val_loss: 0.3948\n",
            "Epoch 82/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4253 - val_loss: 0.4021\n",
            "Epoch 83/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4256 - val_loss: 0.3938\n",
            "Epoch 84/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4241 - val_loss: 0.4027\n",
            "Epoch 85/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4238 - val_loss: 0.3944\n",
            "Epoch 86/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4234 - val_loss: 0.4038\n",
            "Epoch 87/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4241 - val_loss: 0.3941\n",
            "Epoch 88/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4233 - val_loss: 0.4014\n",
            "Epoch 89/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4207 - val_loss: 0.3934\n",
            "Epoch 90/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4231 - val_loss: 0.4007\n",
            "Epoch 91/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4226 - val_loss: 0.3923\n",
            "Epoch 92/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4195 - val_loss: 0.4013\n",
            "Epoch 93/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4210 - val_loss: 0.3924\n",
            "Epoch 94/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4212 - val_loss: 0.4017\n",
            "Epoch 95/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4216 - val_loss: 0.3920\n",
            "Epoch 96/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4220 - val_loss: 0.4021\n",
            "Epoch 97/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4216 - val_loss: 0.3926\n",
            "Epoch 98/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4191 - val_loss: 0.4009\n",
            "Epoch 99/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4214 - val_loss: 0.3913\n",
            "Epoch 100/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4180 - val_loss: 0.4003\n",
            "Epoch 101/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4196 - val_loss: 0.3913\n",
            "Epoch 102/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4174 - val_loss: 0.4000\n",
            "Epoch 103/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4204 - val_loss: 0.3903\n",
            "Epoch 104/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4182 - val_loss: 0.3987\n",
            "Epoch 105/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4177 - val_loss: 0.3901\n",
            "Epoch 106/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4184 - val_loss: 0.3986\n",
            "Epoch 107/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.4200 - val_loss: 0.3896\n",
            "Epoch 108/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4169 - val_loss: 0.3987\n",
            "Epoch 109/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4179 - val_loss: 0.3889\n",
            "Epoch 110/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4170 - val_loss: 0.3982\n",
            "Epoch 111/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4167 - val_loss: 0.3887\n",
            "Epoch 112/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4162 - val_loss: 0.3983\n",
            "Epoch 113/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4155 - val_loss: 0.3873\n",
            "Epoch 114/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4163 - val_loss: 0.3961\n",
            "Epoch 115/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4138 - val_loss: 0.3863\n",
            "Epoch 116/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4147 - val_loss: 0.3964\n",
            "Epoch 117/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4147 - val_loss: 0.3858\n",
            "Epoch 118/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4133 - val_loss: 0.3946\n",
            "Epoch 119/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4142 - val_loss: 0.3857\n",
            "Epoch 120/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4142 - val_loss: 0.3934\n",
            "Epoch 121/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4133 - val_loss: 0.3845\n",
            "Epoch 122/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4120 - val_loss: 0.3934\n",
            "Epoch 123/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4100 - val_loss: 0.3836\n",
            "Epoch 124/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4156 - val_loss: 0.3942\n",
            "Epoch 125/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4113 - val_loss: 0.3843\n",
            "Epoch 126/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4144 - val_loss: 0.3949\n",
            "Epoch 127/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4135 - val_loss: 0.3848\n",
            "Epoch 128/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4137 - val_loss: 0.3947\n",
            "Epoch 129/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4148 - val_loss: 0.3849\n",
            "Epoch 130/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4122 - val_loss: 0.3977\n",
            "Epoch 131/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4152 - val_loss: 0.3857\n",
            "Epoch 132/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4133 - val_loss: 0.3951\n",
            "Epoch 133/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4139 - val_loss: 0.3827\n",
            "Epoch 134/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4101 - val_loss: 0.3910\n",
            "Epoch 135/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4108 - val_loss: 0.3808\n",
            "Epoch 136/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4087 - val_loss: 0.3890\n",
            "Epoch 137/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4097 - val_loss: 0.3796\n",
            "Epoch 138/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4046 - val_loss: 0.3861\n",
            "Epoch 139/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4077 - val_loss: 0.3775\n",
            "Epoch 140/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4047 - val_loss: 0.3844\n",
            "Epoch 141/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4039 - val_loss: 0.3771\n",
            "Epoch 142/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4028 - val_loss: 0.3857\n",
            "Epoch 143/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4043 - val_loss: 0.3775\n",
            "Epoch 144/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4043 - val_loss: 0.3878\n",
            "Epoch 145/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4042 - val_loss: 0.3794\n",
            "Epoch 146/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4052 - val_loss: 0.3897\n",
            "Epoch 147/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4092 - val_loss: 0.3803\n",
            "Epoch 148/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4057 - val_loss: 0.3904\n",
            "Epoch 149/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4080 - val_loss: 0.3803\n",
            "Epoch 150/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4062 - val_loss: 0.3900\n",
            "Epoch 151/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4089 - val_loss: 0.3801\n",
            "Epoch 152/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4057 - val_loss: 0.3880\n",
            "Epoch 153/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4055 - val_loss: 0.3781\n",
            "Epoch 154/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4063 - val_loss: 0.3862\n",
            "Epoch 155/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4070 - val_loss: 0.3756\n",
            "Epoch 156/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4026 - val_loss: 0.3829\n",
            "Epoch 157/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4027 - val_loss: 0.3740\n",
            "Epoch 158/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3996 - val_loss: 0.3820\n",
            "Epoch 159/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4014 - val_loss: 0.3744\n",
            "Epoch 160/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3997 - val_loss: 0.3827\n",
            "Epoch 161/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4006 - val_loss: 0.3745\n",
            "Epoch 162/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4027 - val_loss: 0.3840\n",
            "Epoch 163/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4023 - val_loss: 0.3755\n",
            "Epoch 164/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3994 - val_loss: 0.3856\n",
            "Epoch 165/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4036 - val_loss: 0.3753\n",
            "Epoch 166/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3998 - val_loss: 0.3826\n",
            "Epoch 167/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4005 - val_loss: 0.3746\n",
            "Epoch 168/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4011 - val_loss: 0.3843\n",
            "Epoch 169/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4035 - val_loss: 0.3752\n",
            "Epoch 170/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3999 - val_loss: 0.3814\n",
            "Epoch 171/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4008 - val_loss: 0.3729\n",
            "Epoch 172/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3976 - val_loss: 0.3803\n",
            "Epoch 173/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4012 - val_loss: 0.3726\n",
            "Epoch 174/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3996 - val_loss: 0.3804\n",
            "Epoch 175/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3973 - val_loss: 0.3714\n",
            "Epoch 176/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3992 - val_loss: 0.3802\n",
            "Epoch 177/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3982 - val_loss: 0.3728\n",
            "Epoch 178/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4014 - val_loss: 0.3829\n",
            "Epoch 179/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4021 - val_loss: 0.3742\n",
            "Epoch 180/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4015 - val_loss: 0.3837\n",
            "Epoch 181/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4032 - val_loss: 0.3737\n",
            "Epoch 182/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4010 - val_loss: 0.3807\n",
            "Epoch 183/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3992 - val_loss: 0.3713\n",
            "Epoch 184/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3972 - val_loss: 0.3793\n",
            "Epoch 185/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3985 - val_loss: 0.3704\n",
            "Epoch 186/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3960 - val_loss: 0.3765\n",
            "Epoch 187/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3964 - val_loss: 0.3680\n",
            "Epoch 188/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3915 - val_loss: 0.3750\n",
            "Epoch 189/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3933 - val_loss: 0.3682\n",
            "Epoch 190/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3945 - val_loss: 0.3746\n",
            "Epoch 191/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3943 - val_loss: 0.3669\n",
            "Epoch 192/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3944 - val_loss: 0.3732\n",
            "Epoch 193/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3926 - val_loss: 0.3663\n",
            "Epoch 194/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3894 - val_loss: 0.3725\n",
            "Epoch 195/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3935 - val_loss: 0.3657\n",
            "Epoch 196/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3896 - val_loss: 0.3730\n",
            "Epoch 197/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3943 - val_loss: 0.3661\n",
            "Epoch 198/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3902 - val_loss: 0.3729\n",
            "Epoch 199/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3916 - val_loss: 0.3665\n",
            "Epoch 200/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3882 - val_loss: 0.3732\n",
            "Epoch 201/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3939 - val_loss: 0.3662\n",
            "Epoch 202/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3931 - val_loss: 0.3745\n",
            "Epoch 203/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3927 - val_loss: 0.3673\n",
            "Epoch 204/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3922 - val_loss: 0.3749\n",
            "Epoch 205/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3942 - val_loss: 0.3684\n",
            "Epoch 206/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3960 - val_loss: 0.3749\n",
            "Epoch 207/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3945 - val_loss: 0.3668\n",
            "Epoch 208/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3913 - val_loss: 0.3726\n",
            "Epoch 209/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3909 - val_loss: 0.3644\n",
            "Epoch 210/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3862 - val_loss: 0.3709\n",
            "Epoch 211/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3861 - val_loss: 0.3640\n",
            "Epoch 212/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3885 - val_loss: 0.3716\n",
            "Epoch 213/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3877 - val_loss: 0.3645\n",
            "Epoch 214/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3879 - val_loss: 0.3707\n",
            "Epoch 215/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3894 - val_loss: 0.3638\n",
            "Epoch 216/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3879 - val_loss: 0.3688\n",
            "Epoch 217/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3869 - val_loss: 0.3632\n",
            "Epoch 218/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3893 - val_loss: 0.3696\n",
            "Epoch 219/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3893 - val_loss: 0.3634\n",
            "Epoch 220/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3904 - val_loss: 0.3705\n",
            "Epoch 221/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3874 - val_loss: 0.3637\n",
            "Epoch 222/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3879 - val_loss: 0.3714\n",
            "Epoch 223/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3869 - val_loss: 0.3638\n",
            "Epoch 224/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3926 - val_loss: 0.3702\n",
            "Epoch 225/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3913 - val_loss: 0.3635\n",
            "Epoch 226/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3874 - val_loss: 0.3690\n",
            "Epoch 227/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3881 - val_loss: 0.3615\n",
            "Epoch 228/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3839 - val_loss: 0.3675\n",
            "Epoch 229/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3871 - val_loss: 0.3610\n",
            "Epoch 230/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3821 - val_loss: 0.3646\n",
            "Epoch 231/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3830 - val_loss: 0.3593\n",
            "Epoch 232/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3823 - val_loss: 0.3656\n",
            "Epoch 233/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3833 - val_loss: 0.3596\n",
            "Epoch 234/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3810 - val_loss: 0.3644\n",
            "Epoch 235/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3839 - val_loss: 0.3596\n",
            "Epoch 236/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3839 - val_loss: 0.3641\n",
            "Epoch 237/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3826 - val_loss: 0.3584\n",
            "Epoch 238/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3802 - val_loss: 0.3628\n",
            "Epoch 239/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3830 - val_loss: 0.3583\n",
            "Epoch 240/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3824 - val_loss: 0.3626\n",
            "Epoch 241/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3807 - val_loss: 0.3581\n",
            "Epoch 242/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3824 - val_loss: 0.3631\n",
            "Epoch 243/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3829 - val_loss: 0.3574\n",
            "Epoch 244/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3800 - val_loss: 0.3631\n",
            "Epoch 245/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3811 - val_loss: 0.3572\n",
            "Epoch 246/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3818 - val_loss: 0.3641\n",
            "Epoch 247/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3818 - val_loss: 0.3582\n",
            "Epoch 248/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3823 - val_loss: 0.3635\n",
            "Epoch 249/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3828 - val_loss: 0.3579\n",
            "Epoch 250/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3809 - val_loss: 0.3631\n",
            "Epoch 251/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3823 - val_loss: 0.3570\n",
            "Epoch 252/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3812 - val_loss: 0.3621\n",
            "Epoch 253/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3782 - val_loss: 0.3571\n",
            "Epoch 254/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3786 - val_loss: 0.3620\n",
            "Epoch 255/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3791 - val_loss: 0.3563\n",
            "Epoch 256/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3779 - val_loss: 0.3604\n",
            "Epoch 257/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3797 - val_loss: 0.3548\n",
            "Epoch 258/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3781 - val_loss: 0.3597\n",
            "Epoch 259/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3778 - val_loss: 0.3552\n",
            "Epoch 260/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3755 - val_loss: 0.3597\n",
            "Epoch 261/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3759 - val_loss: 0.3550\n",
            "Epoch 262/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3769 - val_loss: 0.3589\n",
            "Epoch 263/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3778 - val_loss: 0.3536\n",
            "Epoch 264/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3756 - val_loss: 0.3585\n",
            "Epoch 265/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3763 - val_loss: 0.3540\n",
            "Epoch 266/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3745 - val_loss: 0.3591\n",
            "Epoch 267/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3752 - val_loss: 0.3536\n",
            "Epoch 268/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3727 - val_loss: 0.3578\n",
            "Epoch 269/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3742 - val_loss: 0.3531\n",
            "Epoch 270/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3715 - val_loss: 0.3571\n",
            "Epoch 271/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3744 - val_loss: 0.3520\n",
            "Epoch 272/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3750 - val_loss: 0.3557\n",
            "Epoch 273/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3720 - val_loss: 0.3515\n",
            "Epoch 274/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3743 - val_loss: 0.3561\n",
            "Epoch 275/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3739 - val_loss: 0.3524\n",
            "Epoch 276/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3764 - val_loss: 0.3566\n",
            "Epoch 277/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3731 - val_loss: 0.3528\n",
            "Epoch 278/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3709 - val_loss: 0.3550\n",
            "Epoch 279/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3716 - val_loss: 0.3529\n",
            "Epoch 280/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3756 - val_loss: 0.3576\n",
            "Epoch 281/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3751 - val_loss: 0.3528\n",
            "Epoch 282/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3749 - val_loss: 0.3578\n",
            "Epoch 283/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3724 - val_loss: 0.3530\n",
            "Epoch 284/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3738 - val_loss: 0.3567\n",
            "Epoch 285/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3720 - val_loss: 0.3519\n",
            "Epoch 286/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3713 - val_loss: 0.3550\n",
            "Epoch 287/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3704 - val_loss: 0.3490\n",
            "Epoch 288/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3703 - val_loss: 0.3527\n",
            "Epoch 289/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3689 - val_loss: 0.3488\n",
            "Epoch 290/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3698 - val_loss: 0.3539\n",
            "Epoch 291/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3698 - val_loss: 0.3484\n",
            "Epoch 292/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3682 - val_loss: 0.3530\n",
            "Epoch 293/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3681 - val_loss: 0.3483\n",
            "Epoch 294/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3645 - val_loss: 0.3509\n",
            "Epoch 295/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3683 - val_loss: 0.3474\n",
            "Epoch 296/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3697 - val_loss: 0.3506\n",
            "Epoch 297/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3646 - val_loss: 0.3470\n",
            "Epoch 298/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3646 - val_loss: 0.3489\n",
            "Epoch 299/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3658 - val_loss: 0.3460\n",
            "Epoch 300/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3623 - val_loss: 0.3491\n",
            "Epoch 301/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3636 - val_loss: 0.3470\n",
            "Epoch 302/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3652 - val_loss: 0.3517\n",
            "Epoch 303/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3671 - val_loss: 0.3482\n",
            "Epoch 304/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3658 - val_loss: 0.3512\n",
            "Epoch 305/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3649 - val_loss: 0.3490\n",
            "Epoch 306/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3659 - val_loss: 0.3512\n",
            "Epoch 307/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3654 - val_loss: 0.3471\n",
            "Epoch 308/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3648 - val_loss: 0.3500\n",
            "Epoch 309/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3659 - val_loss: 0.3460\n",
            "Epoch 310/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3621 - val_loss: 0.3480\n",
            "Epoch 311/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3626 - val_loss: 0.3447\n",
            "Epoch 312/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3633 - val_loss: 0.3485\n",
            "Epoch 313/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3611 - val_loss: 0.3445\n",
            "Epoch 314/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3613 - val_loss: 0.3467\n",
            "Epoch 315/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3592 - val_loss: 0.3444\n",
            "Epoch 316/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3640 - val_loss: 0.3481\n",
            "Epoch 317/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3609 - val_loss: 0.3441\n",
            "Epoch 318/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3602 - val_loss: 0.3463\n",
            "Epoch 319/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3588 - val_loss: 0.3431\n",
            "Epoch 320/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3590 - val_loss: 0.3457\n",
            "Epoch 321/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3606 - val_loss: 0.3425\n",
            "Epoch 322/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3584 - val_loss: 0.3458\n",
            "Epoch 323/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3584 - val_loss: 0.3417\n",
            "Epoch 324/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3555 - val_loss: 0.3444\n",
            "Epoch 325/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3573 - val_loss: 0.3410\n",
            "Epoch 326/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3576 - val_loss: 0.3448\n",
            "Epoch 327/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3574 - val_loss: 0.3402\n",
            "Epoch 328/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3570 - val_loss: 0.3439\n",
            "Epoch 329/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3569 - val_loss: 0.3410\n",
            "Epoch 330/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3565 - val_loss: 0.3451\n",
            "Epoch 331/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3573 - val_loss: 0.3417\n",
            "Epoch 332/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3580 - val_loss: 0.3467\n",
            "Epoch 333/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3606 - val_loss: 0.3426\n",
            "Epoch 334/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3532 - val_loss: 0.3460\n",
            "Epoch 335/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3568 - val_loss: 0.3421\n",
            "Epoch 336/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3572 - val_loss: 0.3467\n",
            "Epoch 337/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3577 - val_loss: 0.3423\n",
            "Epoch 338/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3567 - val_loss: 0.3448\n",
            "Epoch 339/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3542 - val_loss: 0.3405\n",
            "Epoch 340/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3569 - val_loss: 0.3436\n",
            "Epoch 341/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3545 - val_loss: 0.3403\n",
            "Epoch 342/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3539 - val_loss: 0.3423\n",
            "Epoch 343/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3529 - val_loss: 0.3394\n",
            "Epoch 344/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3527 - val_loss: 0.3420\n",
            "Epoch 345/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3523 - val_loss: 0.3403\n",
            "Epoch 346/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3555 - val_loss: 0.3427\n",
            "Epoch 347/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3548 - val_loss: 0.3400\n",
            "Epoch 348/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3527 - val_loss: 0.3412\n",
            "Epoch 349/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3564 - val_loss: 0.3383\n",
            "Epoch 350/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3498 - val_loss: 0.3392\n",
            "Epoch 351/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3510 - val_loss: 0.3372\n",
            "Epoch 352/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3517 - val_loss: 0.3400\n",
            "Epoch 353/500\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.3526 - val_loss: 0.3372\n",
            "Epoch 354/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3498 - val_loss: 0.3384\n",
            "Epoch 355/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3486 - val_loss: 0.3367\n",
            "Epoch 356/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3522 - val_loss: 0.3375\n",
            "Epoch 357/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3489 - val_loss: 0.3370\n",
            "Epoch 358/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3491 - val_loss: 0.3379\n",
            "Epoch 359/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3521 - val_loss: 0.3370\n",
            "Epoch 360/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3470 - val_loss: 0.3370\n",
            "Epoch 361/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3491 - val_loss: 0.3365\n",
            "Epoch 362/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3504 - val_loss: 0.3379\n",
            "Epoch 363/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3501 - val_loss: 0.3367\n",
            "Epoch 364/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3470 - val_loss: 0.3381\n",
            "Epoch 365/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3491 - val_loss: 0.3369\n",
            "Epoch 366/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3497 - val_loss: 0.3373\n",
            "Epoch 367/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3471 - val_loss: 0.3368\n",
            "Epoch 368/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3487 - val_loss: 0.3386\n",
            "Epoch 369/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3477 - val_loss: 0.3384\n",
            "Epoch 370/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3468 - val_loss: 0.3374\n",
            "Epoch 371/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3462 - val_loss: 0.3362\n",
            "Epoch 372/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3473 - val_loss: 0.3357\n",
            "Epoch 373/500\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.3471 - val_loss: 0.3370\n",
            "Epoch 374/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3463 - val_loss: 0.3356\n",
            "Epoch 375/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3428 - val_loss: 0.3358\n",
            "Epoch 376/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3433 - val_loss: 0.3347\n",
            "Epoch 377/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3472 - val_loss: 0.3354\n",
            "Epoch 378/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3445 - val_loss: 0.3358\n",
            "Epoch 379/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3463 - val_loss: 0.3345\n",
            "Epoch 380/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3428 - val_loss: 0.3355\n",
            "Epoch 381/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3424 - val_loss: 0.3333\n",
            "Epoch 382/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3426 - val_loss: 0.3364\n",
            "Epoch 383/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3425 - val_loss: 0.3334\n",
            "Epoch 384/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3448 - val_loss: 0.3366\n",
            "Epoch 385/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3419 - val_loss: 0.3343\n",
            "Epoch 386/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3469 - val_loss: 0.3369\n",
            "Epoch 387/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3445 - val_loss: 0.3334\n",
            "Epoch 388/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3394 - val_loss: 0.3346\n",
            "Epoch 389/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3418 - val_loss: 0.3316\n",
            "Epoch 390/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3419 - val_loss: 0.3342\n",
            "Epoch 391/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3424 - val_loss: 0.3305\n",
            "Epoch 392/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3387 - val_loss: 0.3351\n",
            "Epoch 393/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3420 - val_loss: 0.3297\n",
            "Epoch 394/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3418 - val_loss: 0.3357\n",
            "Epoch 395/500\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.3421 - val_loss: 0.3295\n",
            "Epoch 396/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3415 - val_loss: 0.3373\n",
            "Epoch 397/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3403 - val_loss: 0.3303\n",
            "Epoch 398/500\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.3425 - val_loss: 0.3365\n",
            "Epoch 399/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3391 - val_loss: 0.3294\n",
            "Epoch 400/500\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.3396 - val_loss: 0.3349\n",
            "Epoch 401/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3410 - val_loss: 0.3279\n",
            "Epoch 402/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3370 - val_loss: 0.3337\n",
            "Epoch 403/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3398 - val_loss: 0.3281\n",
            "Epoch 404/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3376 - val_loss: 0.3334\n",
            "Epoch 405/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3352 - val_loss: 0.3266\n",
            "Epoch 406/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3370 - val_loss: 0.3343\n",
            "Epoch 407/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3350 - val_loss: 0.3264\n",
            "Epoch 408/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3403 - val_loss: 0.3355\n",
            "Epoch 409/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3384 - val_loss: 0.3261\n",
            "Epoch 410/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3388 - val_loss: 0.3345\n",
            "Epoch 411/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3351 - val_loss: 0.3253\n",
            "Epoch 412/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3354 - val_loss: 0.3336\n",
            "Epoch 413/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3352 - val_loss: 0.3249\n",
            "Epoch 414/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3362 - val_loss: 0.3326\n",
            "Epoch 415/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3338 - val_loss: 0.3252\n",
            "Epoch 416/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3350 - val_loss: 0.3341\n",
            "Epoch 417/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3349 - val_loss: 0.3256\n",
            "Epoch 418/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3327 - val_loss: 0.3339\n",
            "Epoch 419/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3347 - val_loss: 0.3261\n",
            "Epoch 420/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3358 - val_loss: 0.3317\n",
            "Epoch 421/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3365 - val_loss: 0.3257\n",
            "Epoch 422/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3355 - val_loss: 0.3312\n",
            "Epoch 423/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3318 - val_loss: 0.3265\n",
            "Epoch 424/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3355 - val_loss: 0.3322\n",
            "Epoch 425/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3362 - val_loss: 0.3273\n",
            "Epoch 426/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3375 - val_loss: 0.3316\n",
            "Epoch 427/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3341 - val_loss: 0.3260\n",
            "Epoch 428/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3347 - val_loss: 0.3300\n",
            "Epoch 429/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3318 - val_loss: 0.3255\n",
            "Epoch 430/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3321 - val_loss: 0.3277\n",
            "Epoch 431/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3333 - val_loss: 0.3263\n",
            "Epoch 432/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3342 - val_loss: 0.3283\n",
            "Epoch 433/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3325 - val_loss: 0.3275\n",
            "Epoch 434/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3335 - val_loss: 0.3279\n",
            "Epoch 435/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3286 - val_loss: 0.3281\n",
            "Epoch 436/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3334 - val_loss: 0.3284\n",
            "Epoch 437/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3335 - val_loss: 0.3265\n",
            "Epoch 438/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3338 - val_loss: 0.3280\n",
            "Epoch 439/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3326 - val_loss: 0.3268\n",
            "Epoch 440/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3289 - val_loss: 0.3271\n",
            "Epoch 441/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3301 - val_loss: 0.3263\n",
            "Epoch 442/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3327 - val_loss: 0.3253\n",
            "Epoch 443/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3297 - val_loss: 0.3264\n",
            "Epoch 444/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3326 - val_loss: 0.3227\n",
            "Epoch 445/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3315 - val_loss: 0.3275\n",
            "Epoch 446/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3295 - val_loss: 0.3217\n",
            "Epoch 447/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3301 - val_loss: 0.3270\n",
            "Epoch 448/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3319 - val_loss: 0.3208\n",
            "Epoch 449/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3292 - val_loss: 0.3274\n",
            "Epoch 450/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3302 - val_loss: 0.3209\n",
            "Epoch 451/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3276 - val_loss: 0.3264\n",
            "Epoch 452/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3272 - val_loss: 0.3196\n",
            "Epoch 453/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3305 - val_loss: 0.3256\n",
            "Epoch 454/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3261 - val_loss: 0.3182\n",
            "Epoch 455/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3261 - val_loss: 0.3278\n",
            "Epoch 456/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3270 - val_loss: 0.3167\n",
            "Epoch 457/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3246 - val_loss: 0.3278\n",
            "Epoch 458/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3235 - val_loss: 0.3171\n",
            "Epoch 459/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3282 - val_loss: 0.3288\n",
            "Epoch 460/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3243 - val_loss: 0.3163\n",
            "Epoch 461/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3270 - val_loss: 0.3288\n",
            "Epoch 462/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3262 - val_loss: 0.3162\n",
            "Epoch 463/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3299 - val_loss: 0.3298\n",
            "Epoch 464/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3262 - val_loss: 0.3147\n",
            "Epoch 465/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3299 - val_loss: 0.3288\n",
            "Epoch 466/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3244 - val_loss: 0.3131\n",
            "Epoch 467/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3253 - val_loss: 0.3273\n",
            "Epoch 468/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3254 - val_loss: 0.3123\n",
            "Epoch 469/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3239 - val_loss: 0.3271\n",
            "Epoch 470/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3221 - val_loss: 0.3117\n",
            "Epoch 471/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3214 - val_loss: 0.3259\n",
            "Epoch 472/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3224 - val_loss: 0.3113\n",
            "Epoch 473/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3223 - val_loss: 0.3263\n",
            "Epoch 474/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3177 - val_loss: 0.3108\n",
            "Epoch 475/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3208 - val_loss: 0.3262\n",
            "Epoch 476/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3218 - val_loss: 0.3110\n",
            "Epoch 477/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3185 - val_loss: 0.3256\n",
            "Epoch 478/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3180 - val_loss: 0.3104\n",
            "Epoch 479/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3235 - val_loss: 0.3267\n",
            "Epoch 480/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3164 - val_loss: 0.3102\n",
            "Epoch 481/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3197 - val_loss: 0.3265\n",
            "Epoch 482/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3169 - val_loss: 0.3101\n",
            "Epoch 483/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3214 - val_loss: 0.3281\n",
            "Epoch 484/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3182 - val_loss: 0.3101\n",
            "Epoch 485/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3209 - val_loss: 0.3268\n",
            "Epoch 486/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3186 - val_loss: 0.3119\n",
            "Epoch 487/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3192 - val_loss: 0.3264\n",
            "Epoch 488/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3204 - val_loss: 0.3124\n",
            "Epoch 489/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3200 - val_loss: 0.3252\n",
            "Epoch 490/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3198 - val_loss: 0.3151\n",
            "Epoch 491/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3250 - val_loss: 0.3273\n",
            "Epoch 492/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3235 - val_loss: 0.3171\n",
            "Epoch 493/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3221 - val_loss: 0.3279\n",
            "Epoch 494/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3226 - val_loss: 0.3197\n",
            "Epoch 495/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3242 - val_loss: 0.3299\n",
            "Epoch 496/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3289 - val_loss: 0.3209\n",
            "Epoch 497/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3259 - val_loss: 0.3263\n",
            "Epoch 498/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3196 - val_loss: 0.3208\n",
            "Epoch 499/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3247 - val_loss: 0.3256\n",
            "Epoch 500/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3231 - val_loss: 0.3187\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_6 (Bidirection (None, 5, 1024)           2146304   \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 5, 256)            1311744   \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 5, 1)              257       \n",
            "=================================================================\n",
            "Total params: 3,458,305\n",
            "Trainable params: 3,458,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 3298 samples, validate on 825 samples\n",
            "Epoch 1/1000\n",
            "3298/3298 [==============================] - 5s 2ms/step - loss: 1.0056 - val_loss: 0.6876\n",
            "Epoch 2/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.6853 - val_loss: 0.4939\n",
            "Epoch 3/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.5038 - val_loss: 0.3897\n",
            "Epoch 4/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.4093 - val_loss: 0.3341\n",
            "Epoch 5/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3548 - val_loss: 0.2993\n",
            "Epoch 6/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3212 - val_loss: 0.2757\n",
            "Epoch 7/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2883 - val_loss: 0.2582\n",
            "Epoch 8/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2702 - val_loss: 0.2453\n",
            "Epoch 9/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.2579 - val_loss: 0.2352\n",
            "Epoch 10/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2447 - val_loss: 0.2273\n",
            "Epoch 11/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2357 - val_loss: 0.2213\n",
            "Epoch 12/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2290 - val_loss: 0.2167\n",
            "Epoch 13/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2255 - val_loss: 0.2134\n",
            "Epoch 14/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2193 - val_loss: 0.2105\n",
            "Epoch 15/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2172 - val_loss: 0.2084\n",
            "Epoch 16/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2141 - val_loss: 0.2063\n",
            "Epoch 17/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2164 - val_loss: 0.2050\n",
            "Epoch 18/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2148 - val_loss: 0.2040\n",
            "Epoch 19/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2104 - val_loss: 0.2025\n",
            "Epoch 20/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2092 - val_loss: 0.2013\n",
            "Epoch 21/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2075 - val_loss: 0.2003\n",
            "Epoch 22/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2070 - val_loss: 0.1995\n",
            "Epoch 23/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2053 - val_loss: 0.1982\n",
            "Epoch 24/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2057 - val_loss: 0.1979\n",
            "Epoch 25/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2068 - val_loss: 0.1966\n",
            "Epoch 26/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2026 - val_loss: 0.1975\n",
            "Epoch 27/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2026 - val_loss: 0.1966\n",
            "Epoch 28/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2050 - val_loss: 0.1999\n",
            "Epoch 29/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2070 - val_loss: 0.2016\n",
            "Epoch 30/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2114 - val_loss: 0.2052\n",
            "Epoch 31/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2133 - val_loss: 0.2066\n",
            "Epoch 32/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2148 - val_loss: 0.2092\n",
            "Epoch 33/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2150 - val_loss: 0.2101\n",
            "Epoch 34/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2193 - val_loss: 0.2096\n",
            "Epoch 35/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2134 - val_loss: 0.2086\n",
            "Epoch 36/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2194 - val_loss: 0.2073\n",
            "Epoch 37/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2149 - val_loss: 0.2072\n",
            "Epoch 38/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2163 - val_loss: 0.2042\n",
            "Epoch 39/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2138 - val_loss: 0.2019\n",
            "Epoch 40/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2106 - val_loss: 0.1990\n",
            "Epoch 41/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2054 - val_loss: 0.1985\n",
            "Epoch 42/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2078 - val_loss: 0.1959\n",
            "Epoch 43/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2016 - val_loss: 0.1942\n",
            "Epoch 44/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2049 - val_loss: 0.1930\n",
            "Epoch 45/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1988 - val_loss: 0.1912\n",
            "Epoch 46/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2003 - val_loss: 0.1910\n",
            "Epoch 47/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1960 - val_loss: 0.1889\n",
            "Epoch 48/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1945 - val_loss: 0.1886\n",
            "Epoch 49/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1944 - val_loss: 0.1879\n",
            "Epoch 50/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1947 - val_loss: 0.1871\n",
            "Epoch 51/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1922 - val_loss: 0.1843\n",
            "Epoch 52/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1937 - val_loss: 0.1842\n",
            "Epoch 53/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1941 - val_loss: 0.1836\n",
            "Epoch 54/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1899 - val_loss: 0.1833\n",
            "Epoch 55/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1904 - val_loss: 0.1820\n",
            "Epoch 56/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1922 - val_loss: 0.1823\n",
            "Epoch 57/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1888 - val_loss: 0.1815\n",
            "Epoch 58/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1900 - val_loss: 0.1817\n",
            "Epoch 59/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1870 - val_loss: 0.1806\n",
            "Epoch 60/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1885 - val_loss: 0.1809\n",
            "Epoch 61/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1888 - val_loss: 0.1796\n",
            "Epoch 62/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1882 - val_loss: 0.1808\n",
            "Epoch 63/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1849 - val_loss: 0.1805\n",
            "Epoch 64/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1881 - val_loss: 0.1810\n",
            "Epoch 65/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1865 - val_loss: 0.1807\n",
            "Epoch 66/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1864 - val_loss: 0.1806\n",
            "Epoch 67/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1875 - val_loss: 0.1794\n",
            "Epoch 68/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1868 - val_loss: 0.1798\n",
            "Epoch 69/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1879 - val_loss: 0.1801\n",
            "Epoch 70/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1872 - val_loss: 0.1815\n",
            "Epoch 71/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1890 - val_loss: 0.1808\n",
            "Epoch 72/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1886 - val_loss: 0.1817\n",
            "Epoch 73/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1898 - val_loss: 0.1818\n",
            "Epoch 74/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1909 - val_loss: 0.1810\n",
            "Epoch 75/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1900 - val_loss: 0.1792\n",
            "Epoch 76/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1915 - val_loss: 0.1789\n",
            "Epoch 77/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1877 - val_loss: 0.1778\n",
            "Epoch 78/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1860 - val_loss: 0.1761\n",
            "Epoch 79/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1829 - val_loss: 0.1756\n",
            "Epoch 80/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1837 - val_loss: 0.1746\n",
            "Epoch 81/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1811 - val_loss: 0.1733\n",
            "Epoch 82/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1811 - val_loss: 0.1726\n",
            "Epoch 83/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1790 - val_loss: 0.1711\n",
            "Epoch 84/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1781 - val_loss: 0.1715\n",
            "Epoch 85/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1785 - val_loss: 0.1702\n",
            "Epoch 86/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1801 - val_loss: 0.1706\n",
            "Epoch 87/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1776 - val_loss: 0.1696\n",
            "Epoch 88/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1782 - val_loss: 0.1695\n",
            "Epoch 89/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1781 - val_loss: 0.1688\n",
            "Epoch 90/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1763 - val_loss: 0.1691\n",
            "Epoch 91/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1782 - val_loss: 0.1684\n",
            "Epoch 92/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1771 - val_loss: 0.1690\n",
            "Epoch 93/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1754 - val_loss: 0.1687\n",
            "Epoch 94/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1782 - val_loss: 0.1685\n",
            "Epoch 95/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1737 - val_loss: 0.1685\n",
            "Epoch 96/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1762 - val_loss: 0.1681\n",
            "Epoch 97/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1762 - val_loss: 0.1678\n",
            "Epoch 98/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1786 - val_loss: 0.1679\n",
            "Epoch 99/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1743 - val_loss: 0.1676\n",
            "Epoch 100/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1766 - val_loss: 0.1670\n",
            "Epoch 101/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1752 - val_loss: 0.1671\n",
            "Epoch 102/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1770 - val_loss: 0.1664\n",
            "Epoch 103/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1731 - val_loss: 0.1665\n",
            "Epoch 104/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1753 - val_loss: 0.1657\n",
            "Epoch 105/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1727 - val_loss: 0.1662\n",
            "Epoch 106/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1725 - val_loss: 0.1655\n",
            "Epoch 107/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1732 - val_loss: 0.1646\n",
            "Epoch 108/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1710 - val_loss: 0.1641\n",
            "Epoch 109/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1713 - val_loss: 0.1641\n",
            "Epoch 110/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1726 - val_loss: 0.1641\n",
            "Epoch 111/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1718 - val_loss: 0.1643\n",
            "Epoch 112/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1728 - val_loss: 0.1638\n",
            "Epoch 113/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1715 - val_loss: 0.1636\n",
            "Epoch 114/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1701 - val_loss: 0.1625\n",
            "Epoch 115/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1714 - val_loss: 0.1642\n",
            "Epoch 116/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1710 - val_loss: 0.1644\n",
            "Epoch 117/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1712 - val_loss: 0.1653\n",
            "Epoch 118/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1719 - val_loss: 0.1632\n",
            "Epoch 119/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1701 - val_loss: 0.1639\n",
            "Epoch 120/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1730 - val_loss: 0.1633\n",
            "Epoch 121/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1699 - val_loss: 0.1627\n",
            "Epoch 122/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1713 - val_loss: 0.1626\n",
            "Epoch 123/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1704 - val_loss: 0.1633\n",
            "Epoch 124/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1708 - val_loss: 0.1631\n",
            "Epoch 125/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1689 - val_loss: 0.1640\n",
            "Epoch 126/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1748 - val_loss: 0.1632\n",
            "Epoch 127/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1681 - val_loss: 0.1640\n",
            "Epoch 128/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1726 - val_loss: 0.1611\n",
            "Epoch 129/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1706 - val_loss: 0.1621\n",
            "Epoch 130/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1668 - val_loss: 0.1600\n",
            "Epoch 131/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1679 - val_loss: 0.1611\n",
            "Epoch 132/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1680 - val_loss: 0.1598\n",
            "Epoch 133/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1703 - val_loss: 0.1613\n",
            "Epoch 134/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1685 - val_loss: 0.1595\n",
            "Epoch 135/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1656 - val_loss: 0.1594\n",
            "Epoch 136/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1676 - val_loss: 0.1584\n",
            "Epoch 137/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1678 - val_loss: 0.1595\n",
            "Epoch 138/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1684 - val_loss: 0.1580\n",
            "Epoch 139/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1661 - val_loss: 0.1586\n",
            "Epoch 140/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1680 - val_loss: 0.1582\n",
            "Epoch 141/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1660 - val_loss: 0.1589\n",
            "Epoch 142/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1651 - val_loss: 0.1579\n",
            "Epoch 143/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1661 - val_loss: 0.1583\n",
            "Epoch 144/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1687 - val_loss: 0.1563\n",
            "Epoch 145/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1655 - val_loss: 0.1587\n",
            "Epoch 146/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1670 - val_loss: 0.1569\n",
            "Epoch 147/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1629 - val_loss: 0.1579\n",
            "Epoch 148/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1653 - val_loss: 0.1558\n",
            "Epoch 149/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1623 - val_loss: 0.1567\n",
            "Epoch 150/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1648 - val_loss: 0.1561\n",
            "Epoch 151/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1652 - val_loss: 0.1580\n",
            "Epoch 152/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1650 - val_loss: 0.1553\n",
            "Epoch 153/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1638 - val_loss: 0.1557\n",
            "Epoch 154/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1655 - val_loss: 0.1540\n",
            "Epoch 155/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1610 - val_loss: 0.1549\n",
            "Epoch 156/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1634 - val_loss: 0.1530\n",
            "Epoch 157/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1636 - val_loss: 0.1539\n",
            "Epoch 158/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1606 - val_loss: 0.1537\n",
            "Epoch 159/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1634 - val_loss: 0.1551\n",
            "Epoch 160/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1660 - val_loss: 0.1538\n",
            "Epoch 161/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1610 - val_loss: 0.1547\n",
            "Epoch 162/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1593 - val_loss: 0.1527\n",
            "Epoch 163/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1630 - val_loss: 0.1537\n",
            "Epoch 164/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1599 - val_loss: 0.1533\n",
            "Epoch 165/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1601 - val_loss: 0.1535\n",
            "Epoch 166/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1620 - val_loss: 0.1517\n",
            "Epoch 167/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1564 - val_loss: 0.1532\n",
            "Epoch 168/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1608 - val_loss: 0.1521\n",
            "Epoch 169/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1587 - val_loss: 0.1533\n",
            "Epoch 170/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1600 - val_loss: 0.1533\n",
            "Epoch 171/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1615 - val_loss: 0.1539\n",
            "Epoch 172/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1620 - val_loss: 0.1520\n",
            "Epoch 173/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1617 - val_loss: 0.1532\n",
            "Epoch 174/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1626 - val_loss: 0.1522\n",
            "Epoch 175/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1601 - val_loss: 0.1514\n",
            "Epoch 176/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1594 - val_loss: 0.1507\n",
            "Epoch 177/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1585 - val_loss: 0.1520\n",
            "Epoch 178/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1607 - val_loss: 0.1513\n",
            "Epoch 179/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1600 - val_loss: 0.1530\n",
            "Epoch 180/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1608 - val_loss: 0.1520\n",
            "Epoch 181/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1581 - val_loss: 0.1533\n",
            "Epoch 182/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1626 - val_loss: 0.1508\n",
            "Epoch 183/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1606 - val_loss: 0.1527\n",
            "Epoch 184/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1627 - val_loss: 0.1506\n",
            "Epoch 185/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1591 - val_loss: 0.1514\n",
            "Epoch 186/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1606 - val_loss: 0.1495\n",
            "Epoch 187/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1555 - val_loss: 0.1499\n",
            "Epoch 188/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1590 - val_loss: 0.1483\n",
            "Epoch 189/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1576 - val_loss: 0.1490\n",
            "Epoch 190/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1579 - val_loss: 0.1478\n",
            "Epoch 191/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1563 - val_loss: 0.1499\n",
            "Epoch 192/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1594 - val_loss: 0.1487\n",
            "Epoch 193/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1568 - val_loss: 0.1496\n",
            "Epoch 194/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1591 - val_loss: 0.1481\n",
            "Epoch 195/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1550 - val_loss: 0.1485\n",
            "Epoch 196/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1557 - val_loss: 0.1480\n",
            "Epoch 197/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1576 - val_loss: 0.1481\n",
            "Epoch 198/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1584 - val_loss: 0.1480\n",
            "Epoch 199/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1568 - val_loss: 0.1482\n",
            "Epoch 200/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1585 - val_loss: 0.1478\n",
            "Epoch 201/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1568 - val_loss: 0.1483\n",
            "Epoch 202/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1540 - val_loss: 0.1478\n",
            "Epoch 203/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1542 - val_loss: 0.1493\n",
            "Epoch 204/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1557 - val_loss: 0.1479\n",
            "Epoch 205/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1580 - val_loss: 0.1495\n",
            "Epoch 206/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1565 - val_loss: 0.1477\n",
            "Epoch 207/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1552 - val_loss: 0.1498\n",
            "Epoch 208/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1595 - val_loss: 0.1478\n",
            "Epoch 209/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1565 - val_loss: 0.1489\n",
            "Epoch 210/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1564 - val_loss: 0.1477\n",
            "Epoch 211/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1560 - val_loss: 0.1490\n",
            "Epoch 212/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1566 - val_loss: 0.1477\n",
            "Epoch 213/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1579 - val_loss: 0.1496\n",
            "Epoch 214/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1589 - val_loss: 0.1481\n",
            "Epoch 215/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1552 - val_loss: 0.1501\n",
            "Epoch 216/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1600 - val_loss: 0.1478\n",
            "Epoch 217/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1558 - val_loss: 0.1488\n",
            "Epoch 218/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1562 - val_loss: 0.1474\n",
            "Epoch 219/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1573 - val_loss: 0.1489\n",
            "Epoch 220/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1562 - val_loss: 0.1468\n",
            "Epoch 221/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1552 - val_loss: 0.1481\n",
            "Epoch 222/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1536 - val_loss: 0.1469\n",
            "Epoch 223/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1538 - val_loss: 0.1486\n",
            "Epoch 224/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1562 - val_loss: 0.1476\n",
            "Epoch 225/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1566 - val_loss: 0.1494\n",
            "Epoch 226/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1565 - val_loss: 0.1469\n",
            "Epoch 227/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1561 - val_loss: 0.1486\n",
            "Epoch 228/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1570 - val_loss: 0.1471\n",
            "Epoch 229/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1533 - val_loss: 0.1477\n",
            "Epoch 230/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1549 - val_loss: 0.1464\n",
            "Epoch 231/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1551 - val_loss: 0.1484\n",
            "Epoch 232/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1573 - val_loss: 0.1467\n",
            "Epoch 233/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1556 - val_loss: 0.1476\n",
            "Epoch 234/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1568 - val_loss: 0.1475\n",
            "Epoch 235/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1570 - val_loss: 0.1504\n",
            "Epoch 236/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1574 - val_loss: 0.1479\n",
            "Epoch 237/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1539 - val_loss: 0.1486\n",
            "Epoch 238/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1579 - val_loss: 0.1470\n",
            "Epoch 239/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1544 - val_loss: 0.1473\n",
            "Epoch 240/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1554 - val_loss: 0.1448\n",
            "Epoch 241/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1539 - val_loss: 0.1456\n",
            "Epoch 242/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1515 - val_loss: 0.1444\n",
            "Epoch 243/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1526 - val_loss: 0.1441\n",
            "Epoch 244/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1518 - val_loss: 0.1441\n",
            "Epoch 245/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1530 - val_loss: 0.1445\n",
            "Epoch 246/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1505 - val_loss: 0.1435\n",
            "Epoch 247/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1494 - val_loss: 0.1436\n",
            "Epoch 248/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1530 - val_loss: 0.1428\n",
            "Epoch 249/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1502 - val_loss: 0.1429\n",
            "Epoch 250/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1495 - val_loss: 0.1434\n",
            "Epoch 251/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1532 - val_loss: 0.1432\n",
            "Epoch 252/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1520 - val_loss: 0.1428\n",
            "Epoch 253/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1519 - val_loss: 0.1434\n",
            "Epoch 254/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1508 - val_loss: 0.1429\n",
            "Epoch 255/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1512 - val_loss: 0.1435\n",
            "Epoch 256/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1532 - val_loss: 0.1428\n",
            "Epoch 257/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1511 - val_loss: 0.1429\n",
            "Epoch 258/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1493 - val_loss: 0.1416\n",
            "Epoch 259/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1505 - val_loss: 0.1421\n",
            "Epoch 260/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1516 - val_loss: 0.1420\n",
            "Epoch 261/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1508 - val_loss: 0.1425\n",
            "Epoch 262/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1495 - val_loss: 0.1423\n",
            "Epoch 263/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1503 - val_loss: 0.1437\n",
            "Epoch 264/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1527 - val_loss: 0.1428\n",
            "Epoch 265/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1511 - val_loss: 0.1439\n",
            "Epoch 266/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1524 - val_loss: 0.1434\n",
            "Epoch 267/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1497 - val_loss: 0.1444\n",
            "Epoch 268/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1529 - val_loss: 0.1443\n",
            "Epoch 269/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1520 - val_loss: 0.1454\n",
            "Epoch 270/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1554 - val_loss: 0.1449\n",
            "Epoch 271/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1516 - val_loss: 0.1467\n",
            "Epoch 272/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1551 - val_loss: 0.1442\n",
            "Epoch 273/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1539 - val_loss: 0.1449\n",
            "Epoch 274/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1515 - val_loss: 0.1440\n",
            "Epoch 275/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1513 - val_loss: 0.1452\n",
            "Epoch 276/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1503 - val_loss: 0.1445\n",
            "Epoch 277/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1516 - val_loss: 0.1454\n",
            "Epoch 278/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1550 - val_loss: 0.1450\n",
            "Epoch 279/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1521 - val_loss: 0.1468\n",
            "Epoch 280/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1545 - val_loss: 0.1452\n",
            "Epoch 281/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1510 - val_loss: 0.1453\n",
            "Epoch 282/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1530 - val_loss: 0.1444\n",
            "Epoch 283/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1529 - val_loss: 0.1458\n",
            "Epoch 284/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1544 - val_loss: 0.1446\n",
            "Epoch 285/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1520 - val_loss: 0.1453\n",
            "Epoch 286/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1529 - val_loss: 0.1444\n",
            "Epoch 287/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1505 - val_loss: 0.1447\n",
            "Epoch 288/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1545 - val_loss: 0.1437\n",
            "Epoch 289/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1521 - val_loss: 0.1452\n",
            "Epoch 290/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1517 - val_loss: 0.1438\n",
            "Epoch 291/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1506 - val_loss: 0.1444\n",
            "Epoch 292/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1514 - val_loss: 0.1425\n",
            "Epoch 293/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1500 - val_loss: 0.1430\n",
            "Epoch 294/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1512 - val_loss: 0.1416\n",
            "Epoch 295/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1490 - val_loss: 0.1417\n",
            "Epoch 296/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1509 - val_loss: 0.1415\n",
            "Epoch 297/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1506 - val_loss: 0.1417\n",
            "Epoch 298/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1485 - val_loss: 0.1411\n",
            "Epoch 299/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1490 - val_loss: 0.1414\n",
            "Epoch 300/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1482 - val_loss: 0.1412\n",
            "Epoch 301/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1465 - val_loss: 0.1421\n",
            "Epoch 302/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1518 - val_loss: 0.1415\n",
            "Epoch 303/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1505 - val_loss: 0.1418\n",
            "Epoch 304/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1480 - val_loss: 0.1414\n",
            "Epoch 305/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1493 - val_loss: 0.1408\n",
            "Epoch 306/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1501 - val_loss: 0.1410\n",
            "Epoch 307/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1493 - val_loss: 0.1420\n",
            "Epoch 308/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1491 - val_loss: 0.1417\n",
            "Epoch 309/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1476 - val_loss: 0.1422\n",
            "Epoch 310/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1497 - val_loss: 0.1412\n",
            "Epoch 311/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1485 - val_loss: 0.1420\n",
            "Epoch 312/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1493 - val_loss: 0.1412\n",
            "Epoch 313/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1477 - val_loss: 0.1406\n",
            "Epoch 314/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1502 - val_loss: 0.1406\n",
            "Epoch 315/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1461 - val_loss: 0.1407\n",
            "Epoch 316/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1493 - val_loss: 0.1403\n",
            "Epoch 317/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1487 - val_loss: 0.1412\n",
            "Epoch 318/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1449 - val_loss: 0.1406\n",
            "Epoch 319/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1489 - val_loss: 0.1418\n",
            "Epoch 320/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1509 - val_loss: 0.1410\n",
            "Epoch 321/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1476 - val_loss: 0.1414\n",
            "Epoch 322/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1485 - val_loss: 0.1405\n",
            "Epoch 323/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1458 - val_loss: 0.1419\n",
            "Epoch 324/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1478 - val_loss: 0.1417\n",
            "Epoch 325/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1494 - val_loss: 0.1418\n",
            "Epoch 326/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1468 - val_loss: 0.1412\n",
            "Epoch 327/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1474 - val_loss: 0.1415\n",
            "Epoch 328/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1524 - val_loss: 0.1411\n",
            "Epoch 329/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1486 - val_loss: 0.1408\n",
            "Epoch 330/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1494 - val_loss: 0.1406\n",
            "Epoch 331/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1470 - val_loss: 0.1405\n",
            "Epoch 332/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1483 - val_loss: 0.1406\n",
            "Epoch 333/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1473 - val_loss: 0.1413\n",
            "Epoch 334/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1458 - val_loss: 0.1399\n",
            "Epoch 335/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1451 - val_loss: 0.1402\n",
            "Epoch 336/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1486 - val_loss: 0.1395\n",
            "Epoch 337/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1443 - val_loss: 0.1405\n",
            "Epoch 338/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1475 - val_loss: 0.1401\n",
            "Epoch 339/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1493 - val_loss: 0.1410\n",
            "Epoch 340/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1471 - val_loss: 0.1406\n",
            "Epoch 341/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1500 - val_loss: 0.1418\n",
            "Epoch 342/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1471 - val_loss: 0.1411\n",
            "Epoch 343/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1483 - val_loss: 0.1414\n",
            "Epoch 344/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1467 - val_loss: 0.1405\n",
            "Epoch 345/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1482 - val_loss: 0.1417\n",
            "Epoch 346/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1502 - val_loss: 0.1406\n",
            "Epoch 347/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1470 - val_loss: 0.1408\n",
            "Epoch 348/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1471 - val_loss: 0.1405\n",
            "Epoch 349/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1448 - val_loss: 0.1407\n",
            "Epoch 350/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1494 - val_loss: 0.1402\n",
            "Epoch 351/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1461 - val_loss: 0.1393\n",
            "Epoch 352/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1470 - val_loss: 0.1390\n",
            "Epoch 353/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1458 - val_loss: 0.1391\n",
            "Epoch 354/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1461 - val_loss: 0.1385\n",
            "Epoch 355/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1451 - val_loss: 0.1386\n",
            "Epoch 356/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1465 - val_loss: 0.1383\n",
            "Epoch 357/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1464 - val_loss: 0.1385\n",
            "Epoch 358/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1478 - val_loss: 0.1379\n",
            "Epoch 359/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1467 - val_loss: 0.1380\n",
            "Epoch 360/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1448 - val_loss: 0.1373\n",
            "Epoch 361/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1432 - val_loss: 0.1370\n",
            "Epoch 362/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1434 - val_loss: 0.1372\n",
            "Epoch 363/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1469 - val_loss: 0.1373\n",
            "Epoch 364/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1447 - val_loss: 0.1382\n",
            "Epoch 365/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1442 - val_loss: 0.1377\n",
            "Epoch 366/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1458 - val_loss: 0.1375\n",
            "Epoch 367/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1446 - val_loss: 0.1371\n",
            "Epoch 368/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1447 - val_loss: 0.1375\n",
            "Epoch 369/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1440 - val_loss: 0.1381\n",
            "Epoch 370/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1455 - val_loss: 0.1381\n",
            "Epoch 371/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1459 - val_loss: 0.1391\n",
            "Epoch 372/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1433 - val_loss: 0.1393\n",
            "Epoch 373/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1476 - val_loss: 0.1403\n",
            "Epoch 374/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1472 - val_loss: 0.1393\n",
            "Epoch 375/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1444 - val_loss: 0.1393\n",
            "Epoch 376/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1476 - val_loss: 0.1390\n",
            "Epoch 377/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1461 - val_loss: 0.1403\n",
            "Epoch 378/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1478 - val_loss: 0.1396\n",
            "Epoch 379/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1472 - val_loss: 0.1401\n",
            "Epoch 380/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1478 - val_loss: 0.1394\n",
            "Epoch 381/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1463 - val_loss: 0.1395\n",
            "Epoch 382/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1454 - val_loss: 0.1385\n",
            "Epoch 383/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1463 - val_loss: 0.1400\n",
            "Epoch 384/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1452 - val_loss: 0.1392\n",
            "Epoch 385/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1449 - val_loss: 0.1405\n",
            "Epoch 386/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1458 - val_loss: 0.1385\n",
            "Epoch 387/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1438 - val_loss: 0.1387\n",
            "Epoch 388/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1464 - val_loss: 0.1391\n",
            "Epoch 389/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1433 - val_loss: 0.1392\n",
            "Epoch 390/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1473 - val_loss: 0.1392\n",
            "Epoch 391/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1464 - val_loss: 0.1400\n",
            "Epoch 392/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1465 - val_loss: 0.1397\n",
            "Epoch 393/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1453 - val_loss: 0.1400\n",
            "Epoch 394/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1480 - val_loss: 0.1395\n",
            "Epoch 395/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1453 - val_loss: 0.1386\n",
            "Epoch 396/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1458 - val_loss: 0.1383\n",
            "Epoch 397/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1448 - val_loss: 0.1388\n",
            "Epoch 398/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1451 - val_loss: 0.1383\n",
            "Epoch 399/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1468 - val_loss: 0.1378\n",
            "Epoch 400/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1458 - val_loss: 0.1378\n",
            "Epoch 401/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1449 - val_loss: 0.1384\n",
            "Epoch 402/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1449 - val_loss: 0.1385\n",
            "Epoch 403/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1462 - val_loss: 0.1391\n",
            "Epoch 404/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1456 - val_loss: 0.1388\n",
            "Epoch 405/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1470 - val_loss: 0.1392\n",
            "Epoch 406/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1462 - val_loss: 0.1396\n",
            "Epoch 407/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1454 - val_loss: 0.1393\n",
            "Epoch 408/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1489 - val_loss: 0.1383\n",
            "Epoch 409/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1446 - val_loss: 0.1388\n",
            "Epoch 410/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1447 - val_loss: 0.1375\n",
            "Epoch 411/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1429 - val_loss: 0.1371\n",
            "Epoch 412/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1451 - val_loss: 0.1368\n",
            "Epoch 413/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1443 - val_loss: 0.1371\n",
            "Epoch 414/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1442 - val_loss: 0.1366\n",
            "Epoch 415/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1438 - val_loss: 0.1366\n",
            "Epoch 416/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1436 - val_loss: 0.1367\n",
            "Epoch 417/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1421 - val_loss: 0.1361\n",
            "Epoch 418/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1433 - val_loss: 0.1355\n",
            "Epoch 419/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1430 - val_loss: 0.1357\n",
            "Epoch 420/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1421 - val_loss: 0.1353\n",
            "Epoch 421/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1408 - val_loss: 0.1354\n",
            "Epoch 422/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1407 - val_loss: 0.1353\n",
            "Epoch 423/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1421 - val_loss: 0.1353\n",
            "Epoch 424/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1440 - val_loss: 0.1356\n",
            "Epoch 425/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1428 - val_loss: 0.1360\n",
            "Epoch 426/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1416 - val_loss: 0.1361\n",
            "Epoch 427/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1419 - val_loss: 0.1357\n",
            "Epoch 428/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1444 - val_loss: 0.1369\n",
            "Epoch 429/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1421 - val_loss: 0.1363\n",
            "Epoch 430/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1417 - val_loss: 0.1360\n",
            "Epoch 431/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1428 - val_loss: 0.1365\n",
            "Epoch 432/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1413 - val_loss: 0.1369\n",
            "Epoch 433/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1421 - val_loss: 0.1365\n",
            "Epoch 434/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1429 - val_loss: 0.1365\n",
            "Epoch 435/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1424 - val_loss: 0.1354\n",
            "Epoch 436/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1431 - val_loss: 0.1359\n",
            "Epoch 437/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1436 - val_loss: 0.1356\n",
            "Epoch 438/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1413 - val_loss: 0.1363\n",
            "Epoch 439/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1447 - val_loss: 0.1365\n",
            "Epoch 440/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1441 - val_loss: 0.1366\n",
            "Epoch 441/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1424 - val_loss: 0.1370\n",
            "Epoch 442/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1430 - val_loss: 0.1373\n",
            "Epoch 443/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1437 - val_loss: 0.1382\n",
            "Epoch 444/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1441 - val_loss: 0.1374\n",
            "Epoch 445/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1424 - val_loss: 0.1379\n",
            "Epoch 446/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1437 - val_loss: 0.1368\n",
            "Epoch 447/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1435 - val_loss: 0.1370\n",
            "Epoch 448/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1435 - val_loss: 0.1363\n",
            "Epoch 449/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1422 - val_loss: 0.1365\n",
            "Epoch 450/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1420 - val_loss: 0.1359\n",
            "Epoch 451/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1407 - val_loss: 0.1360\n",
            "Epoch 452/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1419 - val_loss: 0.1363\n",
            "Epoch 453/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1423 - val_loss: 0.1372\n",
            "Epoch 454/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1442 - val_loss: 0.1373\n",
            "Epoch 455/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1434 - val_loss: 0.1372\n",
            "Epoch 456/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1431 - val_loss: 0.1364\n",
            "Epoch 457/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1427 - val_loss: 0.1370\n",
            "Epoch 458/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1419 - val_loss: 0.1374\n",
            "Epoch 459/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1429 - val_loss: 0.1372\n",
            "Epoch 460/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1427 - val_loss: 0.1371\n",
            "Epoch 461/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1426 - val_loss: 0.1354\n",
            "Epoch 462/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1419 - val_loss: 0.1355\n",
            "Epoch 463/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1390 - val_loss: 0.1350\n",
            "Epoch 464/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1417 - val_loss: 0.1354\n",
            "Epoch 465/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1405 - val_loss: 0.1356\n",
            "Epoch 466/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1414 - val_loss: 0.1358\n",
            "Epoch 467/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1399 - val_loss: 0.1350\n",
            "Epoch 468/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1408 - val_loss: 0.1361\n",
            "Epoch 469/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1407 - val_loss: 0.1357\n",
            "Epoch 470/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1434 - val_loss: 0.1369\n",
            "Epoch 471/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1425 - val_loss: 0.1368\n",
            "Epoch 472/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1426 - val_loss: 0.1368\n",
            "Epoch 473/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1416 - val_loss: 0.1366\n",
            "Epoch 474/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1420 - val_loss: 0.1361\n",
            "Epoch 475/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1439 - val_loss: 0.1359\n",
            "Epoch 476/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1418 - val_loss: 0.1360\n",
            "Epoch 477/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1403 - val_loss: 0.1358\n",
            "Epoch 478/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1423 - val_loss: 0.1353\n",
            "Epoch 479/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1404 - val_loss: 0.1344\n",
            "Epoch 480/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1410 - val_loss: 0.1347\n",
            "Epoch 481/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1421 - val_loss: 0.1342\n",
            "Epoch 482/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1408 - val_loss: 0.1351\n",
            "Epoch 483/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1420 - val_loss: 0.1344\n",
            "Epoch 484/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1418 - val_loss: 0.1347\n",
            "Epoch 485/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1377 - val_loss: 0.1343\n",
            "Epoch 486/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1403 - val_loss: 0.1347\n",
            "Epoch 487/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1402 - val_loss: 0.1341\n",
            "Epoch 488/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1404 - val_loss: 0.1345\n",
            "Epoch 489/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1408 - val_loss: 0.1347\n",
            "Epoch 490/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1401 - val_loss: 0.1352\n",
            "Epoch 491/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1416 - val_loss: 0.1358\n",
            "Epoch 492/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1403 - val_loss: 0.1365\n",
            "Epoch 493/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1412 - val_loss: 0.1365\n",
            "Epoch 494/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1428 - val_loss: 0.1355\n",
            "Epoch 495/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1424 - val_loss: 0.1365\n",
            "Epoch 496/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1436 - val_loss: 0.1360\n",
            "Epoch 497/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1413 - val_loss: 0.1356\n",
            "Epoch 498/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1415 - val_loss: 0.1343\n",
            "Epoch 499/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1407 - val_loss: 0.1352\n",
            "Epoch 500/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1417 - val_loss: 0.1353\n",
            "Epoch 501/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1406 - val_loss: 0.1359\n",
            "Epoch 502/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1423 - val_loss: 0.1365\n",
            "Epoch 503/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1423 - val_loss: 0.1366\n",
            "Epoch 504/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1429 - val_loss: 0.1354\n",
            "Epoch 505/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1410 - val_loss: 0.1350\n",
            "Epoch 506/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1409 - val_loss: 0.1342\n",
            "Epoch 507/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1402 - val_loss: 0.1340\n",
            "Epoch 508/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1397 - val_loss: 0.1339\n",
            "Epoch 509/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1375 - val_loss: 0.1342\n",
            "Epoch 510/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1394 - val_loss: 0.1352\n",
            "Epoch 511/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1404 - val_loss: 0.1345\n",
            "Epoch 512/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1413 - val_loss: 0.1353\n",
            "Epoch 513/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1419 - val_loss: 0.1346\n",
            "Epoch 514/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1388 - val_loss: 0.1344\n",
            "Epoch 515/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1405 - val_loss: 0.1337\n",
            "Epoch 516/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1410 - val_loss: 0.1342\n",
            "Epoch 517/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1403 - val_loss: 0.1330\n",
            "Epoch 518/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1376 - val_loss: 0.1326\n",
            "Epoch 519/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1372 - val_loss: 0.1313\n",
            "Epoch 520/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1373 - val_loss: 0.1321\n",
            "Epoch 521/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1376 - val_loss: 0.1312\n",
            "Epoch 522/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1387 - val_loss: 0.1324\n",
            "Epoch 523/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1354 - val_loss: 0.1306\n",
            "Epoch 524/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1380 - val_loss: 0.1321\n",
            "Epoch 525/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1368 - val_loss: 0.1309\n",
            "Epoch 526/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1377 - val_loss: 0.1324\n",
            "Epoch 527/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1372 - val_loss: 0.1314\n",
            "Epoch 528/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1368 - val_loss: 0.1324\n",
            "Epoch 529/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1363 - val_loss: 0.1311\n",
            "Epoch 530/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1390 - val_loss: 0.1327\n",
            "Epoch 531/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1366 - val_loss: 0.1313\n",
            "Epoch 532/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1378 - val_loss: 0.1326\n",
            "Epoch 533/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1386 - val_loss: 0.1313\n",
            "Epoch 534/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1375 - val_loss: 0.1325\n",
            "Epoch 535/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1358 - val_loss: 0.1320\n",
            "Epoch 536/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1364 - val_loss: 0.1322\n",
            "Epoch 537/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1373 - val_loss: 0.1317\n",
            "Epoch 538/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1395 - val_loss: 0.1324\n",
            "Epoch 539/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1390 - val_loss: 0.1326\n",
            "Epoch 540/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1374 - val_loss: 0.1325\n",
            "Epoch 541/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1389 - val_loss: 0.1330\n",
            "Epoch 542/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1383 - val_loss: 0.1330\n",
            "Epoch 543/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1404 - val_loss: 0.1325\n",
            "Epoch 544/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1385 - val_loss: 0.1331\n",
            "Epoch 545/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1384 - val_loss: 0.1332\n",
            "Epoch 546/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1404 - val_loss: 0.1332\n",
            "Epoch 547/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1371 - val_loss: 0.1324\n",
            "Epoch 548/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1379 - val_loss: 0.1336\n",
            "Epoch 549/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1377 - val_loss: 0.1326\n",
            "Epoch 550/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1388 - val_loss: 0.1335\n",
            "Epoch 551/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1368 - val_loss: 0.1332\n",
            "Epoch 552/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1407 - val_loss: 0.1343\n",
            "Epoch 553/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1369 - val_loss: 0.1350\n",
            "Epoch 554/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1411 - val_loss: 0.1348\n",
            "Epoch 555/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1379 - val_loss: 0.1352\n",
            "Epoch 556/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1409 - val_loss: 0.1365\n",
            "Epoch 557/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1395 - val_loss: 0.1354\n",
            "Epoch 558/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1409 - val_loss: 0.1360\n",
            "Epoch 559/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1424 - val_loss: 0.1357\n",
            "Epoch 560/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1406 - val_loss: 0.1366\n",
            "Epoch 561/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1413 - val_loss: 0.1357\n",
            "Epoch 562/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1409 - val_loss: 0.1353\n",
            "Epoch 563/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1382 - val_loss: 0.1338\n",
            "Epoch 564/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1390 - val_loss: 0.1347\n",
            "Epoch 565/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1408 - val_loss: 0.1340\n",
            "Epoch 566/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1396 - val_loss: 0.1340\n",
            "Epoch 567/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1390 - val_loss: 0.1326\n",
            "Epoch 568/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1403 - val_loss: 0.1337\n",
            "Epoch 569/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1373 - val_loss: 0.1314\n",
            "Epoch 570/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1385 - val_loss: 0.1333\n",
            "Epoch 571/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1372 - val_loss: 0.1313\n",
            "Epoch 572/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1398 - val_loss: 0.1328\n",
            "Epoch 573/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1374 - val_loss: 0.1304\n",
            "Epoch 574/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1380 - val_loss: 0.1319\n",
            "Epoch 575/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1347 - val_loss: 0.1292\n",
            "Epoch 576/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1354 - val_loss: 0.1309\n",
            "Epoch 577/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1351 - val_loss: 0.1290\n",
            "Epoch 578/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1354 - val_loss: 0.1305\n",
            "Epoch 579/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1339 - val_loss: 0.1286\n",
            "Epoch 580/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1342 - val_loss: 0.1304\n",
            "Epoch 581/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1353 - val_loss: 0.1287\n",
            "Epoch 582/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1333 - val_loss: 0.1309\n",
            "Epoch 583/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1361 - val_loss: 0.1293\n",
            "Epoch 584/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1351 - val_loss: 0.1314\n",
            "Epoch 585/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1356 - val_loss: 0.1300\n",
            "Epoch 586/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1351 - val_loss: 0.1320\n",
            "Epoch 587/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1362 - val_loss: 0.1307\n",
            "Epoch 588/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1347 - val_loss: 0.1326\n",
            "Epoch 589/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1356 - val_loss: 0.1308\n",
            "Epoch 590/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1368 - val_loss: 0.1328\n",
            "Epoch 591/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1359 - val_loss: 0.1313\n",
            "Epoch 592/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1375 - val_loss: 0.1329\n",
            "Epoch 593/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1381 - val_loss: 0.1313\n",
            "Epoch 594/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1396 - val_loss: 0.1332\n",
            "Epoch 595/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1387 - val_loss: 0.1323\n",
            "Epoch 596/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1398 - val_loss: 0.1330\n",
            "Epoch 597/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1393 - val_loss: 0.1318\n",
            "Epoch 598/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1373 - val_loss: 0.1327\n",
            "Epoch 599/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1376 - val_loss: 0.1325\n",
            "Epoch 600/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1391 - val_loss: 0.1331\n",
            "Epoch 601/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1381 - val_loss: 0.1325\n",
            "Epoch 602/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1402 - val_loss: 0.1335\n",
            "Epoch 603/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1368 - val_loss: 0.1330\n",
            "Epoch 604/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1406 - val_loss: 0.1334\n",
            "Epoch 605/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1370 - val_loss: 0.1327\n",
            "Epoch 606/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1375 - val_loss: 0.1337\n",
            "Epoch 607/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1375 - val_loss: 0.1331\n",
            "Epoch 608/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1377 - val_loss: 0.1328\n",
            "Epoch 609/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1371 - val_loss: 0.1324\n",
            "Epoch 610/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1379 - val_loss: 0.1322\n",
            "Epoch 611/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1377 - val_loss: 0.1319\n",
            "Epoch 612/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1402 - val_loss: 0.1322\n",
            "Epoch 613/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1367 - val_loss: 0.1323\n",
            "Epoch 614/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1367 - val_loss: 0.1322\n",
            "Epoch 615/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1380 - val_loss: 0.1321\n",
            "Epoch 616/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1357 - val_loss: 0.1316\n",
            "Epoch 617/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1360 - val_loss: 0.1320\n",
            "Epoch 618/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1351 - val_loss: 0.1315\n",
            "Epoch 619/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1371 - val_loss: 0.1314\n",
            "Epoch 620/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1368 - val_loss: 0.1306\n",
            "Epoch 621/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1346 - val_loss: 0.1310\n",
            "Epoch 622/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1377 - val_loss: 0.1298\n",
            "Epoch 623/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1337 - val_loss: 0.1306\n",
            "Epoch 624/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1354 - val_loss: 0.1294\n",
            "Epoch 625/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1346 - val_loss: 0.1296\n",
            "Epoch 626/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1357 - val_loss: 0.1291\n",
            "Epoch 627/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1318 - val_loss: 0.1293\n",
            "Epoch 628/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1341 - val_loss: 0.1286\n",
            "Epoch 629/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1339 - val_loss: 0.1294\n",
            "Epoch 630/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1347 - val_loss: 0.1292\n",
            "Epoch 631/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1349 - val_loss: 0.1296\n",
            "Epoch 632/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1343 - val_loss: 0.1287\n",
            "Epoch 633/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1345 - val_loss: 0.1285\n",
            "Epoch 634/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1331 - val_loss: 0.1285\n",
            "Epoch 635/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1332 - val_loss: 0.1282\n",
            "Epoch 636/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1311 - val_loss: 0.1280\n",
            "Epoch 637/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1325 - val_loss: 0.1275\n",
            "Epoch 638/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1323 - val_loss: 0.1281\n",
            "Epoch 639/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1324 - val_loss: 0.1272\n",
            "Epoch 640/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1353 - val_loss: 0.1279\n",
            "Epoch 641/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1316 - val_loss: 0.1273\n",
            "Epoch 642/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1330 - val_loss: 0.1291\n",
            "Epoch 643/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1335 - val_loss: 0.1280\n",
            "Epoch 644/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1303 - val_loss: 0.1293\n",
            "Epoch 645/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1325 - val_loss: 0.1282\n",
            "Epoch 646/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1325 - val_loss: 0.1297\n",
            "Epoch 647/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1332 - val_loss: 0.1289\n",
            "Epoch 648/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1339 - val_loss: 0.1305\n",
            "Epoch 649/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1357 - val_loss: 0.1302\n",
            "Epoch 650/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1370 - val_loss: 0.1322\n",
            "Epoch 651/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1368 - val_loss: 0.1311\n",
            "Epoch 652/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1378 - val_loss: 0.1324\n",
            "Epoch 653/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1359 - val_loss: 0.1302\n",
            "Epoch 654/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1360 - val_loss: 0.1325\n",
            "Epoch 655/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1349 - val_loss: 0.1301\n",
            "Epoch 656/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1377 - val_loss: 0.1323\n",
            "Epoch 657/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1358 - val_loss: 0.1297\n",
            "Epoch 658/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1363 - val_loss: 0.1313\n",
            "Epoch 659/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1359 - val_loss: 0.1300\n",
            "Epoch 660/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1356 - val_loss: 0.1316\n",
            "Epoch 661/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1350 - val_loss: 0.1290\n",
            "Epoch 662/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1351 - val_loss: 0.1310\n",
            "Epoch 663/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1344 - val_loss: 0.1284\n",
            "Epoch 664/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1337 - val_loss: 0.1304\n",
            "Epoch 665/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1348 - val_loss: 0.1278\n",
            "Epoch 666/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1321 - val_loss: 0.1294\n",
            "Epoch 667/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1332 - val_loss: 0.1263\n",
            "Epoch 668/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1314 - val_loss: 0.1284\n",
            "Epoch 669/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1318 - val_loss: 0.1265\n",
            "Epoch 670/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1346 - val_loss: 0.1287\n",
            "Epoch 671/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1319 - val_loss: 0.1269\n",
            "Epoch 672/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1343 - val_loss: 0.1291\n",
            "Epoch 673/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1316 - val_loss: 0.1270\n",
            "Epoch 674/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1341 - val_loss: 0.1298\n",
            "Epoch 675/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1305 - val_loss: 0.1273\n",
            "Epoch 676/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1340 - val_loss: 0.1297\n",
            "Epoch 677/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1325 - val_loss: 0.1276\n",
            "Epoch 678/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1337 - val_loss: 0.1302\n",
            "Epoch 679/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1337 - val_loss: 0.1271\n",
            "Epoch 680/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1321 - val_loss: 0.1306\n",
            "Epoch 681/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1335 - val_loss: 0.1275\n",
            "Epoch 682/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1331 - val_loss: 0.1297\n",
            "Epoch 683/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1330 - val_loss: 0.1269\n",
            "Epoch 684/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1332 - val_loss: 0.1292\n",
            "Epoch 685/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1342 - val_loss: 0.1269\n",
            "Epoch 686/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1320 - val_loss: 0.1293\n",
            "Epoch 687/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1310 - val_loss: 0.1272\n",
            "Epoch 688/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1328 - val_loss: 0.1292\n",
            "Epoch 689/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1320 - val_loss: 0.1268\n",
            "Epoch 690/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1339 - val_loss: 0.1293\n",
            "Epoch 691/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1349 - val_loss: 0.1271\n",
            "Epoch 692/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1328 - val_loss: 0.1282\n",
            "Epoch 693/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1327 - val_loss: 0.1262\n",
            "Epoch 694/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1326 - val_loss: 0.1274\n",
            "Epoch 695/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1303 - val_loss: 0.1269\n",
            "Epoch 696/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1322 - val_loss: 0.1279\n",
            "Epoch 697/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1310 - val_loss: 0.1277\n",
            "Epoch 698/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1323 - val_loss: 0.1284\n",
            "Epoch 699/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1315 - val_loss: 0.1288\n",
            "Epoch 700/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1346 - val_loss: 0.1299\n",
            "Epoch 701/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1334 - val_loss: 0.1304\n",
            "Epoch 702/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1352 - val_loss: 0.1318\n",
            "Epoch 703/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1342 - val_loss: 0.1310\n",
            "Epoch 704/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1350 - val_loss: 0.1315\n",
            "Epoch 705/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1345 - val_loss: 0.1321\n",
            "Epoch 706/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1362 - val_loss: 0.1321\n",
            "Epoch 707/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1334 - val_loss: 0.1309\n",
            "Epoch 708/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1342 - val_loss: 0.1309\n",
            "Epoch 709/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1346 - val_loss: 0.1307\n",
            "Epoch 710/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1356 - val_loss: 0.1306\n",
            "Epoch 711/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1350 - val_loss: 0.1290\n",
            "Epoch 712/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1340 - val_loss: 0.1299\n",
            "Epoch 713/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1326 - val_loss: 0.1289\n",
            "Epoch 714/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1351 - val_loss: 0.1293\n",
            "Epoch 715/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1329 - val_loss: 0.1280\n",
            "Epoch 716/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1310 - val_loss: 0.1281\n",
            "Epoch 717/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1320 - val_loss: 0.1278\n",
            "Epoch 718/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1340 - val_loss: 0.1271\n",
            "Epoch 719/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1305 - val_loss: 0.1258\n",
            "Epoch 720/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1296 - val_loss: 0.1260\n",
            "Epoch 721/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1307 - val_loss: 0.1255\n",
            "Epoch 722/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1304 - val_loss: 0.1261\n",
            "Epoch 723/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1310 - val_loss: 0.1257\n",
            "Epoch 724/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1314 - val_loss: 0.1262\n",
            "Epoch 725/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1290 - val_loss: 0.1248\n",
            "Epoch 726/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1305 - val_loss: 0.1251\n",
            "Epoch 727/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1286 - val_loss: 0.1250\n",
            "Epoch 728/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1298 - val_loss: 0.1255\n",
            "Epoch 729/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1285 - val_loss: 0.1256\n",
            "Epoch 730/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1293 - val_loss: 0.1263\n",
            "Epoch 731/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1304 - val_loss: 0.1259\n",
            "Epoch 732/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1325 - val_loss: 0.1267\n",
            "Epoch 733/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1315 - val_loss: 0.1266\n",
            "Epoch 734/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1316 - val_loss: 0.1272\n",
            "Epoch 735/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1305 - val_loss: 0.1264\n",
            "Epoch 736/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1315 - val_loss: 0.1271\n",
            "Epoch 737/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1307 - val_loss: 0.1271\n",
            "Epoch 738/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1295 - val_loss: 0.1275\n",
            "Epoch 739/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1336 - val_loss: 0.1268\n",
            "Epoch 740/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1296 - val_loss: 0.1275\n",
            "Epoch 741/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1319 - val_loss: 0.1270\n",
            "Epoch 742/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1299 - val_loss: 0.1268\n",
            "Epoch 743/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1299 - val_loss: 0.1261\n",
            "Epoch 744/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1294 - val_loss: 0.1260\n",
            "Epoch 745/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1310 - val_loss: 0.1253\n",
            "Epoch 746/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1302 - val_loss: 0.1263\n",
            "Epoch 747/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1302 - val_loss: 0.1261\n",
            "Epoch 748/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1321 - val_loss: 0.1268\n",
            "Epoch 749/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1295 - val_loss: 0.1253\n",
            "Epoch 750/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1310 - val_loss: 0.1263\n",
            "Epoch 751/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1314 - val_loss: 0.1258\n",
            "Epoch 752/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1306 - val_loss: 0.1262\n",
            "Epoch 753/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1304 - val_loss: 0.1252\n",
            "Epoch 754/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1328 - val_loss: 0.1258\n",
            "Epoch 755/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1311 - val_loss: 0.1258\n",
            "Epoch 756/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1322 - val_loss: 0.1264\n",
            "Epoch 757/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1291 - val_loss: 0.1250\n",
            "Epoch 758/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1289 - val_loss: 0.1258\n",
            "Epoch 759/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1289 - val_loss: 0.1244\n",
            "Epoch 760/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1310 - val_loss: 0.1265\n",
            "Epoch 761/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1325 - val_loss: 0.1246\n",
            "Epoch 762/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1311 - val_loss: 0.1272\n",
            "Epoch 763/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1284 - val_loss: 0.1257\n",
            "Epoch 764/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1315 - val_loss: 0.1293\n",
            "Epoch 765/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1323 - val_loss: 0.1273\n",
            "Epoch 766/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1341 - val_loss: 0.1302\n",
            "Epoch 767/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1322 - val_loss: 0.1270\n",
            "Epoch 768/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1343 - val_loss: 0.1302\n",
            "Epoch 769/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1330 - val_loss: 0.1262\n",
            "Epoch 770/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1313 - val_loss: 0.1298\n",
            "Epoch 771/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1331 - val_loss: 0.1267\n",
            "Epoch 772/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1341 - val_loss: 0.1300\n",
            "Epoch 773/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1326 - val_loss: 0.1262\n",
            "Epoch 774/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1330 - val_loss: 0.1294\n",
            "Epoch 775/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1313 - val_loss: 0.1252\n",
            "Epoch 776/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1312 - val_loss: 0.1287\n",
            "Epoch 777/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1308 - val_loss: 0.1256\n",
            "Epoch 778/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1328 - val_loss: 0.1287\n",
            "Epoch 779/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1296 - val_loss: 0.1246\n",
            "Epoch 780/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1312 - val_loss: 0.1278\n",
            "Epoch 781/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1293 - val_loss: 0.1248\n",
            "Epoch 782/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1308 - val_loss: 0.1270\n",
            "Epoch 783/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1289 - val_loss: 0.1239\n",
            "Epoch 784/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1303 - val_loss: 0.1257\n",
            "Epoch 785/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1287 - val_loss: 0.1231\n",
            "Epoch 786/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1281 - val_loss: 0.1252\n",
            "Epoch 787/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1287 - val_loss: 0.1223\n",
            "Epoch 788/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1297 - val_loss: 0.1251\n",
            "Epoch 789/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1278 - val_loss: 0.1223\n",
            "Epoch 790/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1269 - val_loss: 0.1251\n",
            "Epoch 791/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1276 - val_loss: 0.1226\n",
            "Epoch 792/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1282 - val_loss: 0.1253\n",
            "Epoch 793/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1300 - val_loss: 0.1228\n",
            "Epoch 794/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1288 - val_loss: 0.1256\n",
            "Epoch 795/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1294 - val_loss: 0.1228\n",
            "Epoch 796/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1277 - val_loss: 0.1252\n",
            "Epoch 797/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1288 - val_loss: 0.1233\n",
            "Epoch 798/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1303 - val_loss: 0.1262\n",
            "Epoch 799/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1295 - val_loss: 0.1239\n",
            "Epoch 800/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1325 - val_loss: 0.1259\n",
            "Epoch 801/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1301 - val_loss: 0.1235\n",
            "Epoch 802/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1301 - val_loss: 0.1273\n",
            "Epoch 803/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1292 - val_loss: 0.1232\n",
            "Epoch 804/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1290 - val_loss: 0.1264\n",
            "Epoch 805/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1294 - val_loss: 0.1235\n",
            "Epoch 806/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1274 - val_loss: 0.1263\n",
            "Epoch 807/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1289 - val_loss: 0.1227\n",
            "Epoch 808/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1304 - val_loss: 0.1263\n",
            "Epoch 809/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1292 - val_loss: 0.1229\n",
            "Epoch 810/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1285 - val_loss: 0.1263\n",
            "Epoch 811/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1286 - val_loss: 0.1230\n",
            "Epoch 812/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1297 - val_loss: 0.1262\n",
            "Epoch 813/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1305 - val_loss: 0.1233\n",
            "Epoch 814/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1300 - val_loss: 0.1264\n",
            "Epoch 815/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1299 - val_loss: 0.1225\n",
            "Epoch 816/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1288 - val_loss: 0.1254\n",
            "Epoch 817/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1286 - val_loss: 0.1229\n",
            "Epoch 818/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1296 - val_loss: 0.1267\n",
            "Epoch 819/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1295 - val_loss: 0.1230\n",
            "Epoch 820/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1283 - val_loss: 0.1266\n",
            "Epoch 821/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1298 - val_loss: 0.1228\n",
            "Epoch 822/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1274 - val_loss: 0.1246\n",
            "Epoch 823/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1274 - val_loss: 0.1214\n",
            "Epoch 824/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1284 - val_loss: 0.1235\n",
            "Epoch 825/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1277 - val_loss: 0.1207\n",
            "Epoch 826/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1248 - val_loss: 0.1234\n",
            "Epoch 827/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1258 - val_loss: 0.1205\n",
            "Epoch 828/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1249 - val_loss: 0.1222\n",
            "Epoch 829/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1248 - val_loss: 0.1205\n",
            "Epoch 830/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1263 - val_loss: 0.1220\n",
            "Epoch 831/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1268 - val_loss: 0.1201\n",
            "Epoch 832/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1252 - val_loss: 0.1222\n",
            "Epoch 833/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1261 - val_loss: 0.1203\n",
            "Epoch 834/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1249 - val_loss: 0.1227\n",
            "Epoch 835/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1273 - val_loss: 0.1209\n",
            "Epoch 836/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1242 - val_loss: 0.1227\n",
            "Epoch 837/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1263 - val_loss: 0.1217\n",
            "Epoch 838/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1270 - val_loss: 0.1228\n",
            "Epoch 839/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1255 - val_loss: 0.1218\n",
            "Epoch 840/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1264 - val_loss: 0.1233\n",
            "Epoch 841/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1262 - val_loss: 0.1232\n",
            "Epoch 842/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1268 - val_loss: 0.1239\n",
            "Epoch 843/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1281 - val_loss: 0.1244\n",
            "Epoch 844/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1294 - val_loss: 0.1261\n",
            "Epoch 845/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1286 - val_loss: 0.1263\n",
            "Epoch 846/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1318 - val_loss: 0.1274\n",
            "Epoch 847/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1302 - val_loss: 0.1277\n",
            "Epoch 848/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1322 - val_loss: 0.1281\n",
            "Epoch 849/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1329 - val_loss: 0.1280\n",
            "Epoch 850/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1325 - val_loss: 0.1278\n",
            "Epoch 851/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1305 - val_loss: 0.1286\n",
            "Epoch 852/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1305 - val_loss: 0.1271\n",
            "Epoch 853/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1306 - val_loss: 0.1273\n",
            "Epoch 854/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1314 - val_loss: 0.1255\n",
            "Epoch 855/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1296 - val_loss: 0.1262\n",
            "Epoch 856/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1284 - val_loss: 0.1243\n",
            "Epoch 857/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1297 - val_loss: 0.1260\n",
            "Epoch 858/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1289 - val_loss: 0.1242\n",
            "Epoch 859/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1298 - val_loss: 0.1253\n",
            "Epoch 860/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1288 - val_loss: 0.1230\n",
            "Epoch 861/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1269 - val_loss: 0.1247\n",
            "Epoch 862/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1285 - val_loss: 0.1224\n",
            "Epoch 863/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1275 - val_loss: 0.1247\n",
            "Epoch 864/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1265 - val_loss: 0.1221\n",
            "Epoch 865/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1280 - val_loss: 0.1240\n",
            "Epoch 866/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1284 - val_loss: 0.1224\n",
            "Epoch 867/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1298 - val_loss: 0.1242\n",
            "Epoch 868/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1265 - val_loss: 0.1225\n",
            "Epoch 869/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1298 - val_loss: 0.1248\n",
            "Epoch 870/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1280 - val_loss: 0.1232\n",
            "Epoch 871/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1279 - val_loss: 0.1254\n",
            "Epoch 872/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1283 - val_loss: 0.1231\n",
            "Epoch 873/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1294 - val_loss: 0.1247\n",
            "Epoch 874/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1275 - val_loss: 0.1232\n",
            "Epoch 875/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1296 - val_loss: 0.1238\n",
            "Epoch 876/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1275 - val_loss: 0.1227\n",
            "Epoch 877/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1287 - val_loss: 0.1231\n",
            "Epoch 878/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1277 - val_loss: 0.1216\n",
            "Epoch 879/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1271 - val_loss: 0.1224\n",
            "Epoch 880/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1254 - val_loss: 0.1212\n",
            "Epoch 881/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1261 - val_loss: 0.1220\n",
            "Epoch 882/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1258 - val_loss: 0.1203\n",
            "Epoch 883/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1240 - val_loss: 0.1216\n",
            "Epoch 884/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1243 - val_loss: 0.1198\n",
            "Epoch 885/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1253 - val_loss: 0.1213\n",
            "Epoch 886/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1239 - val_loss: 0.1197\n",
            "Epoch 887/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1269 - val_loss: 0.1215\n",
            "Epoch 888/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1238 - val_loss: 0.1192\n",
            "Epoch 889/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1240 - val_loss: 0.1209\n",
            "Epoch 890/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1242 - val_loss: 0.1191\n",
            "Epoch 891/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1242 - val_loss: 0.1214\n",
            "Epoch 892/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1230 - val_loss: 0.1195\n",
            "Epoch 893/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1238 - val_loss: 0.1220\n",
            "Epoch 894/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1243 - val_loss: 0.1206\n",
            "Epoch 895/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1278 - val_loss: 0.1234\n",
            "Epoch 896/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1244 - val_loss: 0.1215\n",
            "Epoch 897/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1258 - val_loss: 0.1231\n",
            "Epoch 898/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1264 - val_loss: 0.1215\n",
            "Epoch 899/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1283 - val_loss: 0.1242\n",
            "Epoch 900/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1262 - val_loss: 0.1221\n",
            "Epoch 901/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1274 - val_loss: 0.1242\n",
            "Epoch 902/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1259 - val_loss: 0.1221\n",
            "Epoch 903/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1283 - val_loss: 0.1249\n",
            "Epoch 904/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1257 - val_loss: 0.1220\n",
            "Epoch 905/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1258 - val_loss: 0.1240\n",
            "Epoch 906/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1261 - val_loss: 0.1219\n",
            "Epoch 907/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1276 - val_loss: 0.1235\n",
            "Epoch 908/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1238 - val_loss: 0.1221\n",
            "Epoch 909/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1260 - val_loss: 0.1238\n",
            "Epoch 910/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1240 - val_loss: 0.1213\n",
            "Epoch 911/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1273 - val_loss: 0.1242\n",
            "Epoch 912/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1259 - val_loss: 0.1212\n",
            "Epoch 913/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1254 - val_loss: 0.1239\n",
            "Epoch 914/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1275 - val_loss: 0.1216\n",
            "Epoch 915/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1272 - val_loss: 0.1236\n",
            "Epoch 916/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1246 - val_loss: 0.1205\n",
            "Epoch 917/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1261 - val_loss: 0.1225\n",
            "Epoch 918/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1263 - val_loss: 0.1195\n",
            "Epoch 919/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1248 - val_loss: 0.1221\n",
            "Epoch 920/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1241 - val_loss: 0.1195\n",
            "Epoch 921/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1236 - val_loss: 0.1229\n",
            "Epoch 922/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1244 - val_loss: 0.1196\n",
            "Epoch 923/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1263 - val_loss: 0.1230\n",
            "Epoch 924/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1258 - val_loss: 0.1194\n",
            "Epoch 925/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1241 - val_loss: 0.1221\n",
            "Epoch 926/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1246 - val_loss: 0.1192\n",
            "Epoch 927/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1235 - val_loss: 0.1230\n",
            "Epoch 928/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1253 - val_loss: 0.1200\n",
            "Epoch 929/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1261 - val_loss: 0.1245\n",
            "Epoch 930/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1234 - val_loss: 0.1209\n",
            "Epoch 931/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1268 - val_loss: 0.1239\n",
            "Epoch 932/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1270 - val_loss: 0.1202\n",
            "Epoch 933/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1266 - val_loss: 0.1233\n",
            "Epoch 934/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1224 - val_loss: 0.1193\n",
            "Epoch 935/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1258 - val_loss: 0.1221\n",
            "Epoch 936/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1231 - val_loss: 0.1183\n",
            "Epoch 937/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1230 - val_loss: 0.1222\n",
            "Epoch 938/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1245 - val_loss: 0.1182\n",
            "Epoch 939/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1243 - val_loss: 0.1220\n",
            "Epoch 940/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1235 - val_loss: 0.1182\n",
            "Epoch 941/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1249 - val_loss: 0.1221\n",
            "Epoch 942/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1262 - val_loss: 0.1186\n",
            "Epoch 943/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1251 - val_loss: 0.1220\n",
            "Epoch 944/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1249 - val_loss: 0.1183\n",
            "Epoch 945/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1237 - val_loss: 0.1214\n",
            "Epoch 946/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1243 - val_loss: 0.1179\n",
            "Epoch 947/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1226 - val_loss: 0.1204\n",
            "Epoch 948/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1242 - val_loss: 0.1176\n",
            "Epoch 949/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1232 - val_loss: 0.1199\n",
            "Epoch 950/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1219 - val_loss: 0.1170\n",
            "Epoch 951/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1226 - val_loss: 0.1193\n",
            "Epoch 952/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1213 - val_loss: 0.1166\n",
            "Epoch 953/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1211 - val_loss: 0.1192\n",
            "Epoch 954/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1243 - val_loss: 0.1165\n",
            "Epoch 955/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1213 - val_loss: 0.1198\n",
            "Epoch 956/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1234 - val_loss: 0.1169\n",
            "Epoch 957/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1221 - val_loss: 0.1203\n",
            "Epoch 958/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1219 - val_loss: 0.1170\n",
            "Epoch 959/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1214 - val_loss: 0.1204\n",
            "Epoch 960/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1211 - val_loss: 0.1172\n",
            "Epoch 961/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1220 - val_loss: 0.1200\n",
            "Epoch 962/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1244 - val_loss: 0.1165\n",
            "Epoch 963/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1215 - val_loss: 0.1195\n",
            "Epoch 964/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1221 - val_loss: 0.1160\n",
            "Epoch 965/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1211 - val_loss: 0.1187\n",
            "Epoch 966/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1211 - val_loss: 0.1160\n",
            "Epoch 967/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1209 - val_loss: 0.1187\n",
            "Epoch 968/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1209 - val_loss: 0.1160\n",
            "Epoch 969/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1228 - val_loss: 0.1185\n",
            "Epoch 970/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1197 - val_loss: 0.1161\n",
            "Epoch 971/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1200 - val_loss: 0.1188\n",
            "Epoch 972/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1192 - val_loss: 0.1163\n",
            "Epoch 973/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1219 - val_loss: 0.1192\n",
            "Epoch 974/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1189 - val_loss: 0.1174\n",
            "Epoch 975/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1220 - val_loss: 0.1207\n",
            "Epoch 976/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1228 - val_loss: 0.1200\n",
            "Epoch 977/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1267 - val_loss: 0.1228\n",
            "Epoch 978/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1264 - val_loss: 0.1213\n",
            "Epoch 979/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1262 - val_loss: 0.1242\n",
            "Epoch 980/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1281 - val_loss: 0.1238\n",
            "Epoch 981/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1283 - val_loss: 0.1256\n",
            "Epoch 982/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1285 - val_loss: 0.1245\n",
            "Epoch 983/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1274 - val_loss: 0.1249\n",
            "Epoch 984/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1279 - val_loss: 0.1246\n",
            "Epoch 985/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1298 - val_loss: 0.1253\n",
            "Epoch 986/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1260 - val_loss: 0.1227\n",
            "Epoch 987/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1273 - val_loss: 0.1234\n",
            "Epoch 988/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1237 - val_loss: 0.1210\n",
            "Epoch 989/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1267 - val_loss: 0.1223\n",
            "Epoch 990/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1233 - val_loss: 0.1206\n",
            "Epoch 991/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1252 - val_loss: 0.1216\n",
            "Epoch 992/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1254 - val_loss: 0.1191\n",
            "Epoch 993/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1237 - val_loss: 0.1202\n",
            "Epoch 994/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1213 - val_loss: 0.1181\n",
            "Epoch 995/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1231 - val_loss: 0.1197\n",
            "Epoch 996/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1205 - val_loss: 0.1177\n",
            "Epoch 997/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1215 - val_loss: 0.1188\n",
            "Epoch 998/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1224 - val_loss: 0.1171\n",
            "Epoch 999/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1209 - val_loss: 0.1182\n",
            "Epoch 1000/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1215 - val_loss: 0.1167\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_7 (Bidirection (None, 5, 1024)           2146304   \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 5, 256)            1311744   \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 5, 1)              257       \n",
            "=================================================================\n",
            "Total params: 3,458,305\n",
            "Trainable params: 3,458,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 3298 samples, validate on 825 samples\n",
            "Epoch 1/1000\n",
            "3298/3298 [==============================] - 6s 2ms/step - loss: 0.4193 - val_loss: 0.3391\n",
            "Epoch 2/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3435 - val_loss: 0.2836\n",
            "Epoch 3/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2907 - val_loss: 0.2398\n",
            "Epoch 4/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.2471 - val_loss: 0.2045\n",
            "Epoch 5/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2128 - val_loss: 0.1766\n",
            "Epoch 6/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1840 - val_loss: 0.1566\n",
            "Epoch 7/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1649 - val_loss: 0.1435\n",
            "Epoch 8/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1488 - val_loss: 0.1359\n",
            "Epoch 9/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1435 - val_loss: 0.1313\n",
            "Epoch 10/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1393 - val_loss: 0.1306\n",
            "Epoch 11/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1400 - val_loss: 0.1343\n",
            "Epoch 12/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1435 - val_loss: 0.1377\n",
            "Epoch 13/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1468 - val_loss: 0.1419\n",
            "Epoch 14/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1516 - val_loss: 0.1417\n",
            "Epoch 15/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1510 - val_loss: 0.1409\n",
            "Epoch 16/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1518 - val_loss: 0.1351\n",
            "Epoch 17/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1473 - val_loss: 0.1330\n",
            "Epoch 18/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1422 - val_loss: 0.1276\n",
            "Epoch 19/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1382 - val_loss: 0.1245\n",
            "Epoch 20/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1340 - val_loss: 0.1202\n",
            "Epoch 21/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1310 - val_loss: 0.1181\n",
            "Epoch 22/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1275 - val_loss: 0.1140\n",
            "Epoch 23/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1251 - val_loss: 0.1127\n",
            "Epoch 24/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1221 - val_loss: 0.1098\n",
            "Epoch 25/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1231 - val_loss: 0.1088\n",
            "Epoch 26/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1204 - val_loss: 0.1066\n",
            "Epoch 27/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1165 - val_loss: 0.1055\n",
            "Epoch 28/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1165 - val_loss: 0.1042\n",
            "Epoch 29/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1153 - val_loss: 0.1033\n",
            "Epoch 30/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1156 - val_loss: 0.1022\n",
            "Epoch 31/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1139 - val_loss: 0.1020\n",
            "Epoch 32/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1112 - val_loss: 0.1012\n",
            "Epoch 33/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1124 - val_loss: 0.1009\n",
            "Epoch 34/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1122 - val_loss: 0.0991\n",
            "Epoch 35/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1100 - val_loss: 0.0988\n",
            "Epoch 36/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1108 - val_loss: 0.0975\n",
            "Epoch 37/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1097 - val_loss: 0.0972\n",
            "Epoch 38/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1084 - val_loss: 0.0959\n",
            "Epoch 39/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1101 - val_loss: 0.0957\n",
            "Epoch 40/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1093 - val_loss: 0.0947\n",
            "Epoch 41/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1077 - val_loss: 0.0945\n",
            "Epoch 42/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1069 - val_loss: 0.0935\n",
            "Epoch 43/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1066 - val_loss: 0.0934\n",
            "Epoch 44/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1075 - val_loss: 0.0930\n",
            "Epoch 45/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1027 - val_loss: 0.0929\n",
            "Epoch 46/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1057 - val_loss: 0.0919\n",
            "Epoch 47/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.1046 - val_loss: 0.0923\n",
            "Epoch 48/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1041 - val_loss: 0.0912\n",
            "Epoch 49/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1034 - val_loss: 0.0910\n",
            "Epoch 50/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1045 - val_loss: 0.0907\n",
            "Epoch 51/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1015 - val_loss: 0.0906\n",
            "Epoch 52/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1042 - val_loss: 0.0903\n",
            "Epoch 53/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1008 - val_loss: 0.0903\n",
            "Epoch 54/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1042 - val_loss: 0.0896\n",
            "Epoch 55/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1033 - val_loss: 0.0895\n",
            "Epoch 56/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1046 - val_loss: 0.0891\n",
            "Epoch 57/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1013 - val_loss: 0.0891\n",
            "Epoch 58/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1036 - val_loss: 0.0887\n",
            "Epoch 59/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1023 - val_loss: 0.0889\n",
            "Epoch 60/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1034 - val_loss: 0.0885\n",
            "Epoch 61/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1024 - val_loss: 0.0884\n",
            "Epoch 62/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1009 - val_loss: 0.0880\n",
            "Epoch 63/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0988 - val_loss: 0.0883\n",
            "Epoch 64/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0988 - val_loss: 0.0879\n",
            "Epoch 65/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1002 - val_loss: 0.0883\n",
            "Epoch 66/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1010 - val_loss: 0.0877\n",
            "Epoch 67/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0985 - val_loss: 0.0878\n",
            "Epoch 68/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0999 - val_loss: 0.0873\n",
            "Epoch 69/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0982 - val_loss: 0.0872\n",
            "Epoch 70/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0981 - val_loss: 0.0867\n",
            "Epoch 71/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0994 - val_loss: 0.0870\n",
            "Epoch 72/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0996 - val_loss: 0.0863\n",
            "Epoch 73/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0995 - val_loss: 0.0865\n",
            "Epoch 74/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1015 - val_loss: 0.0861\n",
            "Epoch 75/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0977 - val_loss: 0.0859\n",
            "Epoch 76/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0994 - val_loss: 0.0857\n",
            "Epoch 77/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1000 - val_loss: 0.0859\n",
            "Epoch 78/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0992 - val_loss: 0.0856\n",
            "Epoch 79/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0976 - val_loss: 0.0856\n",
            "Epoch 80/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0981 - val_loss: 0.0854\n",
            "Epoch 81/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0972 - val_loss: 0.0854\n",
            "Epoch 82/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0960 - val_loss: 0.0852\n",
            "Epoch 83/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0974 - val_loss: 0.0850\n",
            "Epoch 84/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0977 - val_loss: 0.0848\n",
            "Epoch 85/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1001 - val_loss: 0.0848\n",
            "Epoch 86/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0960 - val_loss: 0.0844\n",
            "Epoch 87/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0978 - val_loss: 0.0846\n",
            "Epoch 88/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0983 - val_loss: 0.0844\n",
            "Epoch 89/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0970 - val_loss: 0.0846\n",
            "Epoch 90/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0970 - val_loss: 0.0841\n",
            "Epoch 91/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0974 - val_loss: 0.0845\n",
            "Epoch 92/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0972 - val_loss: 0.0842\n",
            "Epoch 93/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0950 - val_loss: 0.0845\n",
            "Epoch 94/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0982 - val_loss: 0.0840\n",
            "Epoch 95/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0962 - val_loss: 0.0841\n",
            "Epoch 96/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0990 - val_loss: 0.0839\n",
            "Epoch 97/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0990 - val_loss: 0.0845\n",
            "Epoch 98/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0960 - val_loss: 0.0844\n",
            "Epoch 99/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0977 - val_loss: 0.0854\n",
            "Epoch 100/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0985 - val_loss: 0.0849\n",
            "Epoch 101/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0968 - val_loss: 0.0858\n",
            "Epoch 102/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0955 - val_loss: 0.0850\n",
            "Epoch 103/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0995 - val_loss: 0.0854\n",
            "Epoch 104/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0941 - val_loss: 0.0844\n",
            "Epoch 105/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0982 - val_loss: 0.0855\n",
            "Epoch 106/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0971 - val_loss: 0.0847\n",
            "Epoch 107/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0976 - val_loss: 0.0858\n",
            "Epoch 108/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0959 - val_loss: 0.0845\n",
            "Epoch 109/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0992 - val_loss: 0.0854\n",
            "Epoch 110/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0960 - val_loss: 0.0842\n",
            "Epoch 111/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0968 - val_loss: 0.0848\n",
            "Epoch 112/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0976 - val_loss: 0.0841\n",
            "Epoch 113/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0957 - val_loss: 0.0852\n",
            "Epoch 114/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0970 - val_loss: 0.0844\n",
            "Epoch 115/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0992 - val_loss: 0.0849\n",
            "Epoch 116/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0966 - val_loss: 0.0839\n",
            "Epoch 117/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0958 - val_loss: 0.0848\n",
            "Epoch 118/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0957 - val_loss: 0.0839\n",
            "Epoch 119/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0953 - val_loss: 0.0849\n",
            "Epoch 120/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0956 - val_loss: 0.0839\n",
            "Epoch 121/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0952 - val_loss: 0.0842\n",
            "Epoch 122/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0945 - val_loss: 0.0829\n",
            "Epoch 123/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0953 - val_loss: 0.0837\n",
            "Epoch 124/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0944 - val_loss: 0.0829\n",
            "Epoch 125/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0946 - val_loss: 0.0837\n",
            "Epoch 126/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0943 - val_loss: 0.0828\n",
            "Epoch 127/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0948 - val_loss: 0.0834\n",
            "Epoch 128/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0955 - val_loss: 0.0825\n",
            "Epoch 129/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0943 - val_loss: 0.0829\n",
            "Epoch 130/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0946 - val_loss: 0.0819\n",
            "Epoch 131/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0924 - val_loss: 0.0824\n",
            "Epoch 132/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0932 - val_loss: 0.0812\n",
            "Epoch 133/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0942 - val_loss: 0.0823\n",
            "Epoch 134/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0950 - val_loss: 0.0817\n",
            "Epoch 135/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0944 - val_loss: 0.0819\n",
            "Epoch 136/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0918 - val_loss: 0.0811\n",
            "Epoch 137/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0929 - val_loss: 0.0814\n",
            "Epoch 138/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0928 - val_loss: 0.0806\n",
            "Epoch 139/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0937 - val_loss: 0.0816\n",
            "Epoch 140/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0933 - val_loss: 0.0811\n",
            "Epoch 141/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0921 - val_loss: 0.0813\n",
            "Epoch 142/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0919 - val_loss: 0.0808\n",
            "Epoch 143/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0942 - val_loss: 0.0818\n",
            "Epoch 144/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0924 - val_loss: 0.0807\n",
            "Epoch 145/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0935 - val_loss: 0.0812\n",
            "Epoch 146/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0906 - val_loss: 0.0801\n",
            "Epoch 147/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0956 - val_loss: 0.0808\n",
            "Epoch 148/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0908 - val_loss: 0.0796\n",
            "Epoch 149/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0931 - val_loss: 0.0800\n",
            "Epoch 150/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0928 - val_loss: 0.0794\n",
            "Epoch 151/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0897 - val_loss: 0.0798\n",
            "Epoch 152/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0929 - val_loss: 0.0794\n",
            "Epoch 153/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0907 - val_loss: 0.0795\n",
            "Epoch 154/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0931 - val_loss: 0.0789\n",
            "Epoch 155/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0911 - val_loss: 0.0793\n",
            "Epoch 156/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0883 - val_loss: 0.0787\n",
            "Epoch 157/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0913 - val_loss: 0.0793\n",
            "Epoch 158/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0893 - val_loss: 0.0786\n",
            "Epoch 159/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0892 - val_loss: 0.0793\n",
            "Epoch 160/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0905 - val_loss: 0.0789\n",
            "Epoch 161/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0908 - val_loss: 0.0792\n",
            "Epoch 162/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0909 - val_loss: 0.0785\n",
            "Epoch 163/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0902 - val_loss: 0.0791\n",
            "Epoch 164/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0893 - val_loss: 0.0781\n",
            "Epoch 165/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0915 - val_loss: 0.0784\n",
            "Epoch 166/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0913 - val_loss: 0.0777\n",
            "Epoch 167/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0889 - val_loss: 0.0780\n",
            "Epoch 168/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0890 - val_loss: 0.0775\n",
            "Epoch 169/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0905 - val_loss: 0.0781\n",
            "Epoch 170/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0905 - val_loss: 0.0776\n",
            "Epoch 171/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0891 - val_loss: 0.0781\n",
            "Epoch 172/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0903 - val_loss: 0.0777\n",
            "Epoch 173/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0907 - val_loss: 0.0783\n",
            "Epoch 174/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0896 - val_loss: 0.0782\n",
            "Epoch 175/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0904 - val_loss: 0.0793\n",
            "Epoch 176/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0910 - val_loss: 0.0785\n",
            "Epoch 177/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0899 - val_loss: 0.0796\n",
            "Epoch 178/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0880 - val_loss: 0.0785\n",
            "Epoch 179/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0891 - val_loss: 0.0798\n",
            "Epoch 180/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0916 - val_loss: 0.0790\n",
            "Epoch 181/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0911 - val_loss: 0.0802\n",
            "Epoch 182/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0915 - val_loss: 0.0797\n",
            "Epoch 183/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0931 - val_loss: 0.0810\n",
            "Epoch 184/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0911 - val_loss: 0.0792\n",
            "Epoch 185/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0902 - val_loss: 0.0803\n",
            "Epoch 186/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0923 - val_loss: 0.0789\n",
            "Epoch 187/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0932 - val_loss: 0.0802\n",
            "Epoch 188/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0908 - val_loss: 0.0778\n",
            "Epoch 189/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0904 - val_loss: 0.0786\n",
            "Epoch 190/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0890 - val_loss: 0.0779\n",
            "Epoch 191/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0868 - val_loss: 0.0790\n",
            "Epoch 192/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0913 - val_loss: 0.0779\n",
            "Epoch 193/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0906 - val_loss: 0.0788\n",
            "Epoch 194/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0906 - val_loss: 0.0777\n",
            "Epoch 195/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0916 - val_loss: 0.0784\n",
            "Epoch 196/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0898 - val_loss: 0.0773\n",
            "Epoch 197/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0886 - val_loss: 0.0785\n",
            "Epoch 198/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0910 - val_loss: 0.0770\n",
            "Epoch 199/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0902 - val_loss: 0.0780\n",
            "Epoch 200/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0902 - val_loss: 0.0765\n",
            "Epoch 201/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0886 - val_loss: 0.0774\n",
            "Epoch 202/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0892 - val_loss: 0.0759\n",
            "Epoch 203/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0879 - val_loss: 0.0766\n",
            "Epoch 204/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0897 - val_loss: 0.0763\n",
            "Epoch 205/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0889 - val_loss: 0.0776\n",
            "Epoch 206/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0892 - val_loss: 0.0766\n",
            "Epoch 207/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0901 - val_loss: 0.0779\n",
            "Epoch 208/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0896 - val_loss: 0.0767\n",
            "Epoch 209/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0883 - val_loss: 0.0779\n",
            "Epoch 210/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0881 - val_loss: 0.0761\n",
            "Epoch 211/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0875 - val_loss: 0.0769\n",
            "Epoch 212/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0878 - val_loss: 0.0758\n",
            "Epoch 213/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0886 - val_loss: 0.0765\n",
            "Epoch 214/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0882 - val_loss: 0.0757\n",
            "Epoch 215/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0867 - val_loss: 0.0767\n",
            "Epoch 216/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0887 - val_loss: 0.0755\n",
            "Epoch 217/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0879 - val_loss: 0.0762\n",
            "Epoch 218/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0874 - val_loss: 0.0752\n",
            "Epoch 219/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0875 - val_loss: 0.0757\n",
            "Epoch 220/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0899 - val_loss: 0.0752\n",
            "Epoch 221/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0871 - val_loss: 0.0758\n",
            "Epoch 222/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0862 - val_loss: 0.0753\n",
            "Epoch 223/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0877 - val_loss: 0.0761\n",
            "Epoch 224/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0888 - val_loss: 0.0752\n",
            "Epoch 225/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0855 - val_loss: 0.0766\n",
            "Epoch 226/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0873 - val_loss: 0.0755\n",
            "Epoch 227/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0875 - val_loss: 0.0765\n",
            "Epoch 228/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0863 - val_loss: 0.0756\n",
            "Epoch 229/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0884 - val_loss: 0.0764\n",
            "Epoch 230/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0871 - val_loss: 0.0754\n",
            "Epoch 231/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0869 - val_loss: 0.0758\n",
            "Epoch 232/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0874 - val_loss: 0.0751\n",
            "Epoch 233/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0874 - val_loss: 0.0762\n",
            "Epoch 234/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0876 - val_loss: 0.0754\n",
            "Epoch 235/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0875 - val_loss: 0.0769\n",
            "Epoch 236/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0854 - val_loss: 0.0758\n",
            "Epoch 237/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0894 - val_loss: 0.0772\n",
            "Epoch 238/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0886 - val_loss: 0.0760\n",
            "Epoch 239/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0883 - val_loss: 0.0774\n",
            "Epoch 240/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0903 - val_loss: 0.0762\n",
            "Epoch 241/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0899 - val_loss: 0.0777\n",
            "Epoch 242/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0901 - val_loss: 0.0761\n",
            "Epoch 243/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0875 - val_loss: 0.0778\n",
            "Epoch 244/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0881 - val_loss: 0.0755\n",
            "Epoch 245/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0866 - val_loss: 0.0767\n",
            "Epoch 246/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0872 - val_loss: 0.0755\n",
            "Epoch 247/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0886 - val_loss: 0.0775\n",
            "Epoch 248/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0875 - val_loss: 0.0758\n",
            "Epoch 249/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0867 - val_loss: 0.0765\n",
            "Epoch 250/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0879 - val_loss: 0.0753\n",
            "Epoch 251/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0892 - val_loss: 0.0765\n",
            "Epoch 252/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0892 - val_loss: 0.0751\n",
            "Epoch 253/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0878 - val_loss: 0.0758\n",
            "Epoch 254/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0843 - val_loss: 0.0748\n",
            "Epoch 255/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0865 - val_loss: 0.0753\n",
            "Epoch 256/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0860 - val_loss: 0.0744\n",
            "Epoch 257/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0875 - val_loss: 0.0757\n",
            "Epoch 258/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0862 - val_loss: 0.0743\n",
            "Epoch 259/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0871 - val_loss: 0.0752\n",
            "Epoch 260/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0860 - val_loss: 0.0743\n",
            "Epoch 261/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0872 - val_loss: 0.0758\n",
            "Epoch 262/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0870 - val_loss: 0.0744\n",
            "Epoch 263/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0873 - val_loss: 0.0755\n",
            "Epoch 264/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0875 - val_loss: 0.0740\n",
            "Epoch 265/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0864 - val_loss: 0.0747\n",
            "Epoch 266/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0867 - val_loss: 0.0737\n",
            "Epoch 267/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0867 - val_loss: 0.0740\n",
            "Epoch 268/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0848 - val_loss: 0.0733\n",
            "Epoch 269/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0848 - val_loss: 0.0744\n",
            "Epoch 270/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0860 - val_loss: 0.0736\n",
            "Epoch 271/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0865 - val_loss: 0.0752\n",
            "Epoch 272/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0879 - val_loss: 0.0743\n",
            "Epoch 273/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0864 - val_loss: 0.0747\n",
            "Epoch 274/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0859 - val_loss: 0.0735\n",
            "Epoch 275/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0884 - val_loss: 0.0736\n",
            "Epoch 276/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0857 - val_loss: 0.0727\n",
            "Epoch 277/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0854 - val_loss: 0.0731\n",
            "Epoch 278/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0849 - val_loss: 0.0726\n",
            "Epoch 279/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0838 - val_loss: 0.0735\n",
            "Epoch 280/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0844 - val_loss: 0.0728\n",
            "Epoch 281/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0852 - val_loss: 0.0741\n",
            "Epoch 282/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0850 - val_loss: 0.0734\n",
            "Epoch 283/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0850 - val_loss: 0.0739\n",
            "Epoch 284/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0858 - val_loss: 0.0732\n",
            "Epoch 285/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0843 - val_loss: 0.0738\n",
            "Epoch 286/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0855 - val_loss: 0.0731\n",
            "Epoch 287/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0836 - val_loss: 0.0741\n",
            "Epoch 288/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0854 - val_loss: 0.0739\n",
            "Epoch 289/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0872 - val_loss: 0.0754\n",
            "Epoch 290/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0858 - val_loss: 0.0740\n",
            "Epoch 291/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0840 - val_loss: 0.0747\n",
            "Epoch 292/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0838 - val_loss: 0.0735\n",
            "Epoch 293/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0831 - val_loss: 0.0745\n",
            "Epoch 294/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0835 - val_loss: 0.0731\n",
            "Epoch 295/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0864 - val_loss: 0.0743\n",
            "Epoch 296/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0857 - val_loss: 0.0730\n",
            "Epoch 297/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0850 - val_loss: 0.0740\n",
            "Epoch 298/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0848 - val_loss: 0.0726\n",
            "Epoch 299/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0859 - val_loss: 0.0737\n",
            "Epoch 300/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0858 - val_loss: 0.0726\n",
            "Epoch 301/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0847 - val_loss: 0.0734\n",
            "Epoch 302/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0845 - val_loss: 0.0721\n",
            "Epoch 303/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0847 - val_loss: 0.0731\n",
            "Epoch 304/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0840 - val_loss: 0.0721\n",
            "Epoch 305/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0834 - val_loss: 0.0734\n",
            "Epoch 306/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0842 - val_loss: 0.0722\n",
            "Epoch 307/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0842 - val_loss: 0.0738\n",
            "Epoch 308/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0864 - val_loss: 0.0728\n",
            "Epoch 309/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0845 - val_loss: 0.0738\n",
            "Epoch 310/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0849 - val_loss: 0.0727\n",
            "Epoch 311/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0854 - val_loss: 0.0735\n",
            "Epoch 312/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0846 - val_loss: 0.0725\n",
            "Epoch 313/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0836 - val_loss: 0.0735\n",
            "Epoch 314/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0834 - val_loss: 0.0727\n",
            "Epoch 315/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0858 - val_loss: 0.0737\n",
            "Epoch 316/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0830 - val_loss: 0.0726\n",
            "Epoch 317/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0862 - val_loss: 0.0743\n",
            "Epoch 318/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0841 - val_loss: 0.0726\n",
            "Epoch 319/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0822 - val_loss: 0.0733\n",
            "Epoch 320/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0836 - val_loss: 0.0722\n",
            "Epoch 321/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0843 - val_loss: 0.0741\n",
            "Epoch 322/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0834 - val_loss: 0.0726\n",
            "Epoch 323/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0848 - val_loss: 0.0740\n",
            "Epoch 324/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0835 - val_loss: 0.0724\n",
            "Epoch 325/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0835 - val_loss: 0.0738\n",
            "Epoch 326/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0841 - val_loss: 0.0723\n",
            "Epoch 327/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0866 - val_loss: 0.0733\n",
            "Epoch 328/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0825 - val_loss: 0.0722\n",
            "Epoch 329/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0841 - val_loss: 0.0730\n",
            "Epoch 330/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0832 - val_loss: 0.0719\n",
            "Epoch 331/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0835 - val_loss: 0.0720\n",
            "Epoch 332/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0833 - val_loss: 0.0714\n",
            "Epoch 333/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0817 - val_loss: 0.0718\n",
            "Epoch 334/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0816 - val_loss: 0.0711\n",
            "Epoch 335/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0808 - val_loss: 0.0716\n",
            "Epoch 336/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0837 - val_loss: 0.0711\n",
            "Epoch 337/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0826 - val_loss: 0.0720\n",
            "Epoch 338/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0843 - val_loss: 0.0714\n",
            "Epoch 339/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0832 - val_loss: 0.0732\n",
            "Epoch 340/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0831 - val_loss: 0.0713\n",
            "Epoch 341/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0838 - val_loss: 0.0725\n",
            "Epoch 342/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0850 - val_loss: 0.0712\n",
            "Epoch 343/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0812 - val_loss: 0.0729\n",
            "Epoch 344/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0819 - val_loss: 0.0718\n",
            "Epoch 345/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0828 - val_loss: 0.0738\n",
            "Epoch 346/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0841 - val_loss: 0.0726\n",
            "Epoch 347/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0852 - val_loss: 0.0747\n",
            "Epoch 348/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0855 - val_loss: 0.0733\n",
            "Epoch 349/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0840 - val_loss: 0.0751\n",
            "Epoch 350/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0854 - val_loss: 0.0732\n",
            "Epoch 351/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0836 - val_loss: 0.0745\n",
            "Epoch 352/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0849 - val_loss: 0.0728\n",
            "Epoch 353/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0853 - val_loss: 0.0738\n",
            "Epoch 354/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0848 - val_loss: 0.0723\n",
            "Epoch 355/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0842 - val_loss: 0.0732\n",
            "Epoch 356/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0828 - val_loss: 0.0720\n",
            "Epoch 357/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0841 - val_loss: 0.0725\n",
            "Epoch 358/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0836 - val_loss: 0.0716\n",
            "Epoch 359/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0811 - val_loss: 0.0728\n",
            "Epoch 360/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0823 - val_loss: 0.0715\n",
            "Epoch 361/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0837 - val_loss: 0.0726\n",
            "Epoch 362/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0828 - val_loss: 0.0713\n",
            "Epoch 363/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0827 - val_loss: 0.0722\n",
            "Epoch 364/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0819 - val_loss: 0.0711\n",
            "Epoch 365/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0801 - val_loss: 0.0726\n",
            "Epoch 366/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0824 - val_loss: 0.0708\n",
            "Epoch 367/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0818 - val_loss: 0.0720\n",
            "Epoch 368/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0806 - val_loss: 0.0706\n",
            "Epoch 369/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0828 - val_loss: 0.0717\n",
            "Epoch 370/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0814 - val_loss: 0.0706\n",
            "Epoch 371/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0828 - val_loss: 0.0717\n",
            "Epoch 372/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0817 - val_loss: 0.0709\n",
            "Epoch 373/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0828 - val_loss: 0.0723\n",
            "Epoch 374/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0811 - val_loss: 0.0708\n",
            "Epoch 375/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0825 - val_loss: 0.0722\n",
            "Epoch 376/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0840 - val_loss: 0.0709\n",
            "Epoch 377/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0818 - val_loss: 0.0715\n",
            "Epoch 378/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0816 - val_loss: 0.0707\n",
            "Epoch 379/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0822 - val_loss: 0.0719\n",
            "Epoch 380/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0835 - val_loss: 0.0708\n",
            "Epoch 381/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0797 - val_loss: 0.0713\n",
            "Epoch 382/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0811 - val_loss: 0.0705\n",
            "Epoch 383/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0814 - val_loss: 0.0711\n",
            "Epoch 384/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0815 - val_loss: 0.0703\n",
            "Epoch 385/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0822 - val_loss: 0.0707\n",
            "Epoch 386/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0835 - val_loss: 0.0701\n",
            "Epoch 387/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0815 - val_loss: 0.0711\n",
            "Epoch 388/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0818 - val_loss: 0.0703\n",
            "Epoch 389/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0815 - val_loss: 0.0719\n",
            "Epoch 390/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0819 - val_loss: 0.0706\n",
            "Epoch 391/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0821 - val_loss: 0.0720\n",
            "Epoch 392/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0824 - val_loss: 0.0702\n",
            "Epoch 393/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0811 - val_loss: 0.0710\n",
            "Epoch 394/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0806 - val_loss: 0.0701\n",
            "Epoch 395/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0804 - val_loss: 0.0710\n",
            "Epoch 396/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0808 - val_loss: 0.0702\n",
            "Epoch 397/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0802 - val_loss: 0.0716\n",
            "Epoch 398/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0806 - val_loss: 0.0706\n",
            "Epoch 399/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0815 - val_loss: 0.0716\n",
            "Epoch 400/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0809 - val_loss: 0.0705\n",
            "Epoch 401/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0812 - val_loss: 0.0712\n",
            "Epoch 402/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0822 - val_loss: 0.0704\n",
            "Epoch 403/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0806 - val_loss: 0.0714\n",
            "Epoch 404/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0810 - val_loss: 0.0705\n",
            "Epoch 405/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0808 - val_loss: 0.0719\n",
            "Epoch 406/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0793 - val_loss: 0.0703\n",
            "Epoch 407/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0829 - val_loss: 0.0711\n",
            "Epoch 408/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0827 - val_loss: 0.0701\n",
            "Epoch 409/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0813 - val_loss: 0.0716\n",
            "Epoch 410/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0827 - val_loss: 0.0709\n",
            "Epoch 411/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0824 - val_loss: 0.0718\n",
            "Epoch 412/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0815 - val_loss: 0.0703\n",
            "Epoch 413/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0792 - val_loss: 0.0708\n",
            "Epoch 414/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0801 - val_loss: 0.0697\n",
            "Epoch 415/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0806 - val_loss: 0.0708\n",
            "Epoch 416/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0795 - val_loss: 0.0696\n",
            "Epoch 417/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0780 - val_loss: 0.0706\n",
            "Epoch 418/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0798 - val_loss: 0.0691\n",
            "Epoch 419/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0806 - val_loss: 0.0701\n",
            "Epoch 420/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0800 - val_loss: 0.0696\n",
            "Epoch 421/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0802 - val_loss: 0.0704\n",
            "Epoch 422/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0829 - val_loss: 0.0699\n",
            "Epoch 423/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0805 - val_loss: 0.0710\n",
            "Epoch 424/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0785 - val_loss: 0.0703\n",
            "Epoch 425/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0808 - val_loss: 0.0713\n",
            "Epoch 426/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0819 - val_loss: 0.0704\n",
            "Epoch 427/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0822 - val_loss: 0.0713\n",
            "Epoch 428/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0817 - val_loss: 0.0704\n",
            "Epoch 429/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0816 - val_loss: 0.0720\n",
            "Epoch 430/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0819 - val_loss: 0.0703\n",
            "Epoch 431/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0829 - val_loss: 0.0717\n",
            "Epoch 432/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.0813 - val_loss: 0.0705\n",
            "Epoch 433/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0805 - val_loss: 0.0721\n",
            "Epoch 434/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0812 - val_loss: 0.0711\n",
            "Epoch 435/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0819 - val_loss: 0.0724\n",
            "Epoch 436/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0826 - val_loss: 0.0711\n",
            "Epoch 437/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0813 - val_loss: 0.0719\n",
            "Epoch 438/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0822 - val_loss: 0.0708\n",
            "Epoch 439/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0789 - val_loss: 0.0722\n",
            "Epoch 440/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0806 - val_loss: 0.0709\n",
            "Epoch 441/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0821 - val_loss: 0.0724\n",
            "Epoch 442/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0820 - val_loss: 0.0706\n",
            "Epoch 443/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0812 - val_loss: 0.0720\n",
            "Epoch 444/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0801 - val_loss: 0.0701\n",
            "Epoch 445/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0806 - val_loss: 0.0707\n",
            "Epoch 446/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0795 - val_loss: 0.0698\n",
            "Epoch 447/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0812 - val_loss: 0.0708\n",
            "Epoch 448/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0805 - val_loss: 0.0697\n",
            "Epoch 449/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0810 - val_loss: 0.0711\n",
            "Epoch 450/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0812 - val_loss: 0.0697\n",
            "Epoch 451/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0784 - val_loss: 0.0706\n",
            "Epoch 452/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.0801 - val_loss: 0.0697\n",
            "Epoch 453/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0800 - val_loss: 0.0709\n",
            "Epoch 454/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0791 - val_loss: 0.0694\n",
            "Epoch 455/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0786 - val_loss: 0.0700\n",
            "Epoch 456/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0796 - val_loss: 0.0687\n",
            "Epoch 457/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0783 - val_loss: 0.0699\n",
            "Epoch 458/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0787 - val_loss: 0.0686\n",
            "Epoch 459/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0772 - val_loss: 0.0693\n",
            "Epoch 460/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0777 - val_loss: 0.0683\n",
            "Epoch 461/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0772 - val_loss: 0.0697\n",
            "Epoch 462/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0777 - val_loss: 0.0685\n",
            "Epoch 463/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0801 - val_loss: 0.0691\n",
            "Epoch 464/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0785 - val_loss: 0.0681\n",
            "Epoch 465/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0790 - val_loss: 0.0689\n",
            "Epoch 466/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0778 - val_loss: 0.0683\n",
            "Epoch 467/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0791 - val_loss: 0.0689\n",
            "Epoch 468/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0776 - val_loss: 0.0684\n",
            "Epoch 469/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0763 - val_loss: 0.0687\n",
            "Epoch 470/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0800 - val_loss: 0.0684\n",
            "Epoch 471/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0791 - val_loss: 0.0693\n",
            "Epoch 472/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0775 - val_loss: 0.0687\n",
            "Epoch 473/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0799 - val_loss: 0.0706\n",
            "Epoch 474/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0796 - val_loss: 0.0690\n",
            "Epoch 475/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0789 - val_loss: 0.0705\n",
            "Epoch 476/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0792 - val_loss: 0.0694\n",
            "Epoch 477/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0785 - val_loss: 0.0702\n",
            "Epoch 478/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0796 - val_loss: 0.0692\n",
            "Epoch 479/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0781 - val_loss: 0.0706\n",
            "Epoch 480/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0799 - val_loss: 0.0702\n",
            "Epoch 481/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0794 - val_loss: 0.0708\n",
            "Epoch 482/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0791 - val_loss: 0.0694\n",
            "Epoch 483/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0811 - val_loss: 0.0704\n",
            "Epoch 484/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0778 - val_loss: 0.0691\n",
            "Epoch 485/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0799 - val_loss: 0.0712\n",
            "Epoch 486/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0811 - val_loss: 0.0695\n",
            "Epoch 487/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0788 - val_loss: 0.0712\n",
            "Epoch 488/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0788 - val_loss: 0.0694\n",
            "Epoch 489/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0793 - val_loss: 0.0709\n",
            "Epoch 490/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0805 - val_loss: 0.0700\n",
            "Epoch 491/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0791 - val_loss: 0.0715\n",
            "Epoch 492/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0806 - val_loss: 0.0700\n",
            "Epoch 493/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0800 - val_loss: 0.0715\n",
            "Epoch 494/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0794 - val_loss: 0.0689\n",
            "Epoch 495/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0788 - val_loss: 0.0703\n",
            "Epoch 496/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0787 - val_loss: 0.0687\n",
            "Epoch 497/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0783 - val_loss: 0.0705\n",
            "Epoch 498/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0789 - val_loss: 0.0692\n",
            "Epoch 499/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0798 - val_loss: 0.0700\n",
            "Epoch 500/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0765 - val_loss: 0.0686\n",
            "Epoch 501/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0789 - val_loss: 0.0696\n",
            "Epoch 502/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0780 - val_loss: 0.0684\n",
            "Epoch 503/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0784 - val_loss: 0.0694\n",
            "Epoch 504/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0781 - val_loss: 0.0684\n",
            "Epoch 505/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0787 - val_loss: 0.0700\n",
            "Epoch 506/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0791 - val_loss: 0.0684\n",
            "Epoch 507/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0764 - val_loss: 0.0691\n",
            "Epoch 508/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0768 - val_loss: 0.0681\n",
            "Epoch 509/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0765 - val_loss: 0.0691\n",
            "Epoch 510/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0773 - val_loss: 0.0687\n",
            "Epoch 511/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0795 - val_loss: 0.0702\n",
            "Epoch 512/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0798 - val_loss: 0.0695\n",
            "Epoch 513/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0787 - val_loss: 0.0711\n",
            "Epoch 514/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0784 - val_loss: 0.0695\n",
            "Epoch 515/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0795 - val_loss: 0.0716\n",
            "Epoch 516/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0829 - val_loss: 0.0699\n",
            "Epoch 517/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0791 - val_loss: 0.0712\n",
            "Epoch 518/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0791 - val_loss: 0.0694\n",
            "Epoch 519/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0770 - val_loss: 0.0711\n",
            "Epoch 520/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0794 - val_loss: 0.0698\n",
            "Epoch 521/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0791 - val_loss: 0.0713\n",
            "Epoch 522/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0795 - val_loss: 0.0694\n",
            "Epoch 523/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0790 - val_loss: 0.0711\n",
            "Epoch 524/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0796 - val_loss: 0.0692\n",
            "Epoch 525/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0776 - val_loss: 0.0705\n",
            "Epoch 526/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0778 - val_loss: 0.0689\n",
            "Epoch 527/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0779 - val_loss: 0.0706\n",
            "Epoch 528/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0795 - val_loss: 0.0690\n",
            "Epoch 529/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0788 - val_loss: 0.0696\n",
            "Epoch 530/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0766 - val_loss: 0.0685\n",
            "Epoch 531/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0771 - val_loss: 0.0700\n",
            "Epoch 532/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0792 - val_loss: 0.0684\n",
            "Epoch 533/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0788 - val_loss: 0.0699\n",
            "Epoch 534/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0779 - val_loss: 0.0684\n",
            "Epoch 535/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0759 - val_loss: 0.0690\n",
            "Epoch 536/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0764 - val_loss: 0.0681\n",
            "Epoch 537/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0776 - val_loss: 0.0690\n",
            "Epoch 538/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0776 - val_loss: 0.0684\n",
            "Epoch 539/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0773 - val_loss: 0.0697\n",
            "Epoch 540/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0776 - val_loss: 0.0686\n",
            "Epoch 541/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0785 - val_loss: 0.0699\n",
            "Epoch 542/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0778 - val_loss: 0.0684\n",
            "Epoch 543/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0781 - val_loss: 0.0692\n",
            "Epoch 544/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0757 - val_loss: 0.0678\n",
            "Epoch 545/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0769 - val_loss: 0.0691\n",
            "Epoch 546/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0774 - val_loss: 0.0677\n",
            "Epoch 547/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0763 - val_loss: 0.0688\n",
            "Epoch 548/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0777 - val_loss: 0.0676\n",
            "Epoch 549/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0764 - val_loss: 0.0690\n",
            "Epoch 550/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0774 - val_loss: 0.0679\n",
            "Epoch 551/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0778 - val_loss: 0.0693\n",
            "Epoch 552/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0779 - val_loss: 0.0679\n",
            "Epoch 553/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0777 - val_loss: 0.0694\n",
            "Epoch 554/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0785 - val_loss: 0.0683\n",
            "Epoch 555/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0774 - val_loss: 0.0696\n",
            "Epoch 556/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0769 - val_loss: 0.0686\n",
            "Epoch 557/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0785 - val_loss: 0.0693\n",
            "Epoch 558/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0774 - val_loss: 0.0682\n",
            "Epoch 559/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0782 - val_loss: 0.0691\n",
            "Epoch 560/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0783 - val_loss: 0.0680\n",
            "Epoch 561/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0758 - val_loss: 0.0695\n",
            "Epoch 562/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0783 - val_loss: 0.0682\n",
            "Epoch 563/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0773 - val_loss: 0.0693\n",
            "Epoch 564/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0783 - val_loss: 0.0684\n",
            "Epoch 565/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0793 - val_loss: 0.0693\n",
            "Epoch 566/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0777 - val_loss: 0.0681\n",
            "Epoch 567/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0798 - val_loss: 0.0694\n",
            "Epoch 568/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0762 - val_loss: 0.0683\n",
            "Epoch 569/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0765 - val_loss: 0.0694\n",
            "Epoch 570/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0774 - val_loss: 0.0680\n",
            "Epoch 571/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0783 - val_loss: 0.0697\n",
            "Epoch 572/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0762 - val_loss: 0.0681\n",
            "Epoch 573/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0764 - val_loss: 0.0693\n",
            "Epoch 574/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0787 - val_loss: 0.0684\n",
            "Epoch 575/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0748 - val_loss: 0.0699\n",
            "Epoch 576/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0773 - val_loss: 0.0682\n",
            "Epoch 577/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0787 - val_loss: 0.0694\n",
            "Epoch 578/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0756 - val_loss: 0.0679\n",
            "Epoch 579/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0777 - val_loss: 0.0694\n",
            "Epoch 580/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0766 - val_loss: 0.0682\n",
            "Epoch 581/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0767 - val_loss: 0.0692\n",
            "Epoch 582/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0783 - val_loss: 0.0678\n",
            "Epoch 583/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0760 - val_loss: 0.0684\n",
            "Epoch 584/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0779 - val_loss: 0.0674\n",
            "Epoch 585/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0762 - val_loss: 0.0683\n",
            "Epoch 586/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0763 - val_loss: 0.0676\n",
            "Epoch 587/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0763 - val_loss: 0.0694\n",
            "Epoch 588/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0759 - val_loss: 0.0675\n",
            "Epoch 589/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0779 - val_loss: 0.0694\n",
            "Epoch 590/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0781 - val_loss: 0.0681\n",
            "Epoch 591/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0775 - val_loss: 0.0700\n",
            "Epoch 592/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0777 - val_loss: 0.0680\n",
            "Epoch 593/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0769 - val_loss: 0.0698\n",
            "Epoch 594/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0786 - val_loss: 0.0681\n",
            "Epoch 595/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0768 - val_loss: 0.0699\n",
            "Epoch 596/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0780 - val_loss: 0.0683\n",
            "Epoch 597/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0774 - val_loss: 0.0702\n",
            "Epoch 598/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0783 - val_loss: 0.0681\n",
            "Epoch 599/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0780 - val_loss: 0.0694\n",
            "Epoch 600/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0771 - val_loss: 0.0682\n",
            "Epoch 601/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0769 - val_loss: 0.0697\n",
            "Epoch 602/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0763 - val_loss: 0.0684\n",
            "Epoch 603/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0796 - val_loss: 0.0699\n",
            "Epoch 604/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0795 - val_loss: 0.0687\n",
            "Epoch 605/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0793 - val_loss: 0.0702\n",
            "Epoch 606/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0777 - val_loss: 0.0695\n",
            "Epoch 607/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0767 - val_loss: 0.0707\n",
            "Epoch 608/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0787 - val_loss: 0.0687\n",
            "Epoch 609/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0782 - val_loss: 0.0692\n",
            "Epoch 610/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0780 - val_loss: 0.0680\n",
            "Epoch 611/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0761 - val_loss: 0.0695\n",
            "Epoch 612/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0767 - val_loss: 0.0676\n",
            "Epoch 613/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0770 - val_loss: 0.0693\n",
            "Epoch 614/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0767 - val_loss: 0.0676\n",
            "Epoch 615/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0775 - val_loss: 0.0687\n",
            "Epoch 616/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0763 - val_loss: 0.0670\n",
            "Epoch 617/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0770 - val_loss: 0.0688\n",
            "Epoch 618/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0767 - val_loss: 0.0671\n",
            "Epoch 619/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0757 - val_loss: 0.0680\n",
            "Epoch 620/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0753 - val_loss: 0.0669\n",
            "Epoch 621/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0748 - val_loss: 0.0673\n",
            "Epoch 622/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0746 - val_loss: 0.0666\n",
            "Epoch 623/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0755 - val_loss: 0.0674\n",
            "Epoch 624/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0760 - val_loss: 0.0669\n",
            "Epoch 625/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0746 - val_loss: 0.0678\n",
            "Epoch 626/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0742 - val_loss: 0.0668\n",
            "Epoch 627/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0745 - val_loss: 0.0684\n",
            "Epoch 628/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0766 - val_loss: 0.0670\n",
            "Epoch 629/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0749 - val_loss: 0.0679\n",
            "Epoch 630/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0743 - val_loss: 0.0668\n",
            "Epoch 631/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0767 - val_loss: 0.0684\n",
            "Epoch 632/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0767 - val_loss: 0.0671\n",
            "Epoch 633/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0738 - val_loss: 0.0681\n",
            "Epoch 634/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0763 - val_loss: 0.0668\n",
            "Epoch 635/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0747 - val_loss: 0.0678\n",
            "Epoch 636/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0761 - val_loss: 0.0665\n",
            "Epoch 637/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0752 - val_loss: 0.0674\n",
            "Epoch 638/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0758 - val_loss: 0.0664\n",
            "Epoch 639/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0748 - val_loss: 0.0676\n",
            "Epoch 640/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0753 - val_loss: 0.0664\n",
            "Epoch 641/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0757 - val_loss: 0.0673\n",
            "Epoch 642/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0748 - val_loss: 0.0666\n",
            "Epoch 643/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0758 - val_loss: 0.0678\n",
            "Epoch 644/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0749 - val_loss: 0.0664\n",
            "Epoch 645/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0765 - val_loss: 0.0678\n",
            "Epoch 646/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0761 - val_loss: 0.0668\n",
            "Epoch 647/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0741 - val_loss: 0.0677\n",
            "Epoch 648/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0747 - val_loss: 0.0666\n",
            "Epoch 649/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0749 - val_loss: 0.0676\n",
            "Epoch 650/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0759 - val_loss: 0.0671\n",
            "Epoch 651/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0761 - val_loss: 0.0687\n",
            "Epoch 652/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0761 - val_loss: 0.0671\n",
            "Epoch 653/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0761 - val_loss: 0.0677\n",
            "Epoch 654/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0761 - val_loss: 0.0669\n",
            "Epoch 655/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0769 - val_loss: 0.0677\n",
            "Epoch 656/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0751 - val_loss: 0.0667\n",
            "Epoch 657/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0777 - val_loss: 0.0684\n",
            "Epoch 658/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0768 - val_loss: 0.0670\n",
            "Epoch 659/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0764 - val_loss: 0.0688\n",
            "Epoch 660/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0757 - val_loss: 0.0677\n",
            "Epoch 661/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0766 - val_loss: 0.0693\n",
            "Epoch 662/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0769 - val_loss: 0.0676\n",
            "Epoch 663/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0772 - val_loss: 0.0692\n",
            "Epoch 664/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0770 - val_loss: 0.0676\n",
            "Epoch 665/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0767 - val_loss: 0.0695\n",
            "Epoch 666/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0790 - val_loss: 0.0682\n",
            "Epoch 667/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0775 - val_loss: 0.0700\n",
            "Epoch 668/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0762 - val_loss: 0.0676\n",
            "Epoch 669/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0753 - val_loss: 0.0695\n",
            "Epoch 670/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0771 - val_loss: 0.0673\n",
            "Epoch 671/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0768 - val_loss: 0.0685\n",
            "Epoch 672/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0755 - val_loss: 0.0667\n",
            "Epoch 673/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0758 - val_loss: 0.0673\n",
            "Epoch 674/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0759 - val_loss: 0.0665\n",
            "Epoch 675/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0758 - val_loss: 0.0683\n",
            "Epoch 676/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0758 - val_loss: 0.0671\n",
            "Epoch 677/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0736 - val_loss: 0.0682\n",
            "Epoch 678/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0755 - val_loss: 0.0667\n",
            "Epoch 679/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0744 - val_loss: 0.0681\n",
            "Epoch 680/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0757 - val_loss: 0.0667\n",
            "Epoch 681/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0738 - val_loss: 0.0685\n",
            "Epoch 682/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0769 - val_loss: 0.0664\n",
            "Epoch 683/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0728 - val_loss: 0.0681\n",
            "Epoch 684/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0755 - val_loss: 0.0664\n",
            "Epoch 685/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0763 - val_loss: 0.0678\n",
            "Epoch 686/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0765 - val_loss: 0.0667\n",
            "Epoch 687/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0760 - val_loss: 0.0683\n",
            "Epoch 688/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0765 - val_loss: 0.0665\n",
            "Epoch 689/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0768 - val_loss: 0.0678\n",
            "Epoch 690/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0768 - val_loss: 0.0666\n",
            "Epoch 691/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0755 - val_loss: 0.0675\n",
            "Epoch 692/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0753 - val_loss: 0.0663\n",
            "Epoch 693/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0763 - val_loss: 0.0681\n",
            "Epoch 694/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0747 - val_loss: 0.0664\n",
            "Epoch 695/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0774 - val_loss: 0.0682\n",
            "Epoch 696/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0731 - val_loss: 0.0662\n",
            "Epoch 697/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0767 - val_loss: 0.0680\n",
            "Epoch 698/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0734 - val_loss: 0.0664\n",
            "Epoch 699/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0741 - val_loss: 0.0678\n",
            "Epoch 700/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0760 - val_loss: 0.0664\n",
            "Epoch 701/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0763 - val_loss: 0.0677\n",
            "Epoch 702/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0744 - val_loss: 0.0666\n",
            "Epoch 703/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0785 - val_loss: 0.0676\n",
            "Epoch 704/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0739 - val_loss: 0.0661\n",
            "Epoch 705/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0748 - val_loss: 0.0676\n",
            "Epoch 706/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0730 - val_loss: 0.0659\n",
            "Epoch 707/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0751 - val_loss: 0.0676\n",
            "Epoch 708/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0733 - val_loss: 0.0663\n",
            "Epoch 709/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0760 - val_loss: 0.0681\n",
            "Epoch 710/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0751 - val_loss: 0.0661\n",
            "Epoch 711/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0765 - val_loss: 0.0678\n",
            "Epoch 712/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0749 - val_loss: 0.0665\n",
            "Epoch 713/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0762 - val_loss: 0.0678\n",
            "Epoch 714/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0759 - val_loss: 0.0667\n",
            "Epoch 715/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0773 - val_loss: 0.0680\n",
            "Epoch 716/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0752 - val_loss: 0.0661\n",
            "Epoch 717/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0733 - val_loss: 0.0669\n",
            "Epoch 718/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0747 - val_loss: 0.0658\n",
            "Epoch 719/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0722 - val_loss: 0.0663\n",
            "Epoch 720/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0751 - val_loss: 0.0657\n",
            "Epoch 721/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0725 - val_loss: 0.0669\n",
            "Epoch 722/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0741 - val_loss: 0.0660\n",
            "Epoch 723/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0741 - val_loss: 0.0670\n",
            "Epoch 724/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0732 - val_loss: 0.0658\n",
            "Epoch 725/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0748 - val_loss: 0.0673\n",
            "Epoch 726/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0747 - val_loss: 0.0659\n",
            "Epoch 727/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0741 - val_loss: 0.0671\n",
            "Epoch 728/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0743 - val_loss: 0.0655\n",
            "Epoch 729/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0746 - val_loss: 0.0670\n",
            "Epoch 730/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0752 - val_loss: 0.0656\n",
            "Epoch 731/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0730 - val_loss: 0.0670\n",
            "Epoch 732/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0744 - val_loss: 0.0658\n",
            "Epoch 733/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0765 - val_loss: 0.0676\n",
            "Epoch 734/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0762 - val_loss: 0.0659\n",
            "Epoch 735/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0732 - val_loss: 0.0680\n",
            "Epoch 736/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0762 - val_loss: 0.0657\n",
            "Epoch 737/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0750 - val_loss: 0.0679\n",
            "Epoch 738/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0764 - val_loss: 0.0661\n",
            "Epoch 739/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0756 - val_loss: 0.0685\n",
            "Epoch 740/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0766 - val_loss: 0.0674\n",
            "Epoch 741/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0759 - val_loss: 0.0687\n",
            "Epoch 742/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0772 - val_loss: 0.0668\n",
            "Epoch 743/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0754 - val_loss: 0.0689\n",
            "Epoch 744/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0768 - val_loss: 0.0670\n",
            "Epoch 745/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0760 - val_loss: 0.0686\n",
            "Epoch 746/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0755 - val_loss: 0.0663\n",
            "Epoch 747/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0736 - val_loss: 0.0672\n",
            "Epoch 748/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0755 - val_loss: 0.0658\n",
            "Epoch 749/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0725 - val_loss: 0.0669\n",
            "Epoch 750/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0731 - val_loss: 0.0657\n",
            "Epoch 751/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0737 - val_loss: 0.0673\n",
            "Epoch 752/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0747 - val_loss: 0.0652\n",
            "Epoch 753/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0752 - val_loss: 0.0660\n",
            "Epoch 754/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0741 - val_loss: 0.0647\n",
            "Epoch 755/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0737 - val_loss: 0.0655\n",
            "Epoch 756/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0753 - val_loss: 0.0647\n",
            "Epoch 757/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0753 - val_loss: 0.0656\n",
            "Epoch 758/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0727 - val_loss: 0.0645\n",
            "Epoch 759/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0745 - val_loss: 0.0656\n",
            "Epoch 760/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0741 - val_loss: 0.0648\n",
            "Epoch 761/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0730 - val_loss: 0.0659\n",
            "Epoch 762/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0738 - val_loss: 0.0648\n",
            "Epoch 763/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0752 - val_loss: 0.0659\n",
            "Epoch 764/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0751 - val_loss: 0.0647\n",
            "Epoch 765/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0747 - val_loss: 0.0661\n",
            "Epoch 766/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0718 - val_loss: 0.0648\n",
            "Epoch 767/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0742 - val_loss: 0.0659\n",
            "Epoch 768/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0727 - val_loss: 0.0649\n",
            "Epoch 769/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0751 - val_loss: 0.0659\n",
            "Epoch 770/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0737 - val_loss: 0.0651\n",
            "Epoch 771/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0733 - val_loss: 0.0663\n",
            "Epoch 772/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0734 - val_loss: 0.0647\n",
            "Epoch 773/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0747 - val_loss: 0.0665\n",
            "Epoch 774/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0746 - val_loss: 0.0650\n",
            "Epoch 775/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0740 - val_loss: 0.0665\n",
            "Epoch 776/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0735 - val_loss: 0.0652\n",
            "Epoch 777/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0748 - val_loss: 0.0669\n",
            "Epoch 778/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0740 - val_loss: 0.0655\n",
            "Epoch 779/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0732 - val_loss: 0.0669\n",
            "Epoch 780/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0718 - val_loss: 0.0653\n",
            "Epoch 781/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0742 - val_loss: 0.0668\n",
            "Epoch 782/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0726 - val_loss: 0.0653\n",
            "Epoch 783/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0724 - val_loss: 0.0668\n",
            "Epoch 784/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0728 - val_loss: 0.0652\n",
            "Epoch 785/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0757 - val_loss: 0.0668\n",
            "Epoch 786/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0745 - val_loss: 0.0649\n",
            "Epoch 787/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0753 - val_loss: 0.0666\n",
            "Epoch 788/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0740 - val_loss: 0.0654\n",
            "Epoch 789/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0747 - val_loss: 0.0669\n",
            "Epoch 790/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0746 - val_loss: 0.0655\n",
            "Epoch 791/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0740 - val_loss: 0.0665\n",
            "Epoch 792/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0739 - val_loss: 0.0648\n",
            "Epoch 793/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0720 - val_loss: 0.0662\n",
            "Epoch 794/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0763 - val_loss: 0.0650\n",
            "Epoch 795/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0752 - val_loss: 0.0668\n",
            "Epoch 796/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0740 - val_loss: 0.0651\n",
            "Epoch 797/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0739 - val_loss: 0.0667\n",
            "Epoch 798/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0740 - val_loss: 0.0650\n",
            "Epoch 799/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0724 - val_loss: 0.0662\n",
            "Epoch 800/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0721 - val_loss: 0.0646\n",
            "Epoch 801/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0731 - val_loss: 0.0673\n",
            "Epoch 802/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0764 - val_loss: 0.0654\n",
            "Epoch 803/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0734 - val_loss: 0.0672\n",
            "Epoch 804/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0730 - val_loss: 0.0659\n",
            "Epoch 805/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0736 - val_loss: 0.0683\n",
            "Epoch 806/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0761 - val_loss: 0.0658\n",
            "Epoch 807/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0732 - val_loss: 0.0681\n",
            "Epoch 808/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0744 - val_loss: 0.0657\n",
            "Epoch 809/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0750 - val_loss: 0.0676\n",
            "Epoch 810/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0755 - val_loss: 0.0661\n",
            "Epoch 811/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0760 - val_loss: 0.0682\n",
            "Epoch 812/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0746 - val_loss: 0.0652\n",
            "Epoch 813/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0734 - val_loss: 0.0668\n",
            "Epoch 814/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0720 - val_loss: 0.0650\n",
            "Epoch 815/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0745 - val_loss: 0.0662\n",
            "Epoch 816/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0731 - val_loss: 0.0645\n",
            "Epoch 817/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0737 - val_loss: 0.0660\n",
            "Epoch 818/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0743 - val_loss: 0.0647\n",
            "Epoch 819/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0744 - val_loss: 0.0655\n",
            "Epoch 820/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0749 - val_loss: 0.0641\n",
            "Epoch 821/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0720 - val_loss: 0.0655\n",
            "Epoch 822/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0724 - val_loss: 0.0640\n",
            "Epoch 823/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0744 - val_loss: 0.0651\n",
            "Epoch 824/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0732 - val_loss: 0.0638\n",
            "Epoch 825/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0724 - val_loss: 0.0648\n",
            "Epoch 826/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0718 - val_loss: 0.0638\n",
            "Epoch 827/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0739 - val_loss: 0.0648\n",
            "Epoch 828/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0733 - val_loss: 0.0636\n",
            "Epoch 829/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0733 - val_loss: 0.0648\n",
            "Epoch 830/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0728 - val_loss: 0.0635\n",
            "Epoch 831/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0706 - val_loss: 0.0645\n",
            "Epoch 832/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0730 - val_loss: 0.0635\n",
            "Epoch 833/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0710 - val_loss: 0.0641\n",
            "Epoch 834/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0692 - val_loss: 0.0634\n",
            "Epoch 835/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0734 - val_loss: 0.0635\n",
            "Epoch 836/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0726 - val_loss: 0.0634\n",
            "Epoch 837/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0747 - val_loss: 0.0639\n",
            "Epoch 838/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0713 - val_loss: 0.0635\n",
            "Epoch 839/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0721 - val_loss: 0.0639\n",
            "Epoch 840/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0694 - val_loss: 0.0630\n",
            "Epoch 841/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0714 - val_loss: 0.0647\n",
            "Epoch 842/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0733 - val_loss: 0.0633\n",
            "Epoch 843/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0714 - val_loss: 0.0652\n",
            "Epoch 844/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0717 - val_loss: 0.0634\n",
            "Epoch 845/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0720 - val_loss: 0.0649\n",
            "Epoch 846/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0719 - val_loss: 0.0636\n",
            "Epoch 847/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0727 - val_loss: 0.0651\n",
            "Epoch 848/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0720 - val_loss: 0.0639\n",
            "Epoch 849/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0747 - val_loss: 0.0658\n",
            "Epoch 850/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0727 - val_loss: 0.0645\n",
            "Epoch 851/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0732 - val_loss: 0.0659\n",
            "Epoch 852/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0727 - val_loss: 0.0643\n",
            "Epoch 853/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0737 - val_loss: 0.0654\n",
            "Epoch 854/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0739 - val_loss: 0.0644\n",
            "Epoch 855/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0746 - val_loss: 0.0658\n",
            "Epoch 856/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0720 - val_loss: 0.0636\n",
            "Epoch 857/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0720 - val_loss: 0.0654\n",
            "Epoch 858/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0735 - val_loss: 0.0639\n",
            "Epoch 859/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0728 - val_loss: 0.0659\n",
            "Epoch 860/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0720 - val_loss: 0.0640\n",
            "Epoch 861/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0703 - val_loss: 0.0656\n",
            "Epoch 862/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0735 - val_loss: 0.0638\n",
            "Epoch 863/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0728 - val_loss: 0.0653\n",
            "Epoch 864/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0728 - val_loss: 0.0639\n",
            "Epoch 865/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0731 - val_loss: 0.0649\n",
            "Epoch 866/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0708 - val_loss: 0.0635\n",
            "Epoch 867/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0719 - val_loss: 0.0658\n",
            "Epoch 868/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0716 - val_loss: 0.0640\n",
            "Epoch 869/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0736 - val_loss: 0.0654\n",
            "Epoch 870/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0717 - val_loss: 0.0637\n",
            "Epoch 871/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0718 - val_loss: 0.0664\n",
            "Epoch 872/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0728 - val_loss: 0.0647\n",
            "Epoch 873/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0734 - val_loss: 0.0663\n",
            "Epoch 874/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0733 - val_loss: 0.0646\n",
            "Epoch 875/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0728 - val_loss: 0.0663\n",
            "Epoch 876/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0730 - val_loss: 0.0645\n",
            "Epoch 877/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0721 - val_loss: 0.0663\n",
            "Epoch 878/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0730 - val_loss: 0.0646\n",
            "Epoch 879/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0729 - val_loss: 0.0664\n",
            "Epoch 880/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0733 - val_loss: 0.0644\n",
            "Epoch 881/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0742 - val_loss: 0.0667\n",
            "Epoch 882/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.0741 - val_loss: 0.0640\n",
            "Epoch 883/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0717 - val_loss: 0.0665\n",
            "Epoch 884/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0723 - val_loss: 0.0637\n",
            "Epoch 885/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0741 - val_loss: 0.0659\n",
            "Epoch 886/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0719 - val_loss: 0.0639\n",
            "Epoch 887/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0723 - val_loss: 0.0659\n",
            "Epoch 888/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0742 - val_loss: 0.0637\n",
            "Epoch 889/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0727 - val_loss: 0.0650\n",
            "Epoch 890/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0715 - val_loss: 0.0634\n",
            "Epoch 891/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0726 - val_loss: 0.0650\n",
            "Epoch 892/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0720 - val_loss: 0.0636\n",
            "Epoch 893/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0711 - val_loss: 0.0645\n",
            "Epoch 894/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0718 - val_loss: 0.0630\n",
            "Epoch 895/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0713 - val_loss: 0.0644\n",
            "Epoch 896/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0706 - val_loss: 0.0630\n",
            "Epoch 897/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0744 - val_loss: 0.0642\n",
            "Epoch 898/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0729 - val_loss: 0.0628\n",
            "Epoch 899/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0712 - val_loss: 0.0636\n",
            "Epoch 900/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0691 - val_loss: 0.0627\n",
            "Epoch 901/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0712 - val_loss: 0.0631\n",
            "Epoch 902/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0704 - val_loss: 0.0626\n",
            "Epoch 903/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0718 - val_loss: 0.0641\n",
            "Epoch 904/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0706 - val_loss: 0.0626\n",
            "Epoch 905/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0707 - val_loss: 0.0638\n",
            "Epoch 906/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0739 - val_loss: 0.0629\n",
            "Epoch 907/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0714 - val_loss: 0.0643\n",
            "Epoch 908/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0718 - val_loss: 0.0626\n",
            "Epoch 909/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0732 - val_loss: 0.0642\n",
            "Epoch 910/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0712 - val_loss: 0.0627\n",
            "Epoch 911/1000\n",
            "3298/3298 [==============================] - 0s 57us/step - loss: 0.0704 - val_loss: 0.0646\n",
            "Epoch 912/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0727 - val_loss: 0.0628\n",
            "Epoch 913/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0718 - val_loss: 0.0644\n",
            "Epoch 914/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0705 - val_loss: 0.0632\n",
            "Epoch 915/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0724 - val_loss: 0.0653\n",
            "Epoch 916/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0714 - val_loss: 0.0630\n",
            "Epoch 917/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0717 - val_loss: 0.0652\n",
            "Epoch 918/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0716 - val_loss: 0.0636\n",
            "Epoch 919/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0724 - val_loss: 0.0660\n",
            "Epoch 920/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0739 - val_loss: 0.0638\n",
            "Epoch 921/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0731 - val_loss: 0.0656\n",
            "Epoch 922/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0732 - val_loss: 0.0636\n",
            "Epoch 923/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0714 - val_loss: 0.0653\n",
            "Epoch 924/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0718 - val_loss: 0.0626\n",
            "Epoch 925/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0707 - val_loss: 0.0651\n",
            "Epoch 926/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0708 - val_loss: 0.0634\n",
            "Epoch 927/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0721 - val_loss: 0.0646\n",
            "Epoch 928/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0714 - val_loss: 0.0629\n",
            "Epoch 929/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0711 - val_loss: 0.0641\n",
            "Epoch 930/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0726 - val_loss: 0.0629\n",
            "Epoch 931/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0710 - val_loss: 0.0640\n",
            "Epoch 932/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0711 - val_loss: 0.0628\n",
            "Epoch 933/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0720 - val_loss: 0.0646\n",
            "Epoch 934/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0709 - val_loss: 0.0633\n",
            "Epoch 935/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0720 - val_loss: 0.0645\n",
            "Epoch 936/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0716 - val_loss: 0.0629\n",
            "Epoch 937/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0726 - val_loss: 0.0637\n",
            "Epoch 938/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0722 - val_loss: 0.0627\n",
            "Epoch 939/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0697 - val_loss: 0.0640\n",
            "Epoch 940/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0708 - val_loss: 0.0627\n",
            "Epoch 941/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0718 - val_loss: 0.0639\n",
            "Epoch 942/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0725 - val_loss: 0.0628\n",
            "Epoch 943/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0721 - val_loss: 0.0631\n",
            "Epoch 944/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0710 - val_loss: 0.0622\n",
            "Epoch 945/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0708 - val_loss: 0.0634\n",
            "Epoch 946/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0695 - val_loss: 0.0624\n",
            "Epoch 947/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0709 - val_loss: 0.0632\n",
            "Epoch 948/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0708 - val_loss: 0.0619\n",
            "Epoch 949/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0699 - val_loss: 0.0632\n",
            "Epoch 950/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0712 - val_loss: 0.0623\n",
            "Epoch 951/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0711 - val_loss: 0.0637\n",
            "Epoch 952/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0731 - val_loss: 0.0622\n",
            "Epoch 953/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0715 - val_loss: 0.0641\n",
            "Epoch 954/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0718 - val_loss: 0.0623\n",
            "Epoch 955/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0714 - val_loss: 0.0636\n",
            "Epoch 956/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0733 - val_loss: 0.0617\n",
            "Epoch 957/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0715 - val_loss: 0.0626\n",
            "Epoch 958/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0684 - val_loss: 0.0616\n",
            "Epoch 959/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0709 - val_loss: 0.0628\n",
            "Epoch 960/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0699 - val_loss: 0.0618\n",
            "Epoch 961/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0701 - val_loss: 0.0623\n",
            "Epoch 962/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0690 - val_loss: 0.0616\n",
            "Epoch 963/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0705 - val_loss: 0.0624\n",
            "Epoch 964/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0711 - val_loss: 0.0614\n",
            "Epoch 965/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0711 - val_loss: 0.0623\n",
            "Epoch 966/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0700 - val_loss: 0.0614\n",
            "Epoch 967/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0686 - val_loss: 0.0620\n",
            "Epoch 968/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0706 - val_loss: 0.0616\n",
            "Epoch 969/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0686 - val_loss: 0.0620\n",
            "Epoch 970/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0681 - val_loss: 0.0614\n",
            "Epoch 971/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0697 - val_loss: 0.0628\n",
            "Epoch 972/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0688 - val_loss: 0.0619\n",
            "Epoch 973/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0711 - val_loss: 0.0639\n",
            "Epoch 974/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0720 - val_loss: 0.0618\n",
            "Epoch 975/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0713 - val_loss: 0.0638\n",
            "Epoch 976/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0717 - val_loss: 0.0620\n",
            "Epoch 977/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0725 - val_loss: 0.0640\n",
            "Epoch 978/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0706 - val_loss: 0.0623\n",
            "Epoch 979/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0718 - val_loss: 0.0645\n",
            "Epoch 980/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0729 - val_loss: 0.0626\n",
            "Epoch 981/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0697 - val_loss: 0.0639\n",
            "Epoch 982/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0702 - val_loss: 0.0624\n",
            "Epoch 983/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0714 - val_loss: 0.0639\n",
            "Epoch 984/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0722 - val_loss: 0.0624\n",
            "Epoch 985/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0701 - val_loss: 0.0644\n",
            "Epoch 986/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0702 - val_loss: 0.0629\n",
            "Epoch 987/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0730 - val_loss: 0.0649\n",
            "Epoch 988/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0726 - val_loss: 0.0626\n",
            "Epoch 989/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0718 - val_loss: 0.0653\n",
            "Epoch 990/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0728 - val_loss: 0.0628\n",
            "Epoch 991/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0720 - val_loss: 0.0650\n",
            "Epoch 992/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0709 - val_loss: 0.0628\n",
            "Epoch 993/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0728 - val_loss: 0.0646\n",
            "Epoch 994/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0698 - val_loss: 0.0619\n",
            "Epoch 995/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0697 - val_loss: 0.0637\n",
            "Epoch 996/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0705 - val_loss: 0.0621\n",
            "Epoch 997/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0693 - val_loss: 0.0636\n",
            "Epoch 998/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0710 - val_loss: 0.0616\n",
            "Epoch 999/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0712 - val_loss: 0.0635\n",
            "Epoch 1000/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0698 - val_loss: 0.0617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X94jTMXFTA6K",
        "colab_type": "text"
      },
      "source": [
        "## Model Selection<a id=\"Selection\"></a>\n",
        "The following models were compared\n",
        "- Bidirectional LSTM with Time Distributed(Best performance)\n",
        "- LSTM with Time Distributed\n",
        "- MLP\n",
        "- Bidirectional GRU with Time Distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "68by3caOTA6L",
        "colab_type": "code",
        "outputId": "57a4a7b7-8eba-4ee9-bdac-e94bb3f2ca40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "def ai_errors(predictions, observations, history = None) :\n",
        "    '''\n",
        "    PURPOSE: Provide descriptive statistics on the predicted output versus the observed measurments\n",
        "    METHOD:  Take the errors of the predictions and answers and then calculate standard descriptive statistics\n",
        "    INPUT:   predictions - 2D array of predictions of observed output\n",
        "             observations - 2D array measurements of observed output\n",
        "             history - Keras history model for displaying model loss, default is None if not available\n",
        "    OUTPUT:\n",
        "    '''\n",
        "    errors = []\n",
        "    for i in range(len(predictions)) :\n",
        "        for j in range(len(predictions[i])) :\n",
        "            # Calculate errors\n",
        "            error = predictions[i][j] - observations[i][j]\n",
        "            errors.append(error)\n",
        "    \n",
        "    # Display history and erros\n",
        "    plt.figure(1)\n",
        "    plt.hist(errors, bins = 50)\n",
        "    plt.title('error histogram')\n",
        "    plt.xlabel('error')\n",
        "    plt.ylabel('frequency')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.figure(2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    return pd.DataFrame(errors)\n",
        "\n",
        "# Predict values\n",
        "wind_predictions = model_wind.predict(X_test)\n",
        "lat_predictions = model_lat.predict(X_test)\n",
        "long_predictions = model_long.predict(X_test)\n",
        "\n",
        "# Scale back our predictions\n",
        "# Wind\n",
        "wind_predictions_scaled = [scaler.inverse_transform([[0,0,winds[0],0,0,0,0,0,0,0,0] for winds in prediction])\n",
        "                           for prediction in wind_predictions]\n",
        "y_wind_test_scaled = [scaler.inverse_transform([[0,0,winds[0],0,0,0,0,0,0,0,0] for winds in observation])\n",
        "                      for observation in y_test_wind]\n",
        "# Latitude\n",
        "lat_predictions_scaled = [scaler.inverse_transform([[lat[0],0,0,0,0,0,0,0,0,0,0] for lat in prediction])\n",
        "                          for prediction in lat_predictions]\n",
        "y_lat_test_scaled = [scaler.inverse_transform([[lat[0],0,0,0,0,0,0,0,0,0,0] for lat in observation])\n",
        "                     for observation in y_test_lat]\n",
        "# Longitude\n",
        "long_predictions_scaled = [scaler.inverse_transform([[0,long[0],0,0,0,0,0,0,0,0,0] for long in prediction])\n",
        "                           for prediction in long_predictions]\n",
        "y_long_test_scaled = [scaler.inverse_transform([[0,long[0],0,0,0,0,0,0,0,0,0] for long in observation])\n",
        "                      for observation in y_test_long]\n",
        "\n",
        "# Record wind predictions and observations\n",
        "print(\"Wind\")\n",
        "wind_predictions = [[pred[2] for pred in hurricanes_pred] for hurricanes_pred in wind_predictions_scaled]\n",
        "wind_observations = [[obsrv[2] for obsrv in hurricanes_obsrv] for hurricanes_obsrv in y_wind_test_scaled]\n",
        "\n",
        "# Present Errors\n",
        "ai_errors(wind_predictions, wind_observations, model_wind_history).describe()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wind\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.759751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>19.568697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-97.007904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-7.642652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.177456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>14.390935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.657005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count  4125.000000\n",
              "mean      2.759751\n",
              "std      19.568697\n",
              "min     -97.007904\n",
              "25%      -7.642652\n",
              "50%       4.177456\n",
              "75%      14.390935\n",
              "max      77.657005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "oB2lPcNjTA6N",
        "colab_type": "code",
        "outputId": "2cff8042-c022-4f3b-f765-11d17f0d4b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "print(\"Lat\")\n",
        "lat_predictions = [[pred[0] for pred in hurricanes_pred] for hurricanes_pred in lat_predictions_scaled]\n",
        "lat_observations = [[obsrv[0] for obsrv in hurricanes_obsrv] for hurricanes_obsrv in y_lat_test_scaled]\n",
        "ai_errors(lat_predictions, lat_observations, model_lat_history).describe()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBUKS1Or/A+sQjP1f29BWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.355589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.291035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-23.628085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-2.050962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.043959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.880657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>13.092535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count  4125.000000\n",
              "mean     -0.355589\n",
              "std       4.291035\n",
              "min     -23.628085\n",
              "25%      -2.050962\n",
              "50%       0.043959\n",
              "75%       1.880657\n",
              "max      13.092535"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSvYUSOdTA6R",
        "colab_type": "code",
        "outputId": "d5ff8afc-ee9c-4118-bcd7-229eec9eb9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "print(\"Long\")\n",
        "long_predictions = [[pred[1] for pred in hurricanes_pred] for hurricanes_pred in long_predictions_scaled]\n",
        "long_observations = [[obsrv[1] for obsrv in hurricanes_obsrv] for hurricanes_obsrv in y_long_test_scaled]\n",
        "ai_errors(long_predictions, long_observations, model_long_history).describe()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Long\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNzCfd6jaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.740573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.028288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-27.727831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-2.657374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.351699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.451127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>53.975726</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count  4125.000000\n",
              "mean      0.740573\n",
              "std       7.028288\n",
              "min     -27.727831\n",
              "25%      -2.657374\n",
              "50%       0.351699\n",
              "75%       3.451127\n",
              "max      53.975726"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynICeA8zTA6V",
        "colab_type": "text"
      },
      "source": [
        "## Model Evaluation & Benchmarks<a id=\"Benchmarks\"></a>\n",
        "\n",
        "The machine learning neural network will use 2 main methods of applied evaluation. The first will be evaluated compared to the other models that predict Atlantic hurricanes. The forecast errors have been loaded into each hurricane object corresponding to their forecast model; both the OFCL (official track) and the BCD5 (model using multivariate regression). The BCD5 model is \"the CLP5 (track) and DSF5 (intensity) models merged\" that uses the best track as input. BCD5 is similar to OCD5 except for the inputs to the models. The *O* is for operational input while the *B* is for best track input. The best track is only available post-season and is better than the operational input.\n",
        "\n",
        "### References\n",
        "http://www.hurricanecity.com/models/models.cgi?page=models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKQPNFILTA6X",
        "colab_type": "code",
        "outputId": "9d99e9ad-cba1-4633-f457-37227605daaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_data = hurdat2('data/hurdat2-1851-2017-050118.txt')\n",
        "\n",
        "# Parse in hurricanes\n",
        "hurricanes_2017 = dict()\n",
        "print(\"Transforming 2017 HURDAT2 into objects . . .\")\n",
        "for index, entry in test_data.hurricanes.iterrows() :\n",
        "    print(\"Transforming {}/{} entries from HURDAT2\".format(index + 1, len(dataset.hurricanes)), end = \"\\r\")\n",
        "    # Filter to capture 2017 data\n",
        "    if entry['storm_id'][-4:] != '2017' :\n",
        "        continue\n",
        "    if entry['storm_id'] not in hurricanes_2017 :\n",
        "        hurricanes_2017[entry['storm_id']] = hurricane(entry['storm_name'], entry['storm_id'])\n",
        "        storm_ids[entry['storm_id']] = entry['storm_name']\n",
        "    # Add entry to hurricane\n",
        "    hurricanes_2017[entry['storm_id']].add_entry(entry[2:])\n",
        "print(\"\\nDone!\")\n",
        "\n",
        "# Filter storms that have more than 6 entries. We need at least 6 to calculate 5 speed vectors\n",
        "storms_filter = [storm for storm in hurricanes_2017.values() if len(storm.entries) > 6]\n",
        "\n",
        "# Begin creating hurricane forecast and track predictions\n",
        "tracks = {\n",
        "    'storms' : [], # Reference storm\n",
        "    'inputs' : [], # The inputs for the ai\n",
        "    'valid_times' : [], # The valid time to compare to the error database\n",
        "}\n",
        "for index, storm in enumerate(storms_filter) :\n",
        "    # Create inputs to ai. ai requires scaled data as input\n",
        "    entries = [entry[1] for entry in sorted(storm.entries.items())] # Extracts data from data structure\n",
        "    \n",
        "    # Scale the entries\n",
        "    for start_index in range(1, len(entries) - 5) : # Go through each entry\n",
        "        # Build feature extraction\n",
        "        extracted_features = []\n",
        "        valid_time = None # Going to be set to the last element in the series\n",
        "        for pivot in range(start_index, start_index + 5) :\n",
        "            extracted_features.append(np.array(list(feature_extraction(entries[pivot], entries[pivot - 1]).values())))\n",
        "            if pivot is start_index + 4 : # We're on the last element\n",
        "                valid_time = entries[pivot]['entry_time']\n",
        "        \n",
        "        # If there's an incomplete value we can't process, skip it\n",
        "        if any(None in entry for entry in extracted_features) :\n",
        "            continue\n",
        "            \n",
        "        # Scale extracted features        \n",
        "        scaled_entries = scaler.transform(extracted_features)\n",
        "        \n",
        "        # Add to our results\n",
        "        tracks['storms'].append(storm)\n",
        "        tracks['inputs'].append(scaled_entries.tolist())\n",
        "        tracks['valid_times'].append(valid_time)\n",
        "        \n",
        "    print(\"\\rDone with track processing {}/{} storms\".format(index + 1, len(storms_filter)), end = '')\n",
        "tracks['inputs'] = np.array(tracks['inputs'])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transforming 2017 HURDAT2 into objects . . .\n",
            "Transforming 50303/20291 entries from HURDAT2\n",
            "Done!\n",
            "Done with track processing 18/18 storms"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b63CD4pbTA6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tracks['wind_predictions_raw'] = model_wind.predict(tracks['inputs'])\n",
        "tracks['lat_predictions_raw'] = model_lat.predict(tracks['inputs'])\n",
        "tracks['long_predictions_raw'] = model_long.predict(tracks['inputs'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGdhct5GTA6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to return the distance between two coordinates in nautical miles\n",
        "import math\n",
        "\n",
        "def distance(origin, destination):\n",
        "    lat1, lon1 = origin\n",
        "    lat2, lon2 = destination\n",
        "    radius = 6371 # km\n",
        "\n",
        "    dlat = math.radians(lat2-lat1)\n",
        "    dlon = math.radians(lon2-lon1)\n",
        "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
        "        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "    d = radius * c\n",
        "\n",
        "    return d * 0.539957 # km to nautical miles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ir6U5L-TA6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale back and store our wind predictions and our lat, long predictions\n",
        "tracks['wind_predictions'] = []\n",
        "tracks['lat_predictions'] = []\n",
        "tracks['long_predictions'] = []\n",
        "intensity_errors = {\n",
        "    '24' : [],\n",
        "    '48' : [],\n",
        "    '72' : [],\n",
        "    '96' : [],\n",
        "    '120' : []\n",
        "}\n",
        "track_errors = {\n",
        "    '24' : [],\n",
        "    '48' : [],\n",
        "    '72' : [],\n",
        "    '96' : [],\n",
        "    '120' : []\n",
        "}\n",
        "for index, prediction in enumerate(tracks['wind_predictions_raw']) :\n",
        "    # Use our standard scaler to scale the raw predictions back\n",
        "    winds_scaled = [scaler.inverse_transform([[0,0,winds[0],0,0,0,0,0,0,0,0] for winds in prediction])] # Index 2 is winds\n",
        "    lat_scaled = [scaler.inverse_transform([[lats[0],0,0,0,0,0,0,0,0,0,0] for lats in tracks['lat_predictions_raw'][index]])] # Index 0 is lat\n",
        "    long_scaled = [scaler.inverse_transform([[0,longs[0],0,0,0,0,0,0,0,0,0] for longs in tracks['long_predictions_raw'][index]])] # Index 1 is long\n",
        "    \n",
        "    # Extract the wind prediction from data structure and store into new data structure\n",
        "    for i in range(len(winds_scaled)) :        \n",
        "        # The new data structure is a tuple of (wind, storm_id, valid_time, forecast_time)\n",
        "        wind_predictions = []\n",
        "        lat_predictions = []\n",
        "        long_predictions = []\n",
        "        for step, pred in enumerate(winds_scaled[i]) :\n",
        "            wind = pred[2]\n",
        "            lat = lat_scaled[i][step][0]\n",
        "            long = long_scaled[i][step][1]\n",
        "            \n",
        "            storm_id = tracks['storms'][index].id\n",
        "            valid_time = tracks['valid_times'][index]\n",
        "            forecast_time = valid_time + datetime.timedelta(days = step + 1)\n",
        "            \n",
        "            # See if we can find the error\n",
        "            if forecast_time in hurricanes_2017[storm_id].entries :\n",
        "                wind_truth = hurricanes_2017[storm_id].entries[forecast_time]['max_wind']\n",
        "                lat_truth = hurricanes_2017[storm_id].entries[forecast_time]['lat']\n",
        "                long_truth = hurricanes_2017[storm_id].entries[forecast_time]['long']\n",
        "                intensity_error = abs(wind_truth - wind)\n",
        "                track_error = distance((lat_truth,long_truth), (lat, long))\n",
        "                \n",
        "                wind_predictions.append({\n",
        "                    'ai-wind' : wind,\n",
        "                    'truth' : wind_truth,\n",
        "                    'storm_id' : storm_id,\n",
        "                    'valid_time' : valid_time,\n",
        "                    'forecast_time' : forecast_time\n",
        "                })\n",
        "                lat_predictions.append({\n",
        "                    'ai-lat' : lat,\n",
        "                    'truth' : lat_truth,\n",
        "                    'storm_id' : storm_id,\n",
        "                    'valid_time' : valid_time,\n",
        "                    'forecast_time' : forecast_time\n",
        "                })\n",
        "                long_predictions.append({\n",
        "                    'ai-long' : long,\n",
        "                    'truth' : long_truth,\n",
        "                    'storm_id' : storm_id,\n",
        "                    'valid_time' : valid_time,\n",
        "                    'forecast_time' : forecast_time\n",
        "                })\n",
        "                if step is 0 :\n",
        "                    intensity_errors['24'].append(intensity_error)\n",
        "                    track_errors['24'].append(track_error)\n",
        "                if step is 1 :\n",
        "                    intensity_errors['48'].append(intensity_error)\n",
        "                    track_errors['48'].append(track_error)\n",
        "                if step is 2 :\n",
        "                    intensity_errors['72'].append(intensity_error)\n",
        "                    track_errors['72'].append(track_error)\n",
        "                if step is 3 :\n",
        "                    intensity_errors['96'].append(intensity_error)\n",
        "                    track_errors['96'].append(track_error)\n",
        "                if step is 4 :\n",
        "                    intensity_errors['120'].append(intensity_error)\n",
        "                    track_errors['120'].append(track_error)\n",
        "                    \n",
        "        tracks['wind_predictions'].append(wind_predictions)\n",
        "        tracks['lat_predictions'].append(lat_predictions)\n",
        "        tracks['long_predictions'].append(long_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlOkSiHUTA6h",
        "colab_type": "code",
        "outputId": "ac4830fd-c637-4c3d-99f6-7c83bde7ada3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['24']).describe()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>432.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.585760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>12.196465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.101230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.447418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>12.081357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>20.646273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>67.397625</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  432.000000\n",
              "mean    14.585760\n",
              "std     12.196465\n",
              "min      0.101230\n",
              "25%      5.447418\n",
              "50%     12.081357\n",
              "75%     20.646273\n",
              "max     67.397625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epSK3WCiTA6j",
        "colab_type": "code",
        "outputId": "17fe4415-8492-4cb5-bb71-d39edacf6505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['48']).describe()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>378.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>21.141168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>15.850025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.146635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.534712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>17.572685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>30.612768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>89.053513</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  378.000000\n",
              "mean    21.141168\n",
              "std     15.850025\n",
              "min      0.146635\n",
              "25%      8.534712\n",
              "50%     17.572685\n",
              "75%     30.612768\n",
              "max     89.053513"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c_0j5MzTA6l",
        "colab_type": "code",
        "outputId": "46cebfeb-9d93-4859-aafa-bbbb925f1d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['72']).describe()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>327.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>22.985324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>17.627245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.018525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.915049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>18.133106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>33.881014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>84.623600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  327.000000\n",
              "mean    22.985324\n",
              "std     17.627245\n",
              "min      0.018525\n",
              "25%      8.915049\n",
              "50%     18.133106\n",
              "75%     33.881014\n",
              "max     84.623600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-IRvW50TA6o",
        "colab_type": "code",
        "outputId": "3622dfb0-3467-4a42-91d0-7b00db47eb2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['96']).describe()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>285.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>24.171330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>18.389228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.253440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>9.753865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>18.296845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>35.760201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>79.143206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  285.000000\n",
              "mean    24.171330\n",
              "std     18.389228\n",
              "min      0.253440\n",
              "25%      9.753865\n",
              "50%     18.296845\n",
              "75%     35.760201\n",
              "max     79.143206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-ZM0xu8TA6s",
        "colab_type": "code",
        "outputId": "382dbe5f-63f3-48b8-b6a0-7bf5135ed07c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['120']).describe()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>249.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>25.021705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>20.283070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.130228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.925484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>20.522021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>36.960035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>85.252114</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  249.000000\n",
              "mean    25.021705\n",
              "std     20.283070\n",
              "min      0.130228\n",
              "25%      7.925484\n",
              "50%     20.522021\n",
              "75%     36.960035\n",
              "max     85.252114"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCERfJiJTA6w",
        "colab_type": "code",
        "outputId": "13dfa4f6-4082-47dc-f247-5aea6e32c978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['24']).describe()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>432.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>240.609969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>184.840750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>15.770467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>114.612848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>193.444850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>287.233055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1131.537887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   432.000000\n",
              "mean    240.609969\n",
              "std     184.840750\n",
              "min      15.770467\n",
              "25%     114.612848\n",
              "50%     193.444850\n",
              "75%     287.233055\n",
              "max    1131.537887"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c4Jc9hBTA60",
        "colab_type": "code",
        "outputId": "43ce9dcb-5459-4687-a9a3-96a6ff957bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['48']).describe()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>378.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>364.248800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>278.477151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>18.206023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>184.281999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>284.372761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>475.543215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1493.807317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   378.000000\n",
              "mean    364.248800\n",
              "std     278.477151\n",
              "min      18.206023\n",
              "25%     184.281999\n",
              "50%     284.372761\n",
              "75%     475.543215\n",
              "max    1493.807317"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40GaDhbcTA63",
        "colab_type": "code",
        "outputId": "13add805-a039-45e2-a0ee-22fb7e7069b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['72']).describe()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>327.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>519.140014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>361.619798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>15.360110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>271.859469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>402.143825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>697.380698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2089.255536</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   327.000000\n",
              "mean    519.140014\n",
              "std     361.619798\n",
              "min      15.360110\n",
              "25%     271.859469\n",
              "50%     402.143825\n",
              "75%     697.380698\n",
              "max    2089.255536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaulJVPBTA66",
        "colab_type": "code",
        "outputId": "1dd40e81-c91d-43af-f1cd-a518ea874e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['96']).describe()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>285.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>667.637950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>449.899953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>54.442301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>341.997106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>516.690425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>916.057419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2122.272550</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   285.000000\n",
              "mean    667.637950\n",
              "std     449.899953\n",
              "min      54.442301\n",
              "25%     341.997106\n",
              "50%     516.690425\n",
              "75%     916.057419\n",
              "max    2122.272550"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_lHpN7tTA69",
        "colab_type": "code",
        "outputId": "da160723-394d-49f1-e9cf-579afe90e105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['120']).describe()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>249.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>807.718650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>533.478419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>82.803137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>407.828285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>667.825127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1131.143418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2194.505428</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   249.000000\n",
              "mean    807.718650\n",
              "std     533.478419\n",
              "min      82.803137\n",
              "25%     407.828285\n",
              "50%     667.825127\n",
              "75%    1131.143418\n",
              "max    2194.505428"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRbJiBIjTA6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "187c6435-2fc6-4b13-ae4a-0197d9b3a26e"
      },
      "source": [
        "# Compare predictions when we can find them\n",
        "import errors\n",
        "errordb = errors.models.models(\"errors/1970-present_OFCL_v_BCD5_ind_ATL_TI_errors_noTDs.txt\")\n",
        "ai_wind_errors = []\n",
        "ai_track_errors = []\n",
        "bcd5_wind_errors = []\n",
        "bcd5_track_errors = []\n",
        "for index, prediction in enumerate(tracks['wind_predictions']) :\n",
        "    # Find the time stamp for the storm ID in the error database\n",
        "    if prediction == [] :\n",
        "      continue\n",
        "    valid_time = prediction[0]['valid_time']\n",
        "    storm_id = prediction[0]['storm_id']\n",
        "    # Check to see if we have error for this storm and at the valid time\n",
        "    if storm_id in errordb.models['BCD5'].storm and valid_time in errordb.models['BCD5'].storm[storm_id] :\n",
        "        print(\"Found {} at {}\".format(storm_id, valid_time))\n",
        "        # If we find it, compare\n",
        "        for i, forecast in enumerate(prediction) :\n",
        "            # See if we can find another prediction like that in the error database\n",
        "            if errordb.models['BCD5'].storm[storm_id][valid_time]['intensity_forecast'][forecast['forecast_time'].to_pydatetime()] :\n",
        "                print(\"\\tIntensity Truth: {}, AI forecast: {}, BCD5 forecast: {}\".format(forecast['truth'],\n",
        "                                                                                         forecast['ai-wind'],\n",
        "                                                                                         errordb.models['BCD5'].storm[storm_id][valid_time]['track_forecast'][forecast['forecast_time'].to_pydatetime()]))\n",
        "                print(\"\\tTrajectory Truth: {}, {}; AI forecast: {}, {} ; AI error: {} BCD5 error: {}\".format(tracks['lat_predictions'][index][i]['truth'],\n",
        "                                                                                                       tracks['long_predictions'][index][i]['truth'],\n",
        "                                                                                                       tracks['lat_predictions'][index][i]['ai-lat'],\n",
        "                                                                                                       tracks['long_predictions'][index][i]['ai-long'],\n",
        "                                                                                                       distance((tracks['lat_predictions'][index][i]['truth'], tracks['long_predictions'][index][i]['truth']), (tracks['lat_predictions'][index][i]['ai-lat'], tracks['long_predictions'][index][i]['ai-long'])),\n",
        "                                                                                                       errordb.models['BCD5'].storm[storm_id][valid_time]['intensity_forecast'][forecast['forecast_time'].to_pydatetime()]\n",
        "                                                                                                      ))\n",
        "                ai_wind_errors.append(abs(forecast['truth'] - forecast['ai-wind']))\n",
        "                ai_track_errors.append(abs(distance((tracks['lat_predictions'][index][i]['truth'], tracks['long_predictions'][index][i]['truth']), (tracks['lat_predictions'][index][i]['ai-lat'], tracks['long_predictions'][index][i]['ai-long']))))\n",
        "                bcd5_wind_errors.append(abs(errordb.models['BCD5'].storm[storm_id][valid_time]['track_forecast'][forecast['forecast_time'].to_pydatetime()]))                \n",
        "                bcd5_track_errors.append(abs(errordb.models['BCD5'].storm[storm_id][valid_time]['intensity_forecast'][forecast['forecast_time'].to_pydatetime()]))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found AL012017 at 2017-04-19 00:00:00\n",
            "Found AL012017 at 2017-04-19 06:00:00\n",
            "Found AL012017 at 2017-04-19 12:00:00\n",
            "Found AL012017 at 2017-04-19 18:00:00\n",
            "Found AL012017 at 2017-04-20 00:00:00\n",
            "Found AL012017 at 2017-04-20 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 43.730534352362156, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 40.0, 46.3; AI forecast: 30.1961571931839, 42.139999875426284 ; AI error: 622.8827655463484 BCD5 error: 391.5\n",
            "Found AL012017 at 2017-04-20 12:00:00\n",
            "Found AL012017 at 2017-04-20 18:00:00\n",
            "Found AL012017 at 2017-04-21 00:00:00\n",
            "Found AL012017 at 2017-04-21 06:00:00\n",
            "Found AL032017 at 2017-06-21 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 51.46191358566284, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 28.5, 93.4; AI forecast: 27.238521575927734, 89.28171446323395 ; AI error: 231.31798414220472 BCD5 error: 42.6\n",
            "Found AL032017 at 2017-06-21 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 53.59216146171093, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 28.05422157049179, 89.41801902055741 ; AI error: 234.5220196502288 BCD5 error: 36.6\n",
            "Found AL032017 at 2017-06-21 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 57.810750901699066, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 30.5, 93.8; AI forecast: 29.33522335290909, 89.37239650487899 ; AI error: 240.76918084660755 BCD5 error: 99.9\n",
            "Found AL032017 at 2017-06-21 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 55.23395627737045, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 31.6, 93.8; AI forecast: 29.711621820926666, 89.67624815702439 ; AI error: 241.25927865679594 BCD5 error: 106.4\n",
            "Found AL032017 at 2017-06-22 00:00:00\n",
            "Found AL032017 at 2017-06-22 06:00:00\n",
            "Found AL032017 at 2017-06-22 12:00:00\n",
            "Found AL032017 at 2017-06-22 18:00:00\n",
            "Found AL032017 at 2017-06-23 00:00:00\n",
            "Found AL032017 at 2017-06-23 06:00:00\n",
            "Found AL062017 at 2017-07-31 18:00:00\n",
            "Found AL062017 at 2017-08-01 00:00:00\n",
            "Found AL072017 at 2017-08-08 00:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 64.43138599395752, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 20.2, 90.9; AI forecast: 20.88818277567625, 87.09134194254875 ; AI error: 218.07402178811208 BCD5 error: 24.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 71.70543015003204, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 20.3, 95.5; AI forecast: 23.85841137766838, 90.18213511109352 ; AI error: 364.87703249344594 BCD5 error: 133.4\n",
            "Found AL072017 at 2017-08-08 06:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 64.3807652592659, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 20.4, 92.2; AI forecast: 22.347052311897276, 88.24026150107383 ; AI error: 250.34314858955574 BCD5 error: 28.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 71.14579409360886, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 20.0, 96.8; AI forecast: 25.912437653541563, 90.60576791167259 ; AI error: 493.0617292612526 BCD5 error: 160.4\n",
            "Found AL072017 at 2017-08-08 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 57.32506409287453, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 20.2, 93.3; AI forecast: 22.447391015291213, 89.66316034197807 ; AI error: 244.07574276419277 BCD5 error: 78.2\n",
            "Found AL072017 at 2017-08-08 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 46.1322614736855, BCD5 forecast: -31.0\n",
            "\tTrajectory Truth: 20.2, 94.4; AI forecast: 22.266428971290587, 91.13042684197426 ; AI error: 221.06225981050602 BCD5 error: 86.3\n",
            "Found AL072017 at 2017-08-09 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 45.802669040858746, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 20.3, 95.5; AI forecast: 21.49677326530218, 92.61077776551247 ; AI error: 177.26810724018276 BCD5 error: 58.6\n",
            "Found AL072017 at 2017-08-09 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 52.50982537865639, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 20.0, 96.8; AI forecast: 21.042959150671958, 91.37766072154045 ; AI error: 311.24447155598745 BCD5 error: 41.1\n",
            "Found AL072017 at 2017-08-09 12:00:00\n",
            "Found AL082017 at 2017-08-13 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 40.88336393237114, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 29.2, 72.1; AI forecast: 25.13622058033943, 72.94793305695057 ; AI error: 248.15656594512703 BCD5 error: 16.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 47.81903721392155, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 31.5, 72.3; AI forecast: 27.044010031223294, 74.32978282570839 ; AI error: 287.8670751073903 BCD5 error: 73.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 58.597803711891174, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 35.4, 69.5; AI forecast: 28.981495881080626, 73.27695645391941 ; AI error: 430.4088186965139 BCD5 error: 7.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 62.990936636924744, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 40.7, 56.2; AI forecast: 31.16489448547363, 71.71027309298515 ; AI error: 944.3154144640653 BCD5 error: 529.1\n",
            "Found AL082017 at 2017-08-13 12:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 43.22938725352287, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.7, 72.2; AI forecast: 26.964157986640927, 72.58289697170258 ; AI error: 165.50254176793123 BCD5 error: 37.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 55.35404101014137, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 32.3, 72.1; AI forecast: 29.738069689273836, 72.9389760941267 ; AI error: 159.7603120390278 BCD5 error: 85.2\n",
            "\tIntensity Truth: 85.0, AI forecast: 63.25091630220413, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 36.8, 67.1; AI forecast: 31.928244733810423, 70.74669585227966 ; AI error: 343.7618702518427 BCD5 error: 80.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 62.17347487807274, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 42.2, 52.0; AI forecast: 34.00503025054932, 68.05389316827059 ; AI error: 901.755746898575 BCD5 error: 673.2\n",
            "Found AL082017 at 2017-08-13 18:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 49.38544437289238, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 30.2, 72.3; AI forecast: 28.95733186006546, 71.92876906096936 ; AI error: 77.08713950800167 BCD5 error: 52.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 59.66084435582161, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 33.2, 71.8; AI forecast: 32.23035620450973, 71.36285926401615 ; AI error: 62.265063246026585 BCD5 error: 71.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 62.38507226109505, BCD5 forecast: -39.0\n",
            "\tTrajectory Truth: 38.2, 64.1; AI forecast: 34.406246876716615, 68.18866900503636 ; AI error: 301.6271168556004 BCD5 error: 213.5\n",
            "Found AL082017 at 2017-08-14 00:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 50.268638506531715, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 30.8, 72.3; AI forecast: 30.185769855976105, 71.826708522439 ; AI error: 44.2674572185634 BCD5 error: 49.2\n",
            "\tIntensity Truth: 75.0, AI forecast: 55.15623211860657, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 34.2, 71.0; AI forecast: 33.322717905044556, 70.85205462872982 ; AI error: 53.18756727011569 BCD5 error: 39.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 57.91961207985878, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 39.4, 60.4; AI forecast: 35.38254842758179, 67.19514817297458 ; AI error: 403.842629767232 BCD5 error: 375.7\n",
            "Found AL082017 at 2017-08-14 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 46.906389594078064, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.5, 72.3; AI forecast: 30.755536067485806, 72.01337861418725 ; AI error: 47.06280047434937 BCD5 error: 31.2\n",
            "\tIntensity Truth: 80.0, AI forecast: 52.28284202516079, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 35.4, 69.5; AI forecast: 33.768049359321594, 70.9333659440279 ; AI error: 120.91272793934222 BCD5 error: 74.5\n",
            "\tIntensity Truth: 80.0, AI forecast: 54.871335327625275, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 40.7, 56.2; AI forecast: 35.37841482162476, 66.85294793397188 ; AI error: 595.8637920074037 BCD5 error: 577.1\n",
            "Found AL082017 at 2017-08-14 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 50.94145514070988, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 32.3, 72.1; AI forecast: 31.62696839570999, 71.81895384788513 ; AI error: 42.86997792538936 BCD5 error: 23.6\n",
            "\tIntensity Truth: 85.0, AI forecast: 55.85400551557541, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 36.8, 67.1; AI forecast: 34.472172784805295, 70.19363900870084 ; AI error: 205.70134190915581 BCD5 error: 208.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 59.41744655370712, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 42.2, 52.0; AI forecast: 36.36503686904907, 65.37486338317395 ; AI error: 712.3052256135564 BCD5 error: 783.0\n",
            "Found AL082017 at 2017-08-14 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 56.016530096530914, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 33.2, 71.8; AI forecast: 32.00570853948593, 71.37790567278861 ; AI error: 74.81634029257783 BCD5 error: 31.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 62.999911308288574, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 38.2, 64.1; AI forecast: 34.997747564315794, 69.35489625781774 ; AI error: 317.9075162704087 BCD5 error: 320.0\n",
            "Found AL082017 at 2017-08-15 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 64.95211094617844, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 34.2, 71.0; AI forecast: 32.68390154838562, 71.06282983720303 ; AI error: 91.08168056812475 BCD5 error: 37.4\n",
            "\tIntensity Truth: 90.0, AI forecast: 67.67437368631363, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 39.4, 60.4; AI forecast: 35.666873526573184, 68.5695281535387 ; AI error: 448.65375921015726 BCD5 error: 436.5\n",
            "Found AL082017 at 2017-08-15 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 68.50493967533112, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 35.4, 69.5; AI forecast: 32.96881244182587, 70.89981379210948 ; AI error: 161.67590476040047 BCD5 error: 72.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 70.16743808984756, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 40.7, 56.2; AI forecast: 35.66172003746033, 68.22805624008178 ; AI error: 642.4261064704492 BCD5 error: 559.8\n",
            "Found AL082017 at 2017-08-15 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 71.97930574417114, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 36.8, 67.1; AI forecast: 33.28590898513794, 70.51956681013107 ; AI error: 269.7158277464891 BCD5 error: 141.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 72.40016609430313, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 42.2, 52.0; AI forecast: 36.11271567344666, 67.81443007290363 ; AI error: 820.2260445564368 BCD5 error: 655.2\n",
            "Found AL082017 at 2017-08-15 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 73.75801175832748, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 38.2, 64.1; AI forecast: 34.392238879203795, 69.91678627431392 ; AI error: 362.4871471850342 BCD5 error: 208.2\n",
            "Found AL082017 at 2017-08-16 00:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 74.45214688777924, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 39.4, 60.4; AI forecast: 35.97073411941528, 68.5032691821456 ; AI error: 436.3727884079106 BCD5 error: 221.8\n",
            "Found AL082017 at 2017-08-16 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 74.06167775392532, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 40.7, 56.2; AI forecast: 37.4466071844101, 66.65850705802441 ; AI error: 524.7378648050446 BCD5 error: 196.0\n",
            "Found AL082017 at 2017-08-16 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 71.38206660747528, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 42.2, 52.0; AI forecast: 39.06885976791382, 63.301038635522126 ; AI error: 547.6338483802631 BCD5 error: 155.3\n",
            "Found AL082017 at 2017-08-16 18:00:00\n",
            "Found AL082017 at 2017-08-17 00:00:00\n",
            "Found AL082017 at 2017-08-17 06:00:00\n",
            "Found AL082017 at 2017-08-17 12:00:00\n",
            "Found AL092017 at 2017-08-17 12:00:00\n",
            "Found AL092017 at 2017-08-17 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 40.90410351753235, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 13.2, 62.2; AI forecast: 14.843951427936553, 59.79313834458589 ; AI error: 171.45718034441452 BCD5 error: 66.7\n",
            "Found AL092017 at 2017-08-18 00:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 46.90796855837107, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 13.4, 64.0; AI forecast: 14.824154579639433, 61.29774742648005 ; AI error: 179.0755986236125 BCD5 error: 59.6\n",
            "Found AL092017 at 2017-08-18 06:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 50.34767381846905, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 13.5, 65.7; AI forecast: 14.565576273202897, 62.240393574535844 ; AI error: 211.42460448011337 BCD5 error: 34.3\n",
            "Found AL092017 at 2017-08-18 12:00:00\n",
            "Found AL092017 at 2017-08-18 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 40.89113801717758, BCD5 forecast: 52.0\n",
            "\tTrajectory Truth: 21.6, 92.4; AI forecast: 21.768464824557302, 88.61975048184395 ; AI error: 211.14351290461795 BCD5 error: 314.4\n",
            "Found AL092017 at 2017-08-19 00:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 41.48912973701954, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 22.0, 92.5; AI forecast: 21.9182440161705, 89.22493389844894 ; AI error: 182.43328458453962 BCD5 error: 282.0\n",
            "Found AL092017 at 2017-08-19 06:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 39.77324068546295, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 22.8, 92.6; AI forecast: 21.721195718646047, 90.6668285548687 ; AI error: 125.43269317119518 BCD5 error: 211.9\n",
            "Found AL092017 at 2017-08-19 12:00:00\n",
            "Found AL092017 at 2017-08-23 12:00:00\n",
            "Found AL092017 at 2017-08-23 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 41.77727468311787, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 24.4, 93.6; AI forecast: 22.698308181762695, 91.69609584212303 ; AI error: 146.35079947514785 BCD5 error: 87.0\n",
            "\tIntensity Truth: 105.0, AI forecast: 48.322865553200245, BCD5 forecast: -39.0\n",
            "\tTrajectory Truth: 27.1, 96.3; AI forecast: 24.79423941373825, 94.11712117791177 ; AI error: 181.79774565685793 BCD5 error: 221.0\n",
            "\tIntensity Truth: 50.0, AI forecast: 52.576401084661484, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 29.0, 97.5; AI forecast: 27.158498299121856, 91.65971178412437 ; AI error: 328.49261246412146 BCD5 error: 263.5\n",
            "\tIntensity Truth: 35.0, AI forecast: 55.74288189411163, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 29.0, 97.2; AI forecast: 30.029446852207183, 89.27815338373185 ; AI error: 418.40775377347404 BCD5 error: 231.1\n",
            "\tIntensity Truth: 40.0, AI forecast: 53.40160772204399, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 32.76362516880035, 86.9157366335392 ; AI error: 532.887534809984 BCD5 error: 251.5\n",
            "Found AL092017 at 2017-08-24 00:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 47.32129707932472, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 25.0, 94.4; AI forecast: 22.871320694684982, 91.97501654624939 ; AI error: 184.4997778597971 BCD5 error: 89.3\n",
            "\tIntensity Truth: 115.0, AI forecast: 53.89985114336014, BCD5 forecast: -47.0\n",
            "\tTrajectory Truth: 27.8, 96.8; AI forecast: 24.806503921747208, 94.52296214699746 ; AI error: 217.52622480275522 BCD5 error: 225.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 60.42496085166931, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 29.2, 97.4; AI forecast: 27.293624556064604, 92.03785094618797 ; AI error: 305.7897273176466 BCD5 error: 243.4\n",
            "\tIntensity Truth: 35.0, AI forecast: 60.175733268260956, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 28.8, 96.8; AI forecast: 29.823178112506866, 89.12143875360489 ; AI error: 406.5848326104696 BCD5 error: 272.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 56.13583669066429, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 32.91428699493408, 87.0768025636673 ; AI error: 514.78148539432 BCD5 error: 347.6\n",
            "Found AL092017 at 2017-08-24 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 56.29481375217438, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 25.6, 95.1; AI forecast: 23.166140151023864, 91.61390273571014 ; AI error: 240.17929807035702 BCD5 error: 92.7\n",
            "\tIntensity Truth: 105.0, AI forecast: 65.85695683956146, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 28.2, 97.1; AI forecast: 25.165439707040786, 94.1343432366848 ; AI error: 241.85940174632373 BCD5 error: 190.4\n",
            "\tIntensity Truth: 40.0, AI forecast: 66.67784512042999, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 29.3, 97.6; AI forecast: 27.147829329967497, 91.1175730407238 ; AI error: 366.3969082428581 BCD5 error: 223.3\n",
            "\tIntensity Truth: 40.0, AI forecast: 62.75890529155731, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 28.6, 96.5; AI forecast: 30.03500363826752, 88.43449973464013 ; AI error: 430.8305585711833 BCD5 error: 300.0\n",
            "\tIntensity Truth: 40.0, AI forecast: 56.97736442089081, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 33.72159135341644, 85.85042783021927 ; AI error: 579.2836903326784 BCD5 error: 399.4\n",
            "Found AL092017 at 2017-08-24 12:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 69.8872622847557, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 26.3, 95.8; AI forecast: 24.254826366901398, 90.85671548843384 ; AI error: 295.1010122181851 BCD5 error: 65.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 74.47853058576584, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 28.7, 97.3; AI forecast: 26.060126197338104, 92.40749778151513 ; AI error: 305.17153679518566 BCD5 error: 100.0\n",
            "\tIntensity Truth: 35.0, AI forecast: 70.5933928489685, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 29.1, 97.5; AI forecast: 28.56879242658615, 89.38273380994796 ; AI error: 428.0435706496717 BCD5 error: 129.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 64.27542239427567, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.5, 96.2; AI forecast: 32.219862973690034, 86.10614047050476 ; AI error: 568.299080793462 BCD5 error: 246.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 55.06995052099228, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 35.93451249599457, 82.81126183569431 ; AI error: 757.5440293730866 BCD5 error: 354.9\n",
            "Found AL092017 at 2017-08-24 18:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 76.84448301792145, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 27.1, 96.3; AI forecast: 24.922733801603314, 89.61193845272064 ; AI error: 383.7547001311964 BCD5 error: 40.1\n",
            "\tIntensity Truth: 50.0, AI forecast: 79.45541143417358, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 29.0, 97.5; AI forecast: 27.430350482463837, 91.34523794054985 ; AI error: 338.93779436014626 BCD5 error: 33.9\n",
            "\tIntensity Truth: 35.0, AI forecast: 73.97929519414902, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 29.0, 97.2; AI forecast: 30.799176251888277, 88.10944439768791 ; AI error: 485.17159443961214 BCD5 error: 88.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 64.68242436647415, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 34.51666462421417, 84.5584987103939 ; AI error: 686.5062960223393 BCD5 error: 239.6\n",
            "\tIntensity Truth: 45.0, AI forecast: 53.71581822633743, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 38.17090146541595, 79.8313064187765 ; AI error: 923.5685322824768 BCD5 error: 342.7\n",
            "Found AL092017 at 2017-08-25 00:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 84.099200963974, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 27.8, 96.8; AI forecast: 27.00987173318863, 89.44625668525696 ; AI error: 394.77086025099106 BCD5 error: 42.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 84.67318773269653, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.2, 97.4; AI forecast: 30.961354196071625, 90.37096728682518 ; AI error: 380.10901595565633 BCD5 error: 79.4\n",
            "\tIntensity Truth: 35.0, AI forecast: 74.69135880470276, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 28.8, 96.8; AI forecast: 34.714556503295896, 86.23438003063202 ; AI error: 645.1845863922557 BCD5 error: 188.1\n",
            "\tIntensity Truth: 40.0, AI forecast: 62.60508418083191, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 38.40998520851135, 80.78440221548081 ; AI error: 953.6797032290501 BCD5 error: 337.7\n",
            "\tIntensity Truth: 45.0, AI forecast: 50.13040781021118, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 41.99710803031921, 75.23306420743465 ; AI error: 1196.4651636668577 BCD5 error: 409.9\n",
            "Found AL092017 at 2017-08-25 06:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 92.76875019073486, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 28.2, 97.1; AI forecast: 29.16753293275833, 89.66364193558692 ; AI error: 395.90395107707093 BCD5 error: 32.0\n",
            "\tIntensity Truth: 40.0, AI forecast: 88.561110496521, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 29.3, 97.6; AI forecast: 33.61196668148041, 90.49245436191559 ; AI error: 446.4997407699373 BCD5 error: 90.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 74.45364266633987, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.6, 96.5; AI forecast: 37.34461436271667, 85.06130957007409 ; AI error: 778.3556038313894 BCD5 error: 263.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 59.937264025211334, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 41.00727763175964, 79.0722012758255 ; AI error: 1101.46661197777 BCD5 error: 415.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 46.389088556170464, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 44.59828171730041, 72.7603913486004 ; AI error: 1345.8514423251952 BCD5 error: 450.7\n",
            "Found AL092017 at 2017-08-25 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 97.80538082122803, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 28.7, 97.3; AI forecast: 29.96483072042465, 91.33231970071793 ; AI error: 321.41902151832346 BCD5 error: 28.9\n",
            "\tIntensity Truth: 35.0, AI forecast: 91.00172877311707, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 29.1, 97.5; AI forecast: 34.126034355163576, 92.23348987102509 ; AI error: 404.3038299625446 BCD5 error: 139.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 74.32885259389877, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.5, 96.2; AI forecast: 37.71924042701721, 87.0981486916542 ; AI error: 717.469101757369 BCD5 error: 329.1\n",
            "\tIntensity Truth: 40.0, AI forecast: 58.18187713623047, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 41.4643490076065, 80.80477159023285 ; AI error: 1044.3464772773639 BCD5 error: 473.7\n",
            "\tIntensity Truth: 40.0, AI forecast: 44.166392385959625, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 45.28662173748016, 74.06049738526345 ; AI error: 1287.922837090686 BCD5 error: 456.3\n",
            "Found AL092017 at 2017-08-25 18:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 101.91582202911377, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 29.0, 97.5; AI forecast: 29.917424929141998, 92.45752586126328 ; AI error: 269.2776479770813 BCD5 error: 49.5\n",
            "\tIntensity Truth: 35.0, AI forecast: 92.28880763053894, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.0, 97.2; AI forecast: 33.75523848533631, 94.59906750321389 ; AI error: 315.06636387167015 BCD5 error: 202.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 73.37118089199066, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 37.307559108734125, 90.21856665015221 ; AI error: 606.4226778070353 BCD5 error: 396.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 56.35978311300278, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 41.321337056159976, 84.05796185433864 ; AI error: 916.0574190693651 BCD5 error: 526.4\n",
            "\tIntensity Truth: 35.0, AI forecast: 42.92548447847366, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 45.250180816650385, 76.2412024229765 ; AI error: 1183.060620695323 BCD5 error: 485.9\n",
            "Found AL092017 at 2017-08-26 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 104.4456696510315, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 29.2, 97.4; AI forecast: 30.4400572180748, 93.45914221405982 ; AI error: 218.34961256401252 BCD5 error: 71.1\n",
            "\tIntensity Truth: 35.0, AI forecast: 91.99553489685059, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 28.8, 96.8; AI forecast: 34.477386355400085, 95.91249197721481 ; AI error: 343.8733968969809 BCD5 error: 236.8\n",
            "\tIntensity Truth: 40.0, AI forecast: 71.36661857366562, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 38.38781814575195, 91.26870525479316 ; AI error: 645.6585914593618 BCD5 error: 462.1\n",
            "\tIntensity Truth: 45.0, AI forecast: 54.75386455655098, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 42.49544486999511, 83.95471598505974 ; AI error: 945.4079647498625 BCD5 error: 587.5\n",
            "Found AL092017 at 2017-08-26 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 100.55554389953613, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 29.3, 97.6; AI forecast: 32.29984719753266, 93.79839777946472 ; AI error: 266.1902166266549 BCD5 error: 55.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 91.09692454338074, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.6, 96.5; AI forecast: 37.20963530540466, 93.41801683306694 ; AI error: 539.6711278253572 BCD5 error: 243.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 71.61891281604767, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 40.855665016174314, 84.73427020311355 ; AI error: 917.5454602322991 BCD5 error: 452.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 54.46067005395889, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 44.339578366279596, 74.56628937721253 ; AI error: 1274.6877086583054 BCD5 error: 539.4\n",
            "Found AL092017 at 2017-08-26 12:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 91.89241170883179, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 29.1, 97.5; AI forecast: 32.01221911907196, 93.74058280587197 ; AI error: 261.406403706678 BCD5 error: 82.2\n",
            "\tIntensity Truth: 40.0, AI forecast: 89.84813928604126, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.5, 96.2; AI forecast: 36.184771394729616, 92.27620247602462 ; AI error: 502.363164053694 BCD5 error: 245.0\n",
            "\tIntensity Truth: 40.0, AI forecast: 74.85435485839844, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 39.51513600349426, 83.32827461957932 ; AI error: 880.139032301397 BCD5 error: 389.4\n",
            "\tIntensity Truth: 40.0, AI forecast: 59.78378817439079, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 43.710496711730954, 75.16194775402546 ; AI error: 1192.639622807641 BCD5 error: 381.2\n",
            "Found AL092017 at 2017-08-26 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 77.90920615196228, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 29.0, 97.2; AI forecast: 30.604386079311368, 93.81304568648338 ; AI error: 201.02478774494202 BCD5 error: 80.7\n",
            "\tIntensity Truth: 40.0, AI forecast: 77.80300378799438, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 34.10315833091736, 92.49321126937866 ; AI error: 384.41377500738713 BCD5 error: 219.0\n",
            "\tIntensity Truth: 45.0, AI forecast: 63.66561621427536, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 37.749830913543704, 85.20245380401612 ; AI error: 715.5241363617702 BCD5 error: 335.8\n",
            "\tIntensity Truth: 35.0, AI forecast: 48.01739580929279, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 41.723316717147824, 78.79955655038357 ; AI error: 960.0722247704025 BCD5 error: 291.8\n",
            "Found AL092017 at 2017-08-27 00:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 56.46746352314949, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 28.8, 96.8; AI forecast: 28.877620255947114, 93.61453076601029 ; AI error: 167.5971849084908 BCD5 error: 91.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 49.89570513367653, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 31.845886540412902, 93.38910086750985 ; AI error: 242.56462397387816 BCD5 error: 216.6\n",
            "\tIntensity Truth: 45.0, AI forecast: 36.87665194272995, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 34.312413120269774, 87.6494784116745 ; AI error: 452.03051518416265 BCD5 error: 288.9\n",
            "Found AL092017 at 2017-08-27 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 30.544060617685318, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.6, 96.5; AI forecast: 27.562607848644255, 93.96150200366975 ; AI error: 148.19002527353624 BCD5 error: 86.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 19.122970402240753, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 29.046536338329314, 94.78537979125977 ; AI error: 57.946201827811215 BCD5 error: 182.2\n",
            "\tIntensity Truth: 40.0, AI forecast: 20.621579885482788, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 30.723357176780702, 89.99911936521531 ; AI error: 203.27285948500509 BCD5 error: 202.7\n",
            "Found AL092017 at 2017-08-27 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 15.140986740589142, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.5, 96.2; AI forecast: 26.642075431346893, 94.20169172883034 ; AI error: 154.11921487493595 BCD5 error: 49.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 13.517530858516693, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 27.98506553173065, 95.55878182053567 ; AI error: 52.39760074946753 BCD5 error: 134.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 20.066483318805695, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 28.922875130176543, 90.67116289734841 ; AI error: 159.13112174314887 BCD5 error: 109.9\n",
            "Found AL092017 at 2017-08-27 18:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 27.612838447093964, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 27.95180892944336, 93.9868670642376 ; AI error: 104.76741637936992 BCD5 error: 48.0\n",
            "\tIntensity Truth: 45.0, AI forecast: 31.461244076490402, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 29.538587152957916, 93.84707943201064 ; AI error: 65.05187304592292 BCD5 error: 120.5\n",
            "\tIntensity Truth: 35.0, AI forecast: 33.868452459573746, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 30.568563997745514, 89.20739982128143 ; AI error: 201.19855686719094 BCD5 error: 99.4\n",
            "Found AL092017 at 2017-08-28 00:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 37.853296250104904, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 28.326405704021454, 92.93631808757783 ; AI error: 130.50435524996726 BCD5 error: 42.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 39.01979334652424, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 29.57653762102127, 93.2837268292904 ; AI error: 48.80100427482989 BCD5 error: 72.8\n",
            "Found AL092017 at 2017-08-28 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 40.23468002676964, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 27.68801667690277, 93.26713898181916 ; AI error: 95.22143446087763 BCD5 error: 47.0\n",
            "\tIntensity Truth: 40.0, AI forecast: 42.3417229950428, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 28.561994957923886, 93.24644402265548 ; AI error: 53.63149550212227 BCD5 error: 44.8\n",
            "Found AL092017 at 2017-08-28 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 42.7255542203784, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 27.238813722133635, 92.18071467280387 ; AI error: 140.9372914045171 BCD5 error: 52.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 49.609820544719696, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 28.406302058696745, 92.15103222727777 ; AI error: 120.91635000112213 BCD5 error: 12.0\n",
            "Found AL092017 at 2017-08-28 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 49.87323507666588, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 27.695873081684113, 91.20975412726402 ; AI error: 165.56578531688305 BCD5 error: 19.8\n",
            "\tIntensity Truth: 35.0, AI forecast: 55.685811787843704, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 28.90344853401184, 90.84595085382462 ; AI error: 155.49420895487347 BCD5 error: 39.2\n",
            "Found AL092017 at 2017-08-29 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 51.25287488102913, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 27.592511904239654, 90.6239938557148 ; AI error: 185.40940979171407 BCD5 error: 24.6\n",
            "Found AL092017 at 2017-08-29 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 55.10067030787468, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 27.661888742446898, 90.60335316061973 ; AI error: 189.39839736468693 BCD5 error: 30.5\n",
            "Found AL092017 at 2017-08-29 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 49.76370893418789, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 27.506147778034208, 90.5750001847744 ; AI error: 215.25873736001978 BCD5 error: 31.8\n",
            "Found AL092017 at 2017-08-29 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 52.691819593310356, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 27.533971512317656, 90.0650705575943 ; AI error: 243.39476284077872 BCD5 error: 19.6\n",
            "Found AL092017 at 2017-08-30 00:00:00\n",
            "Found AL092017 at 2017-08-30 06:00:00\n",
            "Found AL092017 at 2017-08-30 12:00:00\n",
            "Found AL092017 at 2017-08-30 18:00:00\n",
            "Found AL092017 at 2017-08-31 00:00:00\n",
            "Found AL092017 at 2017-08-31 06:00:00\n",
            "Found AL092017 at 2017-08-31 12:00:00\n",
            "Found AL092017 at 2017-08-31 18:00:00\n",
            "Found AL092017 at 2017-09-01 00:00:00\n",
            "Found AL112017 at 2017-08-31 06:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 64.3290576338768, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 17.9, 36.1; AI forecast: 17.842527693510053, 38.80241202712058 ; AI error: 154.4622895690206 BCD5 error: 12.0\n",
            "\tIntensity Truth: 100.0, AI forecast: 71.95009112358093, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 19.1, 41.1; AI forecast: 19.352201004326343, 43.21866781413554 ; AI error: 121.06096493333598 BCD5 error: 84.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 71.12263560295105, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 18.2, 46.7; AI forecast: 21.259195032715798, 49.21963704675436 ; AI error: 232.39464615445243 BCD5 error: 310.6\n",
            "\tIntensity Truth: 105.0, AI forecast: 70.69729208946228, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 17.0, 51.5; AI forecast: 22.97263046503067, 53.84267920926213 ; AI error: 382.15705653044597 BCD5 error: 581.4\n",
            "\tIntensity Truth: 135.0, AI forecast: 71.64525270462036, BCD5 forecast: -59.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 24.43483349084854, 57.039407551009205 ; AI error: 471.7762127342113 BCD5 error: 848.5\n",
            "Found AL112017 at 2017-08-31 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 76.97796404361725, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 18.4, 37.3; AI forecast: 17.954926627874375, 39.21601275801658 ; AI error: 112.51579119942309 BCD5 error: 18.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 77.24435240030289, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 18.9, 42.6; AI forecast: 19.343104855716227, 42.989203721284866 ; AI error: 34.57237313024511 BCD5 error: 168.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 73.70577424764633, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 17.9, 47.9; AI forecast: 21.394328987598417, 48.681732596457 ; AI error: 214.40524046341417 BCD5 error: 423.4\n",
            "\tIntensity Truth: 110.0, AI forecast: 72.3201596736908, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 16.8, 52.6; AI forecast: 23.239752733707427, 52.98733165338635 ; AI error: 387.2618474261659 BCD5 error: 712.6\n",
            "\tIntensity Truth: 150.0, AI forecast: 71.709204018116, BCD5 forecast: -68.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 24.585209178924558, 56.56942473058589 ; AI error: 478.44254452114814 BCD5 error: 987.4\n",
            "Found AL112017 at 2017-08-31 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 79.3756553530693, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 18.8, 38.5; AI forecast: 17.986648899316787, 38.84252165257931 ; AI error: 52.58866290484016 BCD5 error: 48.8\n",
            "\tIntensity Truth: 95.0, AI forecast: 78.40874820947647, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 18.7, 44.1; AI forecast: 19.432934744656084, 42.59784871488809 ; AI error: 95.92987545519135 BCD5 error: 242.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 74.605932533741, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 17.6, 49.2; AI forecast: 21.53016739785671, 48.028485628962514 ; AI error: 245.09526598933576 BCD5 error: 515.9\n",
            "\tIntensity Truth: 115.0, AI forecast: 72.86087214946747, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 16.7, 53.9; AI forecast: 23.260142135620118, 52.65175035893917 ; AI error: 400.1127747009742 BCD5 error: 810.4\n",
            "\tIntensity Truth: 155.0, AI forecast: 69.74788576364517, BCD5 forecast: -66.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 24.451409220695496, 55.923760742740704 ; AI error: 489.2457925926613 BCD5 error: 1087.7\n",
            "Found AL112017 at 2017-09-01 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 87.73039221763611, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 19.1, 39.7; AI forecast: 18.497163128852844, 39.1212659984827 ; AI error: 48.908558097591424 BCD5 error: 51.3\n",
            "\tIntensity Truth: 95.0, AI forecast: 88.0924916267395, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 18.5, 45.5; AI forecast: 20.214068219810724, 42.49587381184101 ; AI error: 198.86277594316283 BCD5 error: 277.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 83.5034430027008, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 17.3, 50.4; AI forecast: 22.305888986587522, 48.12078033089637 ; AI error: 326.94998039586454 BCD5 error: 555.4\n",
            "\tIntensity Truth: 125.0, AI forecast: 78.52634936571121, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 16.6, 55.1; AI forecast: 23.880638897418976, 52.41333608627319 ; AI error: 462.5446125616182 BCD5 error: 848.4\n",
            "\tIntensity Truth: 155.0, AI forecast: 73.13421368598938, BCD5 forecast: -65.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 24.85637943744659, 55.2622626427561 ; AI error: 543.1948844692351 BCD5 error: 1113.5\n",
            "Found AL112017 at 2017-09-01 06:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 98.06363105773926, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 19.1, 41.1; AI forecast: 19.142466314136982, 39.819927752017975 ; AI error: 72.66039609842413 BCD5 error: 75.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 100.03119111061096, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 18.2, 46.7; AI forecast: 20.981841357052325, 43.664575167000294 ; AI error: 239.51656545120716 BCD5 error: 309.7\n",
            "\tIntensity Truth: 105.0, AI forecast: 93.06157231330872, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 17.0, 51.5; AI forecast: 22.939509046077728, 49.03898687511682 ; AI error: 382.666255469037 BCD5 error: 600.4\n",
            "\tIntensity Truth: 135.0, AI forecast: 86.06650233268738, BCD5 forecast: -46.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 24.225485199689864, 52.980707621574396 ; AI error: 496.5441605075484 BCD5 error: 891.6\n",
            "\tIntensity Truth: 155.0, AI forecast: 79.4036015868187, BCD5 forecast: -64.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 25.905498242378233, 54.787321999482806 ; AI error: 632.0623952712034 BCD5 error: 1148.3\n",
            "Found AL112017 at 2017-09-01 12:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 106.73806190490723, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 18.9, 42.6; AI forecast: 19.538997898995877, 41.39782851338386 ; AI error: 78.21181576196666 BCD5 error: 106.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 108.22163224220276, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 17.9, 47.9; AI forecast: 21.250310108065605, 45.31331258267164 ; AI error: 248.72993271440333 BCD5 error: 345.4\n",
            "\tIntensity Truth: 110.0, AI forecast: 99.97567415237427, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 16.8, 52.6; AI forecast: 22.814667987823487, 50.39027749449014 ; AI error: 382.0615437061997 BCD5 error: 629.5\n",
            "\tIntensity Truth: 150.0, AI forecast: 92.27044105529785, BCD5 forecast: -59.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 24.64868000745773, 53.250247636809945 ; AI error: 541.2268969220466 BCD5 error: 914.6\n",
            "\tIntensity Truth: 155.0, AI forecast: 85.88226914405823, BCD5 forecast: -62.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 26.66658341884613, 54.67790285050869 ; AI error: 702.1156400569073 BCD5 error: 1164.7\n",
            "Found AL112017 at 2017-09-01 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 109.63733553886414, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 18.7, 44.1; AI forecast: 19.549634949862956, 42.778531953692436 ; AI error: 90.67238243018616 BCD5 error: 135.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 111.00534081459045, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 17.6, 49.2; AI forecast: 20.868655824661253, 46.787223039567465 ; AI error: 239.19795942836026 BCD5 error: 356.6\n",
            "\tIntensity Truth: 115.0, AI forecast: 103.31941723823547, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 16.7, 53.9; AI forecast: 22.866952019929887, 50.90905852392316 ; AI error: 406.9548029017526 BCD5 error: 627.4\n",
            "\tIntensity Truth: 155.0, AI forecast: 96.75876140594482, BCD5 forecast: -63.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 24.952264976501464, 53.48938387595117 ; AI error: 579.692580923274 BCD5 error: 893.9\n",
            "\tIntensity Truth: 150.0, AI forecast: 90.29759883880615, BCD5 forecast: -56.0\n",
            "\tTrajectory Truth: 18.6, 64.7; AI forecast: 26.933819091320036, 54.136467632278794 ; AI error: 768.9714533599082 BCD5 error: 1131.6\n",
            "Found AL112017 at 2017-09-02 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 106.05249047279358, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 18.5, 45.5; AI forecast: 19.157185150682924, 44.51727226227521 ; AI error: 68.37865534506167 BCD5 error: 119.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 109.00355935096741, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 17.3, 50.4; AI forecast: 20.633823972940444, 47.92453771829605 ; AI error: 244.57017596514234 BCD5 error: 313.3\n",
            "\tIntensity Truth: 125.0, AI forecast: 104.01273250579834, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 16.6, 55.1; AI forecast: 22.644118815660477, 51.842984690517184 ; AI error: 406.90828260951713 BCD5 error: 558.1\n",
            "\tIntensity Truth: 155.0, AI forecast: 98.45020532608032, BCD5 forecast: -64.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 24.468987560272215, 53.7589838784188 ; AI error: 576.4001940401373 BCD5 error: 780.3\n",
            "\tIntensity Truth: 150.0, AI forecast: 92.3927903175354, BCD5 forecast: -56.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 26.320446491241455, 53.850750353559846 ; AI error: 805.6353807508198 BCD5 error: 1005.7\n",
            "Found AL112017 at 2017-09-02 06:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 102.90195107460022, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 18.2, 46.7; AI forecast: 19.461707109212874, 45.837207576632494 ; AI error: 90.23511262510254 BCD5 error: 78.0\n",
            "\tIntensity Truth: 105.0, AI forecast: 108.72102618217468, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 17.0, 51.5; AI forecast: 21.347267162799834, 49.3778486020863 ; AI error: 287.4039143760029 BCD5 error: 240.0\n",
            "\tIntensity Truth: 135.0, AI forecast: 105.07334589958191, BCD5 forecast: -40.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 23.52356263399124, 52.86137372776866 ; AI error: 461.0423752325689 BCD5 error: 439.3\n",
            "\tIntensity Truth: 155.0, AI forecast: 100.15268921852112, BCD5 forecast: -62.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 25.57474592924118, 54.61244204845279 ; AI error: 623.3361042831747 BCD5 error: 597.1\n",
            "\tIntensity Truth: 145.0, AI forecast: 95.36103248596191, BCD5 forecast: -48.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 27.51172033548355, 54.53791374173015 ; AI error: 857.1965957192934 BCD5 error: 793.3\n",
            "Found AL112017 at 2017-09-02 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 103.01501274108887, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 17.9, 47.9; AI forecast: 19.87159846872091, 47.568691761791705 ; AI error: 119.8623975391669 BCD5 error: 71.9\n",
            "\tIntensity Truth: 110.0, AI forecast: 111.52063965797424, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 16.8, 52.6; AI forecast: 22.088937759399414, 50.84077991172671 ; AI error: 332.7891797458555 BCD5 error: 210.3\n",
            "\tIntensity Truth: 150.0, AI forecast: 109.38004493713379, BCD5 forecast: -56.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 24.74774911403656, 54.13986570760608 ; AI error: 524.9930146514218 BCD5 error: 372.2\n",
            "\tIntensity Truth: 155.0, AI forecast: 105.78458547592163, BCD5 forecast: -61.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 27.112989580631254, 55.70964323328808 ; AI error: 685.0131425948377 BCD5 error: 491.2\n",
            "\tIntensity Truth: 145.0, AI forecast: 98.31679105758667, BCD5 forecast: -44.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 29.66403578519821, 55.69036666480824 ; AI error: 919.5639761811376 BCD5 error: 673.6\n",
            "Found AL112017 at 2017-09-02 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 103.67285490036011, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 17.6, 49.2; AI forecast: 19.650031293928624, 48.96234990060329 ; AI error: 123.82521589919308 BCD5 error: 77.4\n",
            "\tIntensity Truth: 115.0, AI forecast: 113.26202630996704, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 16.7, 53.9; AI forecast: 22.01427525281906, 52.47310880571604 ; AI error: 329.140494502326 BCD5 error: 203.2\n",
            "\tIntensity Truth: 155.0, AI forecast: 112.54295110702515, BCD5 forecast: -60.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 24.776543164253233, 55.698640398494895 ; AI error: 512.0134298617625 BCD5 error: 348.0\n",
            "\tIntensity Truth: 150.0, AI forecast: 106.43596887588501, BCD5 forecast: -55.0\n",
            "\tTrajectory Truth: 18.6, 64.7; AI forecast: 27.61869165897369, 57.30974536463618 ; AI error: 677.6378968198045 BCD5 error: 450.6\n",
            "\tIntensity Truth: 145.0, AI forecast: 102.81823754310608, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 30.351373195648193, 58.36060063652694 ; AI error: 871.3716986390767 BCD5 error: 640.5\n",
            "Found AL112017 at 2017-09-03 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 103.71796607971191, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 17.3, 50.4; AI forecast: 19.28072760403156, 50.6678827188909 ; AI error: 119.90019915321518 BCD5 error: 74.0\n",
            "\tIntensity Truth: 125.0, AI forecast: 114.3293821811676, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 16.6, 55.1; AI forecast: 21.395423972606658, 54.15266227722168 ; AI error: 292.89540261956404 BCD5 error: 180.8\n",
            "\tIntensity Truth: 155.0, AI forecast: 110.66494584083557, BCD5 forecast: -59.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 24.40135608911514, 57.216337613388895 ; AI error: 466.65474099958794 BCD5 error: 298.4\n",
            "\tIntensity Truth: 150.0, AI forecast: 109.08905029296875, BCD5 forecast: -55.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 27.313515281677244, 59.55202401056886 ; AI error: 609.4352241256312 BCD5 error: 413.0\n",
            "\tIntensity Truth: 140.0, AI forecast: 104.38269257545471, BCD5 forecast: -39.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 29.978659224510192, 61.30313313528895 ; AI error: 778.6413109618087 BCD5 error: 625.9\n",
            "Found AL112017 at 2017-09-03 06:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 103.49657416343689, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 17.0, 51.5; AI forecast: 18.489836567640303, 52.23550849407911 ; AI error: 98.84452976314455 BCD5 error: 60.3\n",
            "\tIntensity Truth: 135.0, AI forecast: 110.42227506637573, BCD5 forecast: -40.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 20.566759614646433, 55.778432902786875 ; AI error: 240.77732519683028 BCD5 error: 144.5\n",
            "\tIntensity Truth: 155.0, AI forecast: 112.56292819976807, BCD5 forecast: -61.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 23.580378687381742, 59.414289934188126 ; AI error: 379.64857054292185 BCD5 error: 247.0\n",
            "\tIntensity Truth: 145.0, AI forecast: 110.08026957511902, BCD5 forecast: -53.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 26.4503959774971, 62.24425904154777 ; AI error: 501.6142731215315 BCD5 error: 377.0\n",
            "\tIntensity Truth: 135.0, AI forecast: 105.09413242340088, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 29.078748273849484, 63.716064222157 ; AI error: 686.5604929927758 BCD5 error: 598.0\n",
            "Found AL112017 at 2017-09-03 12:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 99.08949732780457, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 16.8, 52.6; AI forecast: 18.073995733261107, 52.998175140470266 ; AI error: 79.81917132403841 BCD5 error: 58.7\n",
            "\tIntensity Truth: 150.0, AI forecast: 112.3135232925415, BCD5 forecast: -50.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 20.042717288434506, 56.889145681727676 ; AI error: 207.29810654884952 BCD5 error: 114.1\n",
            "\tIntensity Truth: 155.0, AI forecast: 112.97505378723145, BCD5 forecast: -58.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 22.982456040382385, 60.74225463494658 ; AI error: 326.48962751909073 BCD5 error: 201.8\n",
            "\tIntensity Truth: 145.0, AI forecast: 110.31604766845703, BCD5 forecast: -51.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 25.828555643558502, 63.01700481474399 ; AI error: 472.620350322054 BCD5 error: 322.4\n",
            "\tIntensity Truth: 135.0, AI forecast: 108.37226152420044, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 28.34441888332367, 64.83469849526882 ; AI error: 664.5191303979415 BCD5 error: 559.5\n",
            "Found AL112017 at 2017-09-03 18:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 108.42190384864807, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 16.7, 53.9; AI forecast: 17.987843769788743, 53.88567564077675 ; AI error: 77.32711441962125 BCD5 error: 47.9\n",
            "\tIntensity Truth: 155.0, AI forecast: 119.42445874214172, BCD5 forecast: -54.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 20.05052159130573, 57.752812650613485 ; AI error: 206.32597327340955 BCD5 error: 84.2\n",
            "\tIntensity Truth: 150.0, AI forecast: 118.5241961479187, BCD5 forecast: -52.0\n",
            "\tTrajectory Truth: 18.6, 64.7; AI forecast: 23.12719007730484, 60.82857223451137 ; AI error: 347.8823179577779 BCD5 error: 153.7\n",
            "\tIntensity Truth: 145.0, AI forecast: 117.43780612945557, BCD5 forecast: -50.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 25.99410904049873, 63.40288338959217 ; AI error: 499.61501369269433 BCD5 error: 282.3\n",
            "\tIntensity Truth: 140.0, AI forecast: 111.28854155540466, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 28.46478987932205, 65.4812913313508 ; AI error: 690.1356069423364 BCD5 error: 531.2\n",
            "Found AL112017 at 2017-09-04 00:00:00\n",
            "\tIntensity Truth: 125.0, AI forecast: 110.14908790588379, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 16.6, 55.1; AI forecast: 17.87021999955177, 54.9296952297911 ; AI error: 76.88733515080602 BCD5 error: 30.5\n",
            "\tIntensity Truth: 155.0, AI forecast: 120.72357892990112, BCD5 forecast: -55.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 19.891470888257025, 58.12712142653763 ; AI error: 209.77820943733101 BCD5 error: 66.4\n",
            "\tIntensity Truth: 150.0, AI forecast: 122.49721765518188, BCD5 forecast: -51.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 22.874770873785017, 61.4238321095705 ; AI error: 346.80728854636425 BCD5 error: 145.2\n",
            "\tIntensity Truth: 140.0, AI forecast: 118.00931453704834, BCD5 forecast: -44.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 25.644294375181197, 64.09111372381449 ; AI error: 504.7419273597529 BCD5 error: 302.5\n",
            "\tIntensity Truth: 145.0, AI forecast: 112.3895263671875, BCD5 forecast: -47.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 28.24038554430008, 65.83087487518787 ; AI error: 718.85447509651 BCD5 error: 541.0\n",
            "Found AL112017 at 2017-09-04 06:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 114.25248622894287, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 17.565957611799238, 55.38375342730433 ; AI error: 82.25065459850988 BCD5 error: 29.5\n",
            "\tIntensity Truth: 155.0, AI forecast: 126.9035017490387, BCD5 forecast: -50.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 19.414308546483515, 58.96802223399281 ; AI error: 196.0643284648588 BCD5 error: 82.0\n",
            "\tIntensity Truth: 145.0, AI forecast: 123.90890717506409, BCD5 forecast: -44.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 22.289639949798584, 62.27741226777434 ; AI error: 336.3938925972985 BCD5 error: 172.0\n",
            "\tIntensity Truth: 135.0, AI forecast: 119.58423376083374, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 25.197092884778975, 64.46511949002743 ; AI error: 530.0179871495448 BCD5 error: 334.1\n",
            "\tIntensity Truth: 130.0, AI forecast: 114.99860644340515, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 27.81414873600006, 66.03416333794594 ; AI error: 741.2969534820943 BCD5 error: 559.6\n",
            "Found AL112017 at 2017-09-04 12:00:00\n",
            "\tIntensity Truth: 150.0, AI forecast: 122.64923214912415, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 17.215745282173156, 56.60355137153528 ; AI error: 75.36669781075219 BCD5 error: 29.5\n",
            "\tIntensity Truth: 155.0, AI forecast: 129.92717742919922, BCD5 forecast: -46.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 18.908099906146525, 60.34320557713508 ; AI error: 175.19796253586844 BCD5 error: 75.1\n",
            "\tIntensity Truth: 145.0, AI forecast: 126.57636642456055, BCD5 forecast: -41.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 21.894381529092787, 63.21427420526743 ; AI error: 339.7556616592239 BCD5 error: 157.4\n",
            "\tIntensity Truth: 135.0, AI forecast: 123.30475449562073, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 24.82210144996643, 65.23001477867365 ; AI error: 552.6193970415018 BCD5 error: 337.3\n",
            "\tIntensity Truth: 110.0, AI forecast: 117.06424951553345, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 27.306493258476255, 67.30723315030336 ; AI error: 708.2972779948392 BCD5 error: 543.8\n",
            "Found AL112017 at 2017-09-04 18:00:00\n",
            "\tIntensity Truth: 155.0, AI forecast: 122.46685147285461, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 16.79183963537216, 57.897486274689435 ; AI error: 75.12868378771688 BCD5 error: 16.6\n",
            "\tIntensity Truth: 150.0, AI forecast: 130.996994972229, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 18.6, 64.7; AI forecast: 18.444315832853317, 61.48424497544765 ; AI error: 183.31030540393778 BCD5 error: 63.7\n",
            "\tIntensity Truth: 145.0, AI forecast: 129.8798966407776, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 21.369902861118316, 64.29973447918891 ; AI error: 344.1902356342634 BCD5 error: 147.7\n",
            "\tIntensity Truth: 140.0, AI forecast: 125.72354316711426, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 24.15952812433243, 66.9426099807024 ; AI error: 516.6904250617531 BCD5 error: 326.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 118.87494683265686, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 26.67161223888397, 69.49690786898137 ; AI error: 620.8539947581737 BCD5 error: 510.9\n",
            "Found AL112017 at 2017-09-05 00:00:00\n",
            "\tIntensity Truth: 155.0, AI forecast: 127.30123281478882, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 16.69101014137268, 58.717597372829914 ; AI error: 114.10064321027107 BCD5 error: 23.7\n",
            "\tIntensity Truth: 150.0, AI forecast: 135.91197729110718, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 18.282524287700653, 62.178154807537794 ; AI error: 235.2038552446971 BCD5 error: 92.4\n",
            "\tIntensity Truth: 140.0, AI forecast: 132.1504843235016, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 21.092621283233164, 65.4297302916646 ; AI error: 356.8158153912471 BCD5 error: 163.2\n",
            "\tIntensity Truth: 145.0, AI forecast: 126.70315504074097, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 23.895212411880493, 68.34974680542946 ; AI error: 500.7726119601889 BCD5 error: 343.2\n",
            "\tIntensity Truth: 100.0, AI forecast: 119.47214007377625, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 26.35340118408203, 70.60229068696499 ; AI error: 588.0488943478011 BCD5 error: 501.2\n",
            "Found AL112017 at 2017-09-05 06:00:00\n",
            "\tIntensity Truth: 155.0, AI forecast: 134.45999145507812, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 16.543330985307694, 59.68699909821152 ; AI error: 144.72801851599112 BCD5 error: 13.3\n",
            "\tIntensity Truth: 145.0, AI forecast: 139.95481848716736, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 17.99176596403122, 63.763017903268334 ; AI error: 240.9296621742138 BCD5 error: 74.2\n",
            "\tIntensity Truth: 135.0, AI forecast: 134.3970227241516, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 20.779803292453288, 67.30778850913048 ; AI error: 332.7626838603232 BCD5 error: 149.3\n",
            "\tIntensity Truth: 130.0, AI forecast: 128.2345986366272, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 23.495002150535583, 69.96871868371963 ; AI error: 465.22468951735425 BCD5 error: 336.0\n",
            "\tIntensity Truth: 115.0, AI forecast: 119.94481444358826, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 25.523826497793195, 72.8671982973814 ; AI error: 473.0555963526137 BCD5 error: 478.1\n",
            "Found AL112017 at 2017-09-05 12:00:00\n",
            "\tIntensity Truth: 155.0, AI forecast: 137.5059473514557, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 16.4051784992218, 61.14020569324493 ; AI error: 160.2796005666592 BCD5 error: 13.3\n",
            "\tIntensity Truth: 145.0, AI forecast: 141.6083860397339, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 17.827566277980804, 65.75396912395954 ; AI error: 232.8817572920706 BCD5 error: 59.2\n",
            "\tIntensity Truth: 135.0, AI forecast: 135.83211064338684, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 20.517386952787636, 69.16339581608773 ; AI error: 319.4072222734575 BCD5 error: 166.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 129.45734024047852, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 22.784208554029465, 72.48246096372604 ; AI error: 377.5052600363338 BCD5 error: 356.7\n",
            "\tIntensity Truth: 115.0, AI forecast: 120.70642232894897, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 24.509094578027725, 75.67488394975662 ; AI error: 318.21772258283505 BCD5 error: 466.9\n",
            "Found AL112017 at 2017-09-05 18:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 145.14663457870483, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 17.747447246313094, 67.82002266347408 ; AI error: 229.80949517558548 BCD5 error: 80.5\n",
            "\tIntensity Truth: 140.0, AI forecast: 139.14408564567566, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 19.956001591682433, 72.1389507561922 ; AI error: 248.8044547680315 BCD5 error: 237.3\n",
            "\tIntensity Truth: 95.0, AI forecast: 131.80152773857117, BCD5 forecast: 35.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 21.8915939360857, 76.07273114323615 ; AI error: 240.15529014525976 BCD5 error: 430.3\n",
            "\tIntensity Truth: 100.0, AI forecast: 119.88102793693542, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 23.629514449834822, 79.39186751246453 ; AI error: 172.821099564938 BCD5 error: 503.7\n",
            "Found AL112017 at 2017-09-06 00:00:00\n",
            "\tIntensity Truth: 150.0, AI forecast: 143.13133478164673, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 16.62666813135147, 64.73932345956564 ; AI error: 175.59485571012212 BCD5 error: 25.6\n",
            "\tIntensity Truth: 140.0, AI forecast: 147.7883505821228, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 17.441466528177262, 70.6492782831192 ; AI error: 229.13326944193258 BCD5 error: 124.0\n",
            "\tIntensity Truth: 145.0, AI forecast: 140.7063627243042, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 19.20580684095621, 75.61118471622467 ; AI error: 195.34822342603337 BCD5 error: 320.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 129.73206281661987, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 21.07687477171421, 79.62835371494293 ; AI error: 156.3599962171123 BCD5 error: 504.9\n",
            "\tIntensity Truth: 80.0, AI forecast: 117.0378053188324, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 23.22722011208534, 82.54916977584362 ; AI error: 219.42806489386732 BCD5 error: 519.5\n",
            "Found AL112017 at 2017-09-06 06:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 144.85639095306396, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 16.910123926401138, 69.50463456362486 ; AI error: 199.60465482423388 BCD5 error: 28.9\n",
            "\tIntensity Truth: 135.0, AI forecast: 145.25396347045898, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 17.86251454949379, 76.60537028312683 ; AI error: 291.1027080634792 BCD5 error: 136.3\n",
            "\tIntensity Truth: 130.0, AI forecast: 133.62924814224243, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 20.432956370711327, 81.07896962463856 ; AI error: 195.1188919237348 BCD5 error: 327.9\n",
            "\tIntensity Truth: 115.0, AI forecast: 122.34216570854187, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 23.547083032131194, 83.24080263674259 ; AI error: 107.15475221805643 BCD5 error: 488.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 110.5683982372284, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 26.698459649085997, 82.75035041868686 ; AI error: 94.80170817650846 BCD5 error: 469.8\n",
            "Found AL112017 at 2017-09-06 12:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 135.82364916801453, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 18.323192316293717, 72.01000406742097 ; AI error: 204.4499757814623 BCD5 error: 23.3\n",
            "\tIntensity Truth: 135.0, AI forecast: 136.95760369300842, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 20.58984292000532, 77.78897645175456 ; AI error: 187.55626560715544 BCD5 error: 152.7\n",
            "\tIntensity Truth: 110.0, AI forecast: 128.1545066833496, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 23.771029621362686, 79.50920508205891 ; AI error: 65.33269642020224 BCD5 error: 335.2\n",
            "\tIntensity Truth: 115.0, AI forecast: 119.4537901878357, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 27.562596583366393, 81.49948393702508 ; AI error: 183.87976797500505 BCD5 error: 457.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 109.89373326301575, BCD5 forecast: 40.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 31.024131083488463, 81.28324501514435 ; AI error: 112.70787286241182 BCD5 error: 440.6\n",
            "Found AL112017 at 2017-09-06 18:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 134.06386256217957, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 17.941786432266234, 72.24376519620418 ; AI error: 195.79320996470327 BCD5 error: 32.8\n",
            "\tIntensity Truth: 140.0, AI forecast: 137.49168634414673, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 19.957175715267656, 79.03495577573776 ; AI error: 209.72941414199735 BCD5 error: 187.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 130.1575231552124, BCD5 forecast: 40.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 22.877044582366942, 81.59134622216224 ; AI error: 78.05885553485217 BCD5 error: 372.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 119.51827764511108, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 26.831610727310178, 83.41163609921932 ; AI error: 118.1845109102421 BCD5 error: 451.9\n",
            "\tIntensity Truth: 45.0, AI forecast: 105.45430421829224, BCD5 forecast: 42.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 31.180911457538606, 81.81231798827648 ; AI error: 88.44132182132577 BCD5 error: 463.4\n",
            "Found AL112017 at 2017-09-07 00:00:00\n",
            "\tIntensity Truth: 140.0, AI forecast: 133.34007263183594, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 19.138148333132268, 74.72731291353702 ; AI error: 202.74591380357546 BCD5 error: 39.7\n",
            "\tIntensity Truth: 145.0, AI forecast: 136.55742764472961, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 20.9059890024364, 80.1536044627428 ; AI error: 179.88812655390217 BCD5 error: 205.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 126.05011940002441, BCD5 forecast: 34.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 24.757256633043287, 83.05299639105797 ; AI error: 143.41490462714944 BCD5 error: 372.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 112.35169172286987, BCD5 forecast: 39.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 29.22301217317581, 82.17252965867519 ; AI error: 147.61870005505855 BCD5 error: 397.1\n",
            "\tIntensity Truth: 35.0, AI forecast: 102.92694330215454, BCD5 forecast: 37.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 33.986745953559875, 80.67789032757283 ; AI error: 225.5165040805212 BCD5 error: 461.1\n",
            "Found AL112017 at 2017-09-07 06:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 132.40028977394104, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 18.659866504371166, 75.27595825791359 ; AI error: 206.8289712145944 BCD5 error: 49.1\n",
            "\tIntensity Truth: 130.0, AI forecast: 133.65283012390137, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 21.271668887138365, 81.72902603447437 ; AI error: 202.753313443491 BCD5 error: 205.4\n",
            "\tIntensity Truth: 115.0, AI forecast: 119.86942052841187, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 25.317143946886063, 82.3116932541132 ; AI error: 111.7213010483735 BCD5 error: 347.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 111.4793586730957, BCD5 forecast: 50.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 30.05810421705246, 82.02278287410736 ; AI error: 111.94790672977653 BCD5 error: 319.1\n",
            "Found AL112017 at 2017-09-07 12:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 128.45526456832886, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 20.75033604800701, 76.31232905089855 ; AI error: 110.04010421854069 BCD5 error: 61.8\n",
            "\tIntensity Truth: 110.0, AI forecast: 123.65392804145813, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 24.28411383628845, 79.41568943858147 ; AI error: 95.32402904402643 BCD5 error: 208.6\n",
            "\tIntensity Truth: 115.0, AI forecast: 116.45745158195496, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 29.294131374359132, 79.79866827726364 ; AI error: 301.8993109839467 BCD5 error: 306.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 105.50487279891968, BCD5 forecast: 62.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 34.71111533641815, 77.89400964379311 ; AI error: 392.11409092092225 BCD5 error: 269.4\n",
            "Found AL112017 at 2017-09-07 18:00:00\n",
            "\tIntensity Truth: 140.0, AI forecast: 117.61494636535645, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 20.945401609688997, 75.48628748357297 ; AI error: 69.52019676830501 BCD5 error: 63.4\n",
            "\tIntensity Truth: 95.0, AI forecast: 122.63333559036255, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 24.73250719308853, 79.81660594344139 ; AI error: 100.24961299074457 BCD5 error: 186.7\n",
            "\tIntensity Truth: 100.0, AI forecast: 112.85534977912903, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 29.709283149242403, 79.75882149338722 ; AI error: 267.43463232380816 BCD5 error: 230.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 104.45817828178406, BCD5 forecast: 65.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 34.955163311958316, 78.38146885931492 ; AI error: 354.60167874954624 BCD5 error: 201.2\n",
            "Found AL112017 at 2017-09-08 00:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 128.3043932914734, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 22.095747995376588, 77.81394470632077 ; AI error: 34.15471240821505 BCD5 error: 69.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 125.30677795410156, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 26.06230264902115, 81.5807121604681 ; AI error: 164.0985432250045 BCD5 error: 159.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 114.11407351493835, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 30.87073855400085, 81.70041700005531 ; AI error: 244.40909367457164 BCD5 error: 144.1\n",
            "\tIntensity Truth: 35.0, AI forecast: 101.58885359764099, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 35.85426766872406, 80.43235559165478 ; AI error: 308.93548592400697 BCD5 error: 150.3\n",
            "Found AL112017 at 2017-09-08 06:00:00\n",
            "\tIntensity Truth: 130.0, AI forecast: 126.06751799583435, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 22.592253476381302, 79.78298256993294 ; AI error: 83.06909935020019 BCD5 error: 60.0\n",
            "\tIntensity Truth: 115.0, AI forecast: 123.07087063789368, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 26.347120040655135, 84.22564800083637 ; AI error: 224.914374488701 BCD5 error: 139.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 108.26996445655823, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 31.053926241397857, 83.92055844962597 ; AI error: 193.44602562028481 BCD5 error: 79.2\n",
            "Found AL112017 at 2017-09-08 12:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 124.54611301422119, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 22.64528815150261, 80.81266362071037 ; AI error: 83.86659171892182 BCD5 error: 47.5\n",
            "\tIntensity Truth: 115.0, AI forecast: 119.70484733581543, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 26.437130737304685, 84.75912992954254 ; AI error: 211.4963375617545 BCD5 error: 112.8\n",
            "\tIntensity Truth: 50.0, AI forecast: 105.47780275344849, BCD5 forecast: 68.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 31.59241328239441, 83.88229210674763 ; AI error: 134.3240765315204 BCD5 error: 127.3\n",
            "Found AL112017 at 2017-09-08 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 121.96622252464294, BCD5 forecast: 46.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 22.516340148448943, 81.3294763058424 ; AI error: 71.66362979167448 BCD5 error: 25.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 117.28806138038635, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 26.553708338737486, 85.1171201646328 ; AI error: 192.96356340913096 BCD5 error: 108.4\n",
            "\tIntensity Truth: 45.0, AI forecast: 106.21792316436768, BCD5 forecast: 72.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 30.49832649230957, 85.09033406376838 ; AI error: 85.57091014291423 BCD5 error: 188.8\n",
            "Found AL112017 at 2017-09-09 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 122.49591588973999, BCD5 forecast: 44.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 22.59792854785919, 81.89621362984181 ; AI error: 73.14674804971864 BCD5 error: 30.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 120.01268982887268, BCD5 forecast: 46.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 25.314749324321745, 86.09820774197578 ; AI error: 253.41475583898313 BCD5 error: 137.6\n",
            "\tIntensity Truth: 35.0, AI forecast: 106.40628695487976, BCD5 forecast: 79.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 29.227225387096404, 87.56186907291412 ; AI error: 229.0440206468183 BCD5 error: 194.0\n",
            "Found AL112017 at 2017-09-09 06:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 122.74768233299255, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 22.73011193871498, 84.13072827458382 ; AI error: 166.6939602527263 BCD5 error: 48.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 121.7619788646698, BCD5 forecast: 56.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 25.33238812088966, 89.50015107989312 ; AI error: 427.4257438074236 BCD5 error: 135.4\n",
            "Found AL112017 at 2017-09-09 12:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 117.90030837059021, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 22.55476638674736, 85.10957067608834 ; AI error: 230.4688681028462 BCD5 error: 54.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 117.59163975715637, BCD5 forecast: 52.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 24.84842540025711, 90.30019676685333 ; AI error: 495.8000759685281 BCD5 error: 169.6\n",
            "Found AL112017 at 2017-09-09 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 106.6427993774414, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 22.722208219766618, 85.62493010163307 ; AI error: 275.7995285460547 BCD5 error: 69.7\n",
            "\tIntensity Truth: 45.0, AI forecast: 105.95265507698059, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 25.97480298280716, 89.26235575675965 ; AI error: 424.1170119694182 BCD5 error: 211.6\n",
            "Found AL112017 at 2017-09-10 00:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 95.62958836555481, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 23.766374057531355, 85.52253380417824 ; AI error: 276.08097721055344 BCD5 error: 107.6\n",
            "\tIntensity Truth: 35.0, AI forecast: 79.54438000917435, BCD5 forecast: 58.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 28.340620231628417, 85.18441778421402 ; AI error: 217.55249858630967 BCD5 error: 263.2\n",
            "Found AL112017 at 2017-09-10 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.34833329916, BCD5 forecast: 33.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 23.659192073345185, 83.82396501600742 ; AI error: 286.3749169263347 BCD5 error: 109.2\n",
            "Found AL112017 at 2017-09-10 12:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 88.33077311515808, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 25.152326548099516, 83.78889347016812 ; AI error: 273.2727606862981 BCD5 error: 98.7\n",
            "Found AL112017 at 2017-09-10 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 109.7965931892395, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 28.091557705402373, 82.68561677634716 ; AI error: 173.90600693788855 BCD5 error: 142.7\n",
            "Found AL112017 at 2017-09-11 00:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 90.44169425964355, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 33.87588210105896, 78.6967380106449 ; AI error: 310.99613853399444 BCD5 error: 174.4\n",
            "Found AL112017 at 2017-09-11 06:00:00\n",
            "Found AL112017 at 2017-09-11 12:00:00\n",
            "Found AL112017 at 2017-09-11 18:00:00\n",
            "Found AL112017 at 2017-09-12 00:00:00\n",
            "Found AL122017 at 2017-09-05 12:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 45.28471725061536, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 13.1, 43.9; AI forecast: 15.520761436223983, 44.0224339812994 ; AI error: 145.5180775690739 BCD5 error: 44.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 46.198517736047506, BCD5 forecast: -31.0\n",
            "\tTrajectory Truth: 14.7, 49.9; AI forecast: 17.113381457328796, 47.870349110662936 ; AI error: 186.35510909221583 BCD5 error: 135.5\n",
            "\tIntensity Truth: 130.0, AI forecast: 49.2506905272603, BCD5 forecast: -68.0\n",
            "\tTrajectory Truth: 16.1, 56.4; AI forecast: 19.516652469336986, 52.74165277853608 ; AI error: 292.9096060556292 BCD5 error: 286.0\n",
            "\tIntensity Truth: 125.0, AI forecast: 54.94395464658737, BCD5 forecast: -67.0\n",
            "\tTrajectory Truth: 17.9, 60.8; AI forecast: 21.225438909232615, 55.87791142021305 ; AI error: 342.59624334751527 BCD5 error: 314.4\n",
            "\tIntensity Truth: 115.0, AI forecast: 58.32535922527313, BCD5 forecast: -52.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 21.994785571098326, 57.46137814354151 ; AI error: 440.1359058772176 BCD5 error: 363.9\n",
            "Found AL122017 at 2017-09-05 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 42.10616942495108, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 13.7, 45.2; AI forecast: 15.37706305384636, 44.22742752283811 ; AI error: 115.47070581914693 BCD5 error: 37.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 43.86630455031991, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 15.1, 51.5; AI forecast: 16.94588404893875, 48.26345582902431 ; AI error: 217.17046268998598 BCD5 error: 101.9\n",
            "\tIntensity Truth: 135.0, AI forecast: 50.376399755477905, BCD5 forecast: -69.0\n",
            "\tTrajectory Truth: 16.4, 57.8; AI forecast: 19.36871777921915, 52.91246059276163 ; AI error: 331.2629179984059 BCD5 error: 202.4\n",
            "\tIntensity Truth: 120.0, AI forecast: 55.765977799892426, BCD5 forecast: -56.0\n",
            "\tTrajectory Truth: 18.6, 61.8; AI forecast: 20.906019465625285, 55.99885755036957 ; AI error: 355.8054390802536 BCD5 error: 189.4\n",
            "\tIntensity Truth: 105.0, AI forecast: 60.81676930189133, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 21.62553360760212, 57.63669104129076 ; AI error: 494.84234491199817 BCD5 error: 266.9\n",
            "Found AL122017 at 2017-09-06 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 48.93079210072756, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 14.1, 46.7; AI forecast: 15.480335986614227, 45.080073745548724 ; AI error: 125.34409645515497 BCD5 error: 56.8\n",
            "\tIntensity Truth: 105.0, AI forecast: 56.24176576733589, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 15.5, 53.2; AI forecast: 17.16720470190048, 48.79213877916336 ; AI error: 272.96877916173827 BCD5 error: 114.8\n",
            "\tIntensity Truth: 135.0, AI forecast: 62.055393904447556, BCD5 forecast: -63.0\n",
            "\tTrajectory Truth: 16.7, 58.9; AI forecast: 19.512557916343212, 53.212354571372266 ; AI error: 365.8297236113392 BCD5 error: 179.2\n",
            "\tIntensity Truth: 115.0, AI forecast: 67.70039647817612, BCD5 forecast: -45.0\n",
            "\tTrajectory Truth: 19.4, 62.9; AI forecast: 21.052447424829005, 56.26176018636906 ; AI error: 386.88017382617335 BCD5 error: 176.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 71.95009529590607, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 22.004894092679024, 57.95686462577432 ; AI error: 539.8334705189832 BCD5 error: 272.9\n",
            "Found AL122017 at 2017-09-06 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 61.16185083985329, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 14.4, 48.3; AI forecast: 15.56472042798996, 45.956242594122884 ; AI error: 152.86618204014178 BCD5 error: 40.7\n",
            "\tIntensity Truth: 115.0, AI forecast: 69.31772321462631, BCD5 forecast: -43.0\n",
            "\tTrajectory Truth: 15.9, 54.9; AI forecast: 17.064904713630675, 49.62178975492716 ; AI error: 311.8136162301792 BCD5 error: 152.8\n",
            "\tIntensity Truth: 130.0, AI forecast: 75.60695677995682, BCD5 forecast: -54.0\n",
            "\tTrajectory Truth: 17.2, 59.9; AI forecast: 19.27757454663515, 53.96005768887699 ; AI error: 360.9210256636454 BCD5 error: 228.7\n",
            "\tIntensity Truth: 115.0, AI forecast: 80.42973279953003, BCD5 forecast: -41.0\n",
            "\tTrajectory Truth: 20.3, 64.0; AI forecast: 20.98128663599491, 56.93799648554995 ; AI error: 398.85745601690314 BCD5 error: 255.3\n",
            "\tIntensity Truth: 90.0, AI forecast: 82.3267388343811, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 22.210166418552397, 59.36217183992267 ; AI error: 524.676399224823 BCD5 error: 343.5\n",
            "Found AL122017 at 2017-09-06 12:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 65.1867750287056, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 14.7, 49.9; AI forecast: 15.283454978466033, 47.1740029066801 ; AI error: 161.93206805357937 BCD5 error: 71.6\n",
            "\tIntensity Truth: 130.0, AI forecast: 75.18637597560883, BCD5 forecast: -55.0\n",
            "\tTrajectory Truth: 16.1, 56.4; AI forecast: 16.38336741924286, 51.187675688415766 ; AI error: 300.9335660140439 BCD5 error: 210.1\n",
            "\tIntensity Truth: 125.0, AI forecast: 81.80733442306519, BCD5 forecast: -46.0\n",
            "\tTrajectory Truth: 17.9, 60.8; AI forecast: 18.4887303173542, 55.566325045097614 ; AI error: 300.5952751869932 BCD5 error: 272.4\n",
            "\tIntensity Truth: 115.0, AI forecast: 85.29160141944885, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 20.295405168831348, 59.23735261186957 ; AI error: 344.679342395548 BCD5 error: 340.5\n",
            "\tIntensity Truth: 85.0, AI forecast: 86.31866931915283, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 21.513760459423064, 62.41462490037083 ; AI error: 449.8387410116425 BCD5 error: 406.4\n",
            "Found AL122017 at 2017-09-06 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 72.92979896068573, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 15.1, 51.5; AI forecast: 15.109400802850722, 48.60470559895038 ; AI error: 167.82882270289045 BCD5 error: 68.1\n",
            "\tIntensity Truth: 135.0, AI forecast: 83.2095193862915, BCD5 forecast: -49.0\n",
            "\tTrajectory Truth: 16.4, 57.8; AI forecast: 16.148693281412125, 52.79576465338468 ; AI error: 288.80457173857076 BCD5 error: 187.0\n",
            "\tIntensity Truth: 120.0, AI forecast: 87.41771817207336, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 18.6, 61.8; AI forecast: 18.3844465136528, 57.776969805732364 ; AI error: 229.4333058352242 BCD5 error: 210.8\n",
            "\tIntensity Truth: 105.0, AI forecast: 89.12161231040955, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 20.225042900443075, 62.103013910353184 ; AI error: 273.16314078573873 BCD5 error: 293.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 90.95871210098267, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 21.396610018610954, 65.25805658400058 ; AI error: 384.71344998490895 BCD5 error: 318.9\n",
            "Found AL122017 at 2017-09-07 00:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 78.76024574041367, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 15.5, 53.2; AI forecast: 15.477102476358414, 49.93190339580178 ; AI error: 189.09536430380513 BCD5 error: 42.2\n",
            "\tIntensity Truth: 135.0, AI forecast: 88.47652912139893, BCD5 forecast: -46.0\n",
            "\tTrajectory Truth: 16.7, 58.9; AI forecast: 16.8740025639534, 54.45267241485416 ; AI error: 255.84855285517312 BCD5 error: 104.8\n",
            "\tIntensity Truth: 115.0, AI forecast: 91.61381721496582, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 19.4, 62.9; AI forecast: 19.278270459175108, 59.73452424779534 ; AI error: 179.47905309317127 BCD5 error: 94.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 94.53709006309509, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 21.075814145803452, 63.827617581188676 ; AI error: 248.6096071922229 BCD5 error: 184.6\n",
            "\tIntensity Truth: 75.0, AI forecast: 92.53149509429932, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 22.18799372315407, 66.19714292436838 ; AI error: 347.91824633261024 BCD5 error: 187.6\n",
            "Found AL122017 at 2017-09-07 06:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 86.44589185714722, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 15.9, 54.9; AI forecast: 16.45675094127655, 51.25940769985318 ; AI error: 212.56780856020205 BCD5 error: 34.6\n",
            "\tIntensity Truth: 130.0, AI forecast: 94.89257216453552, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 17.2, 59.9; AI forecast: 18.39370582103729, 55.85203252024948 ; AI error: 242.24639064874629 BCD5 error: 49.7\n",
            "\tIntensity Truth: 115.0, AI forecast: 97.81036257743835, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 20.3, 64.0; AI forecast: 20.88143722116947, 60.596489052474496 ; AI error: 194.4486682574036 BCD5 error: 33.8\n",
            "\tIntensity Truth: 90.0, AI forecast: 95.98851680755615, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 22.70052218437195, 63.70240663439035 ; AI error: 286.22878802422326 BCD5 error: 148.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 95.60738325119019, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 24.070899301767348, 65.60389760285616 ; AI error: 283.84693023797695 BCD5 error: 108.7\n",
            "Found AL122017 at 2017-09-07 12:00:00\n",
            "\tIntensity Truth: 130.0, AI forecast: 95.12651443481445, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 16.1, 56.4; AI forecast: 17.19112914800644, 52.721088816225524 ; AI error: 221.52882833174652 BCD5 error: 34.0\n",
            "\tIntensity Truth: 125.0, AI forecast: 105.17056107521057, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 17.9, 60.8; AI forecast: 19.15588006824255, 57.29914362821728 ; AI error: 213.07801550894624 BCD5 error: 36.0\n",
            "\tIntensity Truth: 115.0, AI forecast: 103.20074796676636, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 21.598570728302, 61.387445507943625 ; AI error: 220.01590389290163 BCD5 error: 34.1\n",
            "\tIntensity Truth: 85.0, AI forecast: 102.95435547828674, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 23.553292828798295, 64.28238947838544 ; AI error: 300.79560017041064 BCD5 error: 141.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 100.33010482788086, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 24.894049024581907, 66.63854296803474 ; AI error: 190.72855624680136 BCD5 error: 63.8\n",
            "Found AL122017 at 2017-09-07 18:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 107.7427327632904, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 16.4, 57.8; AI forecast: 17.161594969034194, 54.48602044060826 ; AI error: 195.90714847528687 BCD5 error: 32.1\n",
            "\tIntensity Truth: 120.0, AI forecast: 113.57511758804321, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 18.6, 61.8; AI forecast: 18.882345603406428, 59.10245722457766 ; AI error: 154.30682834990807 BCD5 error: 53.6\n",
            "\tIntensity Truth: 105.0, AI forecast: 113.14248085021973, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 21.260745510458946, 63.31598202735185 ; AI error: 186.3179787794608 BCD5 error: 16.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 110.47653198242188, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 23.088035351037977, 66.86893802881241 ; AI error: 250.04286508569936 BCD5 error: 99.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 105.60195446014404, BCD5 forecast: 30.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 24.411388194561003, 70.07838551849127 ; AI error: 230.4633152391218 BCD5 error: 156.2\n",
            "Found AL122017 at 2017-09-08 00:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 111.38947010040283, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 16.7, 58.9; AI forecast: 16.925005733966827, 56.55101363160647 ; AI error: 135.67928989590607 BCD5 error: 49.8\n",
            "\tIntensity Truth: 115.0, AI forecast: 122.21097111701965, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 19.4, 62.9; AI forecast: 18.423614007234573, 61.54263904839754 ; AI error: 96.85240786349816 BCD5 error: 85.7\n",
            "\tIntensity Truth: 100.0, AI forecast: 120.62848329544067, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 20.554659016430378, 66.35367610901594 ; AI error: 178.8482989887553 BCD5 error: 58.3\n",
            "\tIntensity Truth: 75.0, AI forecast: 116.25017881393433, BCD5 forecast: 37.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 22.29233385324478, 70.65684303343296 ; AI error: 302.51544431452487 BCD5 error: 112.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 108.89714002609253, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 23.735609710216522, 74.01169703006744 ; AI error: 437.4416140406739 BCD5 error: 291.2\n",
            "Found AL122017 at 2017-09-08 06:00:00\n",
            "\tIntensity Truth: 130.0, AI forecast: 122.94041872024536, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 17.2, 59.9; AI forecast: 16.932189601659775, 58.544799805432554 ; AI error: 79.42836988813781 BCD5 error: 60.0\n",
            "\tIntensity Truth: 115.0, AI forecast: 130.37259817123413, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 20.3, 64.0; AI forecast: 18.291671693325043, 63.969323109090325 ; AI error: 120.59352210918868 BCD5 error: 109.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 125.47492265701294, BCD5 forecast: 30.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 20.375411453843117, 69.1659309655428 ; AI error: 237.71971625792554 BCD5 error: 121.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 118.14922094345093, BCD5 forecast: 48.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 22.247157460451124, 73.32328101098537 ; AI error: 395.09564941245435 BCD5 error: 192.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 111.09369397163391, BCD5 forecast: 33.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 23.765060526132583, 76.7837023615837 ; AI error: 595.3207426882343 BCD5 error: 391.6\n",
            "Found AL122017 at 2017-09-08 12:00:00\n",
            "\tIntensity Truth: 125.0, AI forecast: 128.94405245780945, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 17.9, 60.8; AI forecast: 17.02906310558319, 60.2689490146935 ; AI error: 60.49324117941123 BCD5 error: 70.9\n",
            "\tIntensity Truth: 115.0, AI forecast: 133.595769405365, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 18.48105753660202, 66.06884825229645 ; AI error: 168.921341331641 BCD5 error: 115.9\n",
            "\tIntensity Truth: 85.0, AI forecast: 126.19055151939392, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 20.81894633099437, 70.84878521859646 ; AI error: 286.4390914437318 BCD5 error: 175.1\n",
            "\tIntensity Truth: 70.0, AI forecast: 120.29211044311523, BCD5 forecast: 54.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 22.829749566316604, 74.75757191479207 ; AI error: 456.39670256870136 BCD5 error: 252.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 113.23261141777039, BCD5 forecast: 35.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 24.006142354011534, 78.20997914075852 ; AI error: 684.0739789528457 BCD5 error: 467.3\n",
            "Found AL122017 at 2017-09-08 18:00:00\n",
            "\tIntensity Truth: 120.0, AI forecast: 132.4038863182068, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 18.6, 61.8; AI forecast: 17.255329591035842, 62.14491933733225 ; AI error: 83.10411109661439 BCD5 error: 59.4\n",
            "\tIntensity Truth: 105.0, AI forecast: 135.504332780838, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 18.926677663624286, 67.82791469395161 ; AI error: 210.2260776170631 BCD5 error: 109.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 130.10589480400085, BCD5 forecast: 43.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 21.307421311736107, 72.50563680827618 ; AI error: 352.6834030067778 BCD5 error: 181.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 124.22239065170288, BCD5 forecast: 52.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 22.91245097517967, 76.64867809712887 ; AI error: 570.0125391208899 BCD5 error: 233.6\n",
            "\tIntensity Truth: 70.0, AI forecast: 113.14274787902832, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 24.55475387573242, 79.71682619154453 ; AI error: 769.6259329349136 BCD5 error: 470.8\n",
            "Found AL122017 at 2017-09-09 00:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 132.44882225990295, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 19.4, 62.9; AI forecast: 17.57138447165489, 63.82740985155105 ; AI error: 121.8299276852317 BCD5 error: 43.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 138.67891311645508, BCD5 forecast: 29.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 19.176637844741343, 69.75605440735816 ; AI error: 275.3894382578448 BCD5 error: 134.6\n",
            "\tIntensity Truth: 75.0, AI forecast: 133.5657036304474, BCD5 forecast: 48.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 21.045234924554823, 74.82983130514621 ; AI error: 474.27714457979494 BCD5 error: 158.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 123.13192009925842, BCD5 forecast: 46.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 23.096734023094175, 78.61198601424694 ; AI error: 685.1511091639532 BCD5 error: 197.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 111.35947108268738, BCD5 forecast: 23.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 25.518601661920545, 80.45427149236203 ; AI error: 800.8082343321034 BCD5 error: 457.9\n",
            "Found AL122017 at 2017-09-09 06:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 136.15046739578247, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 20.3, 64.0; AI forecast: 17.643963652849198, 65.66863124519587 ; AI error: 185.48476063925938 BCD5 error: 32.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 142.72747039794922, BCD5 forecast: 35.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 18.59002168774605, 72.28031543791295 ; AI error: 399.7240045898874 BCD5 error: 143.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 132.08227515220642, BCD5 forecast: 47.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 20.759100762754677, 77.0875692754984 ; AI error: 599.7423198774436 BCD5 error: 96.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 120.10328769683838, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 23.468520486354826, 79.72048732042313 ; AI error: 755.3627408378328 BCD5 error: 242.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 107.51572489738464, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 26.807424175739285, 80.62725686430932 ; AI error: 803.4514822218338 BCD5 error: 507.7\n",
            "Found AL122017 at 2017-09-09 12:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 136.70905828475952, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 17.363433825969697, 67.36047976762056 ; AI error: 258.2431587324258 BCD5 error: 58.8\n",
            "\tIntensity Truth: 85.0, AI forecast: 138.29829692840576, BCD5 forecast: 34.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 18.602076286077498, 73.94337687194347 ; AI error: 480.04121975225235 BCD5 error: 158.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 127.07969903945923, BCD5 forecast: 41.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 21.352388451993466, 77.58798742890357 ; AI error: 634.1252765652754 BCD5 error: 27.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 114.93543744087219, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 25.01006298661232, 79.07960832118988 ; AI error: 721.4649554616291 BCD5 error: 336.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 103.06517243385315, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 29.255586850643155, 78.51095310151577 ; AI error: 705.2389587072847 BCD5 error: 574.7\n",
            "Found AL122017 at 2017-09-09 18:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 126.44991159439087, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 18.411922526359557, 68.04298439472913 ; AI error: 243.46037027914022 BCD5 error: 63.2\n",
            "\tIntensity Truth: 80.0, AI forecast: 128.7597107887268, BCD5 forecast: 34.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 20.57022467330098, 73.09492594003677 ; AI error: 407.2554864156089 BCD5 error: 120.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 118.63210916519165, BCD5 forecast: 36.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 24.614306640625, 74.91255687773227 ; AI error: 438.4160754211025 BCD5 error: 104.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 108.40314507484436, BCD5 forecast: 27.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 29.608437132835387, 74.17015828490257 ; AI error: 521.4337696783954 BCD5 error: 424.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 98.37258338928223, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 34.96379401683807, 70.91410219967365 ; AI error: 633.4317232853224 BCD5 error: 620.2\n",
            "Found AL122017 at 2017-09-10 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 117.28951334953308, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 19.763556756079197, 68.15031193941832 ; AI error: 214.54168037625533 BCD5 error: 62.8\n",
            "\tIntensity Truth: 75.0, AI forecast: 118.21501851081848, BCD5 forecast: 34.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 23.154575216770173, 71.36026645898819 ; AI error: 265.2024312983829 BCD5 error: 60.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 109.45013999938965, BCD5 forecast: 33.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 28.362131655216217, 70.73773719370365 ; AI error: 234.43498931640647 BCD5 error: 232.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 100.63057065010071, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 34.19800295829773, 67.30370937734843 ; AI error: 552.594501090347 BCD5 error: 516.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 91.26927495002747, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 40.6122524023056, 60.67945669367909 ; AI error: 976.710669777977 BCD5 error: 672.3\n",
            "Found AL122017 at 2017-09-10 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 109.38833951950073, BCD5 forecast: 25.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 21.650964878499508, 67.9461069226265 ; AI error: 163.1037644777667 BCD5 error: 48.7\n",
            "\tIntensity Truth: 70.0, AI forecast: 109.89226460456848, BCD5 forecast: 39.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 26.19225739240646, 69.2708624124527 ; AI error: 85.01521534415076 BCD5 error: 42.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 101.5312922000885, BCD5 forecast: 32.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 31.928495573997495, 66.49426498413087 ; AI error: 338.2893526031067 BCD5 error: 354.7\n",
            "\tIntensity Truth: 70.0, AI forecast: 92.918461561203, BCD5 forecast: 23.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 38.46686735153198, 60.443736971169706 ; AI error: 860.480948384118 BCD5 error: 604.6\n",
            "\tIntensity Truth: 60.0, AI forecast: 85.52128791809082, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 44.8531188249588, 52.3702126853168 ; AI error: 1376.4637063689252 BCD5 error: 728.2\n",
            "Found AL122017 at 2017-09-10 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 107.53986597061157, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 23.376317566633222, 68.07391660958528 ; AI error: 141.49323374061305 BCD5 error: 31.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 107.30493068695068, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 28.30122104883194, 68.40813494324684 ; AI error: 36.54993152689038 BCD5 error: 152.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 98.02977681159973, BCD5 forecast: 32.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 34.529268217086795, 63.93872071802616 ; AI error: 532.9065347629513 BCD5 error: 468.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 90.47481417655945, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 41.00499904155731, 56.86422914760187 ; AI error: 1081.3517224402465 BCD5 error: 683.4\n",
            "\tIntensity Truth: 60.0, AI forecast: 82.35614538192749, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 47.0318190574646, 48.52656364440918 ; AI error: 1577.7861326213745 BCD5 error: 782.7\n",
            "Found AL122017 at 2017-09-10 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 104.61071014404297, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 24.665383410453796, 68.89896616190671 ; AI error: 114.85884427069743 BCD5 error: 28.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 105.53307771682739, BCD5 forecast: 29.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 30.312050616741182, 68.11188026070595 ; AI error: 178.80188934290447 BCD5 error: 245.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 98.63171815872192, BCD5 forecast: 25.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 36.75769639015198, 62.530363375693554 ; AI error: 699.828751520884 BCD5 error: 551.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 90.02044796943665, BCD5 forecast: 23.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 43.212841796875, 54.775582095421846 ; AI error: 1245.5205527259855 BCD5 error: 738.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 77.5741496682167, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 49.73178472518921, 45.368891707062716 ; AI error: 1781.3880117794079 BCD5 error: 849.2\n",
            "Found AL122017 at 2017-09-11 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 101.3722014427185, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 26.39457840323448, 69.22798192799091 ; AI error: 49.22876979147385 BCD5 error: 59.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 104.20563578605652, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 32.43262577056885, 67.44260419607163 ; AI error: 340.39611459572393 BCD5 error: 348.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 95.70939660072327, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 38.76162710189819, 60.885737311095 ; AI error: 855.7807836101604 BCD5 error: 615.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 82.21421122550964, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 45.59832897186279, 51.46645734086633 ; AI error: 1440.418205325663 BCD5 error: 769.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.25007712841034, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 51.42360415458679, 42.17700203508139 ; AI error: 1943.3289503132778 BCD5 error: 903.5\n",
            "Found AL122017 at 2017-09-11 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 99.53656196594238, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 27.857774651050565, 69.99105513095856 ; AI error: 49.82025712383178 BCD5 error: 99.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 102.07823395729065, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 34.108489060401915, 67.58452508449554 ; AI error: 474.28088215886476 BCD5 error: 425.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 88.86178493499756, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 41.06171295642852, 59.12068613618612 ; AI error: 1027.8019131930264 BCD5 error: 661.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 78.07556718587875, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 47.53659062385559, 49.462985110282894 ; AI error: 1577.5341883829262 BCD5 error: 785.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 65.98158001899719, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 52.699725818634036, 40.5848832398653 ; AI error: 2031.9704333544944 BCD5 error: 939.8\n",
            "Found AL122017 at 2017-09-11 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 92.09829926490784, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 28.9164696931839, 70.60125288665294 ; AI error: 141.8816506935168 BCD5 error: 144.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 88.81553053855896, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 35.72759938240051, 66.56478453278541 ; AI error: 597.3555863524588 BCD5 error: 490.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 80.65365314483643, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 42.28059649467468, 57.68304908946156 ; AI error: 1133.148512338849 BCD5 error: 732.7\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.77803295850754, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 47.892666530609134, 48.28814598023891 ; AI error: 1621.3381588576435 BCD5 error: 877.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 61.10151067376137, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 52.42069540023803, 39.743703693151474 ; AI error: 2035.311004208132 BCD5 error: 1066.5\n",
            "Found AL122017 at 2017-09-11 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 74.50131356716156, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 30.36558997631073, 69.44833813607693 ; AI error: 208.07358740105605 BCD5 error: 179.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 77.7752161026001, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 37.0154804944992, 64.75452587753534 ; AI error: 698.7394496521243 BCD5 error: 500.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 69.16354328393936, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 42.55075287818909, 55.88672911939211 ; AI error: 1186.0242192891549 BCD5 error: 739.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 63.3515402674675, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 47.475316524505615, 46.52903799265623 ; AI error: 1657.8974156066254 BCD5 error: 906.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 56.84014916419983, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 51.36374797821045, 37.87732157707214 ; AI error: 2049.400751924764 BCD5 error: 1088.2\n",
            "Found AL122017 at 2017-09-12 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 74.76157695055008, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 31.69290406703949, 69.12793933451175 ; AI error: 320.0942955186167 BCD5 error: 194.6\n",
            "\tIntensity Truth: 70.0, AI forecast: 73.21181684732437, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 38.01141817569733, 64.17238349318504 ; AI error: 779.1021277772943 BCD5 error: 450.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.8241496682167, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 43.4204803943634, 54.98016261532902 ; AI error: 1246.2308622621474 BCD5 error: 646.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 62.047851383686066, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 47.931638383865355, 45.083622954785824 ; AI error: 1726.5357892990148 BCD5 error: 821.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 55.57756498456001, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 51.32015585899353, 36.820937517285344 ; AI error: 2059.9936403264655 BCD5 error: 971.4\n",
            "Found AL122017 at 2017-09-12 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 65.6365516781807, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 32.04918575286865, 68.96101966798305 ; AI error: 374.25573656998665 BCD5 error: 176.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 69.28817480802536, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 38.329674291610715, 63.989924801886076 ; AI error: 812.1592041370154 BCD5 error: 400.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 64.95989233255386, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 43.32441160678863, 54.31678458862007 ; AI error: 1252.4264939873985 BCD5 error: 592.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 58.92971321940422, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 47.275037908554076, 44.785236711800096 ; AI error: 1713.0759520228637 BCD5 error: 796.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 52.699478939175606, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 50.76830444335937, 36.326571503281585 ; AI error: 2030.5474179480627 BCD5 error: 911.8\n",
            "Found AL122017 at 2017-09-12 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 69.32931393384933, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 32.880093121528624, 67.80489740222693 ; AI error: 437.8206869910011 BCD5 error: 147.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.20215392112732, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 38.942096853256224, 62.00253932401537 ; AI error: 876.6115474463222 BCD5 error: 342.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 64.94072884321213, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 43.34497299194336, 52.53782464265823 ; AI error: 1300.8598500756257 BCD5 error: 557.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 58.9044226706028, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 47.32429571151734, 42.77675904482602 ; AI error: 1775.1305805190314 BCD5 error: 776.2\n",
            "\tIntensity Truth: 80.0, AI forecast: 55.636912137269974, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 50.95503768920898, 34.30011886358261 ; AI error: 2070.891886101773 BCD5 error: 854.0\n",
            "Found AL122017 at 2017-09-12 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 67.61397302150726, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 32.6471204161644, 65.96049816906452 ; AI error: 435.5309497675148 BCD5 error: 83.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 68.25106412172318, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 37.62035531997681, 60.92518262565136 ; AI error: 816.863678427995 BCD5 error: 240.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 63.398685455322266, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 41.7564952135086, 51.912390977889295 ; AI error: 1260.879159413282 BCD5 error: 448.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 60.4157055914402, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 45.754516792297366, 42.39550732970237 ; AI error: 1734.6193214994175 BCD5 error: 639.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 56.72814518213272, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 49.403226137161255, 34.575224125385276 ; AI error: 1994.6590183743156 BCD5 error: 683.3\n",
            "Found AL122017 at 2017-09-13 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 67.72640258073807, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 30.79180575609207, 65.90056689679622 ; AI error: 341.90402518052997 BCD5 error: 52.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.10978710651398, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 34.648924994468686, 62.30704892799258 ; AI error: 619.4611639450876 BCD5 error: 184.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 65.78267693519592, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 38.13978826999664, 54.832449461519715 ; AI error: 1033.927906143813 BCD5 error: 376.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 61.710990369319916, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 41.582541298866275, 46.994667790830135 ; AI error: 1427.760962782066 BCD5 error: 512.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 57.99986571073532, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 45.13601548671723, 39.95726502239704 ; AI error: 1670.443620387363 BCD5 error: 535.3\n",
            "Found AL122017 at 2017-09-13 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 68.76755833625793, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 29.094783270359038, 65.15328708142042 ; AI error: 255.00268614426955 BCD5 error: 57.4\n",
            "\tIntensity Truth: 60.0, AI forecast: 72.01759308576584, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 31.952266061306, 62.17162531390786 ; AI error: 486.5105664645768 BCD5 error: 184.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 68.04465293884277, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 34.412504363059995, 56.28936995246913 ; AI error: 874.1246702065706 BCD5 error: 366.1\n",
            "\tIntensity Truth: 75.0, AI forecast: 63.5937374830246, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 37.31199762821197, 49.93735863044858 ; AI error: 1188.8232139383063 BCD5 error: 457.1\n",
            "\tIntensity Truth: 75.0, AI forecast: 59.56845551729202, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 40.71503829956055, 44.406572714447975 ; AI error: 1378.8155636979814 BCD5 error: 466.3\n",
            "Found AL122017 at 2017-09-13 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.63019675016403, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 27.786549556255338, 63.48513584733009 ; AI error: 234.73797481611942 BCD5 error: 67.7\n",
            "\tIntensity Truth: 60.0, AI forecast: 75.28130054473877, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 29.869262862205503, 60.88899569883942 ; AI error: 479.9397483925913 BCD5 error: 205.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.8708044886589, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 32.071840977668764, 55.752813055459406 ; AI error: 864.1010102913654 BCD5 error: 345.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 65.96339076757431, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 34.92472603321075, 50.65798418968916 ; AI error: 1103.1535977950189 BCD5 error: 392.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 61.456892639398575, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 38.111177468299864, 46.22788343429565 ; AI error: 1243.0196618148648 BCD5 error: 400.5\n",
            "Found AL122017 at 2017-09-13 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.82446020841599, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 26.300470149517057, 62.8601388387382 ; AI error: 232.15989301956188 BCD5 error: 65.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 76.91867113113403, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 28.05481787919998, 60.8515216961503 ; AI error: 487.9357855123434 BCD5 error: 197.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.36934506893158, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 30.205727422237395, 56.954420321993524 ; AI error: 792.2729725801623 BCD5 error: 284.9\n",
            "\tIntensity Truth: 80.0, AI forecast: 68.80881011486053, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 32.91122434139252, 52.76125864088535 ; AI error: 972.90782099955 BCD5 error: 308.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 64.45018023252487, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 35.95565366744995, 49.466551700979466 ; AI error: 1070.8460468428084 BCD5 error: 328.1\n",
            "Found AL122017 at 2017-09-14 00:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 74.81064140796661, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 25.396109414100646, 63.30210695937276 ; AI error: 233.09728185407815 BCD5 error: 64.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 78.60009104013443, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 27.08512679338455, 62.1790895909071 ; AI error: 455.22385691699617 BCD5 error: 184.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 75.38614898920059, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 29.09775129556656, 58.99756435602903 ; AI error: 681.7898823177111 BCD5 error: 225.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 71.00230157375336, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 31.681037974357604, 55.87436470161192 ; AI error: 801.933456317422 BCD5 error: 234.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 66.48276805877686, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 34.61753091812134, 53.41161032356322 ; AI error: 886.2785343829164 BCD5 error: 285.9\n",
            "Found AL122017 at 2017-09-14 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 76.0146763920784, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 25.009984880685806, 64.30231626182794 ; AI error: 216.6349421032433 BCD5 error: 69.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 80.38037419319153, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 26.59306546449661, 63.53906161338091 ; AI error: 425.1297065631421 BCD5 error: 183.2\n",
            "\tIntensity Truth: 75.0, AI forecast: 77.45835930109024, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 28.55785459280014, 61.22598234787583 ; AI error: 569.4742676499927 BCD5 error: 183.4\n",
            "\tIntensity Truth: 75.0, AI forecast: 72.94725179672241, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 31.118286275863646, 58.96617238037288 ; AI error: 640.4107367095955 BCD5 error: 188.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 67.90151089429855, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 34.135740518569946, 56.690442082332446 ; AI error: 738.9869342807357 BCD5 error: 267.4\n",
            "Found AL122017 at 2017-09-14 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 74.41315442323685, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 24.58525574207306, 65.24463470578193 ; AI error: 228.138621379731 BCD5 error: 64.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 81.1227810382843, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 26.058604633808134, 65.24998310804366 ; AI error: 372.7952674412145 BCD5 error: 149.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 79.98147487640381, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 27.97854894399643, 63.854933604598045 ; AI error: 447.7090624681059 BCD5 error: 140.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 75.71197211742401, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 30.58999881744385, 62.04625199884176 ; AI error: 497.30776727926036 BCD5 error: 153.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 67.91992336511612, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 33.646992683410645, 60.47958220392465 ; AI error: 574.1502170887518 BCD5 error: 258.7\n",
            "Found AL122017 at 2017-09-14 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 74.42390233278275, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 24.31680417060852, 66.56475231349468 ; AI error: 234.20987167602098 BCD5 error: 53.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 81.75150871276855, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 25.671754240989685, 67.33980685323476 ; AI error: 304.76383756766757 BCD5 error: 122.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 80.58923244476318, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 27.541028082370758, 66.31056669801474 ; AI error: 361.5113442539323 BCD5 error: 131.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.74375492334366, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 30.12803155183792, 65.2462880641222 ; AI error: 388.8865771558051 BCD5 error: 158.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 70.49868941307068, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 33.54949996471405, 64.32543953806162 ; AI error: 407.97706449991443 BCD5 error: 274.9\n",
            "Found AL122017 at 2017-09-15 00:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 71.57197833061218, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 24.39480720758438, 68.31404104828835 ; AI error: 212.1226763922388 BCD5 error: 70.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 79.49648380279541, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 25.757656490802763, 69.0342066347599 ; AI error: 255.4145884221163 BCD5 error: 115.5\n",
            "\tIntensity Truth: 80.0, AI forecast: 76.21283680200577, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 27.587082040309905, 68.17362344413996 ; AI error: 314.5271343981708 BCD5 error: 127.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 74.78531748056412, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 30.593020164966582, 67.25531091541052 ; AI error: 332.60347503561445 BCD5 error: 191.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 66.90868109464645, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 34.479125714302064, 65.69369530975818 ; AI error: 326.35889708667685 BCD5 error: 291.1\n",
            "Found AL122017 at 2017-09-15 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 69.68401789665222, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 24.773751628398895, 69.25973658263683 ; AI error: 210.11544417928366 BCD5 error: 70.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 72.98422694206238, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 26.27933686375618, 69.96357207596301 ; AI error: 237.3213161109469 BCD5 error: 83.5\n",
            "\tIntensity Truth: 75.0, AI forecast: 75.06452322006226, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 28.645695221424102, 68.8990577325225 ; AI error: 275.5342850913847 BCD5 error: 98.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 68.86121243238449, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 32.129034793376924, 67.14034570306539 ; AI error: 304.663825640735 BCD5 error: 195.1\n",
            "\tIntensity Truth: 60.0, AI forecast: 65.94871044158936, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 36.343707942962645, 64.13734331876039 ; AI error: 335.4480610581413 BCD5 error: 269.8\n",
            "Found AL122017 at 2017-09-15 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 62.79899924993515, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 25.460487848520277, 70.17431930452585 ; AI error: 186.1123126451858 BCD5 error: 35.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 71.78839057683945, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 27.664919102191924, 70.66165557801723 ; AI error: 182.19492881782767 BCD5 error: 31.7\n",
            "\tIntensity Truth: 70.0, AI forecast: 68.83343517780304, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 30.505033087730407, 68.66622757166624 ; AI error: 221.2901145471057 BCD5 error: 39.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.12567949295044, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 34.23816592693329, 65.59354927539826 ; AI error: 323.8452433458562 BCD5 error: 155.0\n",
            "\tIntensity Truth: 60.0, AI forecast: 65.38767248392105, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 38.585134744644165, 62.221168832480906 ; AI error: 374.1128246566767 BCD5 error: 211.4\n",
            "Found AL122017 at 2017-09-15 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.22847074270248, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 26.930014431476593, 70.80675939321517 ; AI error: 118.62415231096368 BCD5 error: 20.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 74.12489056587219, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 29.864075577259065, 70.53840152919292 ; AI error: 103.39221642261035 BCD5 error: 42.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 74.24242913722992, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 33.0324807882309, 67.40886635929346 ; AI error: 202.08412767894905 BCD5 error: 11.6\n",
            "\tIntensity Truth: 60.0, AI forecast: 71.5315568447113, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 36.91470732688904, 64.11130080521106 ; AI error: 354.43816656267927 BCD5 error: 109.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 69.78731632232666, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 41.105405712127684, 60.31011806130409 ; AI error: 432.3098362037 BCD5 error: 137.3\n",
            "Found AL122017 at 2017-09-16 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 67.63766139745712, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 28.10109188556671, 71.35911334753037 ; AI error: 68.83518516931841 BCD5 error: 33.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 74.78428274393082, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 31.269533896446227, 70.30764696896077 ; AI error: 76.16991514317822 BCD5 error: 56.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 74.43636924028397, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 34.531880259513855, 66.9727248430252 ; AI error: 219.59326907910406 BCD5 error: 24.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 73.83195370435715, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 38.355922389030454, 63.164843030273914 ; AI error: 382.85169385256313 BCD5 error: 113.2\n",
            "\tIntensity Truth: 55.0, AI forecast: 67.35385149717331, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 42.62233695983886, 58.69286058843136 ; AI error: 484.22143384508655 BCD5 error: 113.0\n",
            "Found AL122017 at 2017-09-16 06:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 75.75962424278259, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 28.814923727512358, 71.87596333920956 ; AI error: 59.49988531856849 BCD5 error: 28.6\n",
            "\tIntensity Truth: 75.0, AI forecast: 81.36228919029236, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 32.15282781124115, 71.11979354321957 ; AI error: 35.79020323782292 BCD5 error: 32.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 80.91129422187805, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 35.43399319648743, 67.56423880159855 ; AI error: 197.46356711506877 BCD5 error: 77.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 73.30567121505737, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 39.42345316410065, 63.15513315349817 ; AI error: 366.4994120412411 BCD5 error: 159.2\n",
            "\tIntensity Truth: 55.0, AI forecast: 66.0363918542862, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 43.94169626235961, 58.15940642766654 ; AI error: 512.0182414096878 BCD5 error: 166.2\n",
            "Found AL122017 at 2017-09-16 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 77.32214540243149, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 29.545418417453767, 72.72909963428974 ; AI error: 71.71051761126456 BCD5 error: 12.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 85.22549510002136, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 32.92871105670929, 72.01354310214519 ; AI error: 53.35231361121414 BCD5 error: 42.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 79.22066181898117, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 36.327995133399966, 67.86846182346343 ; AI error: 185.36391738874676 BCD5 error: 139.7\n",
            "\tIntensity Truth: 60.0, AI forecast: 71.90584987401962, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 40.61957032680512, 62.829656422138214 ; AI error: 359.50253981231526 BCD5 error: 191.5\n",
            "\tIntensity Truth: 55.0, AI forecast: 64.76418942213058, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 45.1253600358963, 57.200204863399264 ; AI error: 578.2613140179512 BCD5 error: 221.8\n",
            "Found AL122017 at 2017-09-16 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 82.17303037643433, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 29.997813200950624, 73.40616598427296 ; AI error: 109.9972272483967 BCD5 error: 26.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 83.99908185005188, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 33.46479859352112, 72.4001576334238 ; AI error: 74.37162882214405 BCD5 error: 77.0\n",
            "\tIntensity Truth: 60.0, AI forecast: 78.133105635643, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 37.21534905433654, 67.80263950824738 ; AI error: 177.49883249791606 BCD5 error: 194.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 70.87734669446945, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 41.57404427528381, 62.15138396844267 ; AI error: 360.6849597480818 BCD5 error: 219.1\n",
            "\tIntensity Truth: 50.0, AI forecast: 64.20569449663162, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 45.98496232032775, 55.91996236392296 ; AI error: 658.609041349867 BCD5 error: 293.9\n",
            "Found AL122017 at 2017-09-17 00:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 78.14921498298645, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 30.39273028373718, 73.13536962866783 ; AI error: 120.05600871631476 BCD5 error: 31.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 79.47144985198975, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 34.02751724720001, 71.70852138102055 ; AI error: 54.56512652927872 BCD5 error: 124.6\n",
            "\tIntensity Truth: 60.0, AI forecast: 73.39528232812881, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 37.60028810501099, 66.4108687415719 ; AI error: 227.7911986504914 BCD5 error: 257.8\n",
            "\tIntensity Truth: 55.0, AI forecast: 67.15313971042633, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 41.68919043540954, 59.90343006402254 ; AI error: 415.3760420830206 BCD5 error: 278.0\n",
            "\tIntensity Truth: 45.0, AI forecast: 61.89547911286354, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 46.07365760803222, 52.51266858354211 ; AI error: 793.9273069461592 BCD5 error: 439.4\n",
            "Found AL122017 at 2017-09-17 06:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 79.9796974658966, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 31.34507284164429, 72.48097209334374 ; AI error: 98.21145401719025 BCD5 error: 37.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 79.11787450313568, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 34.99880049228668, 70.2580148473382 ; AI error: 75.0139251241015 BCD5 error: 181.1\n",
            "\tIntensity Truth: 60.0, AI forecast: 71.9014647603035, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 38.34519333839417, 64.03951622545719 ; AI error: 319.15054755204227 BCD5 error: 302.6\n",
            "\tIntensity Truth: 55.0, AI forecast: 65.56589543819427, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 42.49393982887268, 56.251698970946016 ; AI error: 561.1641427092618 BCD5 error: 350.7\n",
            "\tIntensity Truth: 45.0, AI forecast: 58.9523147046566, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 46.83266096115112, 47.76356335878372 ; AI error: 1005.7126369618786 BCD5 error: 565.5\n",
            "Found AL122017 at 2017-09-17 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 82.08916664123535, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 32.16585422754288, 71.35144515633583 ; AI error: 80.46626051556939 BCD5 error: 45.4\n",
            "\tIntensity Truth: 60.0, AI forecast: 79.6864601969719, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 35.716051721572875, 68.62416613548994 ; AI error: 153.44103623901333 BCD5 error: 213.6\n",
            "\tIntensity Truth: 60.0, AI forecast: 71.65083318948746, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 39.264762949943545, 61.55987509712577 ; AI error: 404.5179192303753 BCD5 error: 319.6\n",
            "\tIntensity Truth: 55.0, AI forecast: 63.51089596748352, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 43.476067781448364, 52.76950211599469 ; AI error: 719.3044192113568 BCD5 error: 404.8\n",
            "\tIntensity Truth: 40.0, AI forecast: 56.730618327856064, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 47.61374125480651, 43.30363347530364 ; AI error: 1205.9330827789702 BCD5 error: 642.3\n",
            "Found AL122017 at 2017-09-17 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 83.87140035629272, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 33.013066959381106, 70.41690682321787 ; AI error: 81.31317973596369 BCD5 error: 59.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 81.82702779769897, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 37.070087051391596, 67.06401311606169 ; AI error: 212.63186746246433 BCD5 error: 235.4\n",
            "\tIntensity Truth: 60.0, AI forecast: 71.99525028467178, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 40.87814300060272, 59.15053137391806 ; AI error: 481.4881024263607 BCD5 error: 322.6\n",
            "\tIntensity Truth: 50.0, AI forecast: 63.3004105091095, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 44.97161452770233, 49.2839950799942 ; AI error: 891.9084084633838 BCD5 error: 464.8\n",
            "Found AL122017 at 2017-09-18 00:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 85.82282185554504, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 34.59395945072174, 69.35622318387031 ; AI error: 102.48608374557072 BCD5 error: 99.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 81.29493117332458, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 39.16787855625152, 64.94547180831432 ; AI error: 308.9687402747404 BCD5 error: 263.6\n",
            "\tIntensity Truth: 55.0, AI forecast: 70.6453612446785, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 42.89811546802521, 55.528462927136566 ; AI error: 622.1302606096315 BCD5 error: 349.9\n",
            "\tIntensity Truth: 45.0, AI forecast: 60.741062462329865, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 46.936151313781735, 44.763264851272105 ; AI error: 1112.9288175149227 BCD5 error: 555.5\n",
            "Found AL122017 at 2017-09-18 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 82.91057229042053, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 35.940775990486145, 68.7147574543953 ; AI error: 142.02987540053087 BCD5 error: 125.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 79.83542263507843, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 40.56489768028259, 63.00080020725727 ; AI error: 391.08292996432004 BCD5 error: 284.2\n",
            "\tIntensity Truth: 55.0, AI forecast: 69.11030441522598, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 44.31585369110107, 52.47147325277328 ; AI error: 748.792627526321 BCD5 error: 386.0\n",
            "\tIntensity Truth: 45.0, AI forecast: 59.92378741502762, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 48.298841381073, 41.23595370948314 ; AI error: 1284.0606237149505 BCD5 error: 636.8\n",
            "Found AL122017 at 2017-09-18 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 79.04877662658691, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 36.53143253326416, 68.20696871429682 ; AI error: 169.33238563371583 BCD5 error: 122.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 76.18953227996826, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 41.099956321716306, 61.7451813802123 ; AI error: 415.02112804390447 BCD5 error: 243.4\n",
            "\tIntensity Truth: 55.0, AI forecast: 67.35009849071503, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 44.837267827987674, 50.62554529905319 ; AI error: 829.2016480024503 BCD5 error: 374.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 58.19329470396042, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 48.74645442962647, 39.4611133813858 ; AI error: 1371.0950782574223 BCD5 error: 638.2\n",
            "Found AL122017 at 2017-09-18 18:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 71.66086137294769, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 36.9080908536911, 67.80017473250628 ; AI error: 177.5900405464066 BCD5 error: 82.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 71.5558648109436, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 41.42921485900879, 60.63342338204384 ; AI error: 422.84099080244516 BCD5 error: 158.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 63.55198085308075, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 44.976497650146484, 49.47902099043131 ; AI error: 883.9907662479644 BCD5 error: 343.1\n",
            "Found AL122017 at 2017-09-19 00:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.73048931360245, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 37.46632142066956, 67.46207482218742 ; AI error: 178.1397366012509 BCD5 error: 46.3\n",
            "\tIntensity Truth: 55.0, AI forecast: 66.07428073883057, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 41.80776875019073, 59.935000305622815 ; AI error: 416.10336971587594 BCD5 error: 113.2\n",
            "\tIntensity Truth: 45.0, AI forecast: 59.54613044857979, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 44.88894090652465, 48.840113303065294 ; AI error: 917.1737810835024 BCD5 error: 355.2\n",
            "Found AL122017 at 2017-09-19 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 63.888743817806244, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 37.871465873718265, 67.50252359807492 ; AI error: 156.63603879118102 BCD5 error: 41.2\n",
            "\tIntensity Truth: 55.0, AI forecast: 62.480190843343735, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 41.780551838874814, 59.98434542119503 ; AI error: 389.5649627824037 BCD5 error: 128.2\n",
            "\tIntensity Truth: 45.0, AI forecast: 57.01843053102493, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 44.256388044357294, 49.43488310575485 ; AI error: 898.1107027942646 BCD5 error: 409.0\n",
            "Found AL122017 at 2017-09-19 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 63.07248890399933, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 38.17549619674682, 67.8931680843234 ; AI error: 112.96185482250178 BCD5 error: 48.6\n",
            "\tIntensity Truth: 55.0, AI forecast: 63.007100224494934, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 41.688826942443846, 60.88846408054232 ; AI error: 344.9235417703694 BCD5 error: 192.4\n",
            "\tIntensity Truth: 40.0, AI forecast: 59.984927624464035, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 44.22873854637146, 50.51849161833525 ; AI error: 866.5057566394161 BCD5 error: 492.3\n",
            "Found AL122017 at 2017-09-19 18:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 64.64582055807114, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 38.313671588897705, 68.45907533764839 ; AI error: 69.08359738490641 BCD5 error: 48.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 66.51380181312561, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 41.94529376029968, 61.71810445785522 ; AI error: 321.2299310800002 BCD5 error: 264.3\n",
            "Found AL122017 at 2017-09-20 00:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 68.94008815288544, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 39.09924747943878, 68.53832377046346 ; AI error: 18.145027912852335 BCD5 error: 45.9\n",
            "\tIntensity Truth: 45.0, AI forecast: 67.23601877689362, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 42.79844479560852, 61.3798917800188 ; AI error: 366.27616675678803 BCD5 error: 293.3\n",
            "Found AL122017 at 2017-09-20 06:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 67.47427314519882, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 39.43796284198761, 68.12352499067784 ; AI error: 15.770466796888597 BCD5 error: 70.8\n",
            "\tIntensity Truth: 45.0, AI forecast: 63.657484352588654, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 42.87050502300262, 60.77754874825477 ; AI error: 403.8643094735404 BCD5 error: 373.6\n",
            "Found AL122017 at 2017-09-20 12:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 63.04602384567261, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 39.54413132667541, 67.1700349316001 ; AI error: 39.50781357999342 BCD5 error: 134.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 62.61289894580841, BCD5 forecast: 23.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 42.85558979511261, 59.23216297440231 ; AI error: 479.3546297353723 BCD5 error: 476.9\n",
            "Found AL122017 at 2017-09-20 18:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 65.99037319421768, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 40.102486562728885, 65.49968006759882 ; AI error: 120.86759034617604 BCD5 error: 175.9\n",
            "Found AL122017 at 2017-09-21 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 63.60268920660019, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 40.099212121963504, 64.70864984691143 ; AI error: 165.00765950901294 BCD5 error: 178.5\n",
            "Found AL122017 at 2017-09-21 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 62.30067238211632, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 39.907753467559814, 63.92607803791761 ; AI error: 216.50837224039194 BCD5 error: 140.3\n",
            "Found AL122017 at 2017-09-21 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 61.056520491838455, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 39.94302580356597, 62.990582876652475 ; AI error: 277.44751486504884 BCD5 error: 97.5\n",
            "Found AL122017 at 2017-09-21 18:00:00\n",
            "Found AL122017 at 2017-09-22 00:00:00\n",
            "Found AL122017 at 2017-09-22 06:00:00\n",
            "Found AL122017 at 2017-09-22 12:00:00\n",
            "Found AL132017 at 2017-09-06 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 63.442730605602264, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 21.6, 94.6; AI forecast: 23.039838737249372, 90.16016155481339 ; AI error: 261.2943255743661 BCD5 error: 37.4\n",
            "\tIntensity Truth: 90.0, AI forecast: 61.04652673006058, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 21.1, 96.2; AI forecast: 24.356459075212477, 92.62991602420807 ; AI error: 278.0235152872436 BCD5 error: 183.7\n",
            "Found AL132017 at 2017-09-07 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 65.56371748447418, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 21.5, 95.0; AI forecast: 22.547795808315275, 89.42151905298233 ; AI error: 316.7795377352456 BCD5 error: 49.1\n",
            "\tIntensity Truth: 70.0, AI forecast: 67.67581522464752, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 20.8, 96.9; AI forecast: 23.584824717044828, 91.92858685255051 ; AI error: 322.97055217988395 BCD5 error: 197.1\n",
            "Found AL132017 at 2017-09-07 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 75.76395094394684, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 21.4, 95.3; AI forecast: 22.422769623994828, 88.53963975906372 ; AI error: 381.51084950880676 BCD5 error: 53.1\n",
            "\tIntensity Truth: 35.0, AI forecast: 77.97148644924164, BCD5 forecast: 42.0\n",
            "\tTrajectory Truth: 20.3, 97.4; AI forecast: 23.641593080759048, 90.94031267762185 ; AI error: 411.7589307638563 BCD5 error: 220.9\n",
            "Found AL132017 at 2017-09-07 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 82.95217871665955, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 21.1, 95.7; AI forecast: 22.612986469268797, 87.88047716021538 ; AI error: 445.0446049187856 BCD5 error: 71.6\n",
            "Found AL132017 at 2017-09-07 18:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 89.29036617279053, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 21.1, 96.2; AI forecast: 22.322924715280532, 88.74990963339806 ; AI error: 421.9600548328099 BCD5 error: 58.5\n",
            "Found AL132017 at 2017-09-08 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 77.97811210155487, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 20.8, 96.9; AI forecast: 22.135457348823547, 90.66637070178986 ; AI error: 357.38093129571314 BCD5 error: 47.6\n",
            "Found AL132017 at 2017-09-08 06:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 76.8369123339653, BCD5 forecast: 54.0\n",
            "\tTrajectory Truth: 20.3, 97.4; AI forecast: 21.956352385878564, 91.15906470417977 ; AI error: 363.35350574059714 BCD5 error: 58.6\n",
            "Found AL132017 at 2017-09-08 12:00:00\n",
            "Found AL142017 at 2017-09-16 00:00:00\n",
            "Found AL142017 at 2017-09-16 06:00:00\n",
            "Found AL142017 at 2017-09-16 12:00:00\n",
            "Found AL142017 at 2017-09-16 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 42.10868429392576, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 16.8, 44.4; AI forecast: 18.85871799737215, 48.56437382996082 ; AI error: 268.18465339442804 BCD5 error: 24.9\n",
            "Found AL142017 at 2017-09-17 00:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 49.066077917814255, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 17.6, 45.0; AI forecast: 17.51573963165283, 49.16460649818182 ; AI error: 238.4449286570499 BCD5 error: 64.2\n",
            "Found AL142017 at 2017-09-17 06:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 48.96616689860821, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 18.3, 45.2; AI forecast: 17.248048841953278, 49.20677815824747 ; AI error: 237.6249620551039 BCD5 error: 113.2\n",
            "Found AL142017 at 2017-09-17 12:00:00\n",
            "Found AL142017 at 2017-09-17 18:00:00\n",
            "Found AL142017 at 2017-09-18 00:00:00\n",
            "Found AL142017 at 2017-09-18 06:00:00\n",
            "Found AL142017 at 2017-09-18 12:00:00\n",
            "Found AL142017 at 2017-09-18 18:00:00\n",
            "Found AL142017 at 2017-09-19 00:00:00\n",
            "Found AL142017 at 2017-09-19 06:00:00\n",
            "Found AL142017 at 2017-09-19 12:00:00\n",
            "Found AL142017 at 2017-09-19 18:00:00\n",
            "Found AL142017 at 2017-09-20 00:00:00\n",
            "Found AL142017 at 2017-09-20 06:00:00\n",
            "Found AL142017 at 2017-09-22 12:00:00\n",
            "Found AL142017 at 2017-09-22 18:00:00\n",
            "Found AL142017 at 2017-09-23 00:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 37.71249279379845, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 31.9, 50.1; AI forecast: 29.858065927028655, 51.76782514005899 ; AI error: 149.71593479587347 BCD5 error: 106.6\n",
            "\tIntensity Truth: 85.0, AI forecast: 41.11415296792984, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 31.2, 49.6; AI forecast: 30.126166772842407, 51.305532158166166 ; AI error: 109.15535668398884 BCD5 error: 284.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 46.373693719506264, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 30.3, 51.0; AI forecast: 29.47925518751144, 50.980267819762226 ; AI error: 49.288615024415 BCD5 error: 539.4\n",
            "\tIntensity Truth: 95.0, AI forecast: 54.259783029556274, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 29.9, 55.1; AI forecast: 29.421073782443997, 49.916503846645355 ; AI error: 271.941673446641 BCD5 error: 863.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 58.34653064608574, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 30.12489454746246, 48.040781527757645 ; AI error: 478.5584115989758 BCD5 error: 1061.5\n",
            "Found AL142017 at 2017-09-23 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 39.23335589468479, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 31.7, 50.2; AI forecast: 30.18319611549377, 50.816312327980995 ; AI error: 96.44085471510051 BCD5 error: 108.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 43.916932716965675, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 31.0, 49.5; AI forecast: 30.54359939098358, 50.276272501796484 ; AI error: 48.52372342584508 BCD5 error: 267.7\n",
            "\tIntensity Truth: 85.0, AI forecast: 53.569523468613625, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 30.1, 52.0; AI forecast: 30.11535285711288, 49.54531133323908 ; AI error: 127.49758127040033 BCD5 error: 538.2\n",
            "\tIntensity Truth: 95.0, AI forecast: 59.23979938030243, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 30.1, 56.0; AI forecast: 30.428002619743346, 47.772580520808695 ; AI error: 427.01502681957436 BCD5 error: 838.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 57.922542095184326, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 31.195225870609285, 44.92827679663896 ; AI error: 634.319702182826 BCD5 error: 979.1\n",
            "Found AL142017 at 2017-09-23 12:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 44.37555782496929, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 31.5, 50.1; AI forecast: 31.10404396057129, 50.11595736593008 ; AI error: 23.787480594359497 BCD5 error: 104.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 53.46080832183361, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 30.8, 49.7; AI forecast: 31.841953456401825, 49.16396635174751 ; AI error: 68.3339283755947 BCD5 error: 261.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 59.6537284553051, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 29.9, 53.2; AI forecast: 31.82885419130325, 47.70096721202135 ; AI error: 306.10431374674 BCD5 error: 540.7\n",
            "\tIntensity Truth: 100.0, AI forecast: 59.32412102818489, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 30.3, 56.6; AI forecast: 32.17324274778366, 44.89788467884063 ; AI error: 610.8520572070755 BCD5 error: 797.6\n",
            "\tIntensity Truth: 85.0, AI forecast: 60.66472142934799, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 33.330773329734804, 41.309322121739385 ; AI error: 798.1334410166061 BCD5 error: 871.6\n",
            "Found AL142017 at 2017-09-23 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 54.00206580758095, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 31.3, 49.8; AI forecast: 32.020580208301546, 49.84478116184473 ; AI error: 43.324465441507705 BCD5 error: 98.8\n",
            "\tIntensity Truth: 75.0, AI forecast: 60.887313932180405, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 30.6, 50.2; AI forecast: 33.29286341667175, 48.39401263445615 ; AI error: 186.01845148142567 BCD5 error: 228.5\n",
            "\tIntensity Truth: 95.0, AI forecast: 61.35204628109932, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 29.9, 54.2; AI forecast: 33.39267077445984, 46.035694517195225 ; AI error: 466.81550273283 BCD5 error: 471.2\n",
            "\tIntensity Truth: 100.0, AI forecast: 63.32043558359146, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 30.8, 57.0; AI forecast: 34.214942932128906, 42.67513855397701 ; AI error: 752.9829222141219 BCD5 error: 676.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 62.255956530570984, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 35.75342240333557, 39.20129532516002 ; AI error: 858.7280544764048 BCD5 error: 672.6\n",
            "Found AL142017 at 2017-09-24 00:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 57.01260179281235, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 31.2, 49.6; AI forecast: 32.684718656539914, 50.05451857000589 ; AI error: 92.10163347901697 BCD5 error: 73.9\n",
            "\tIntensity Truth: 80.0, AI forecast: 59.41580578684807, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 30.3, 51.0; AI forecast: 33.89406576156616, 48.08976841568946 ; AI error: 261.64698127735795 BCD5 error: 186.3\n",
            "\tIntensity Truth: 95.0, AI forecast: 63.0974081158638, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 29.9, 55.1; AI forecast: 34.43669166564941, 45.485342398285866 ; AI error: 559.0536795309124 BCD5 error: 431.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 63.33718955516815, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 35.62940671443939, 42.37536603361367 ; AI error: 783.3720476624208 BCD5 error: 583.7\n",
            "\tIntensity Truth: 75.0, AI forecast: 61.230835020542145, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 37.55255937576294, 38.458832293748856 ; AI error: 827.4623169452931 BCD5 error: 501.0\n",
            "Found AL142017 at 2017-09-24 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 60.736246556043625, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 31.0, 49.5; AI forecast: 32.208025419712065, 50.36566449999809 ; AI error: 84.97071513022432 BCD5 error: 55.2\n",
            "\tIntensity Truth: 85.0, AI forecast: 64.94623839855194, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 30.1, 52.0; AI forecast: 33.56739072799682, 49.17428924292326 ; AI error: 253.18237326024078 BCD5 error: 207.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 63.624739944934845, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 30.1, 56.0; AI forecast: 34.25859363079071, 47.65644184798002 ; AI error: 491.78586529968254 BCD5 error: 467.8\n",
            "\tIntensity Truth: 90.0, AI forecast: 60.87377890944481, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 35.66567189693451, 44.713405498862265 ; AI error: 662.2944197774564 BCD5 error: 587.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 60.46105772256851, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 37.68146719932556, 41.10496703088283 ; AI error: 604.7527192501568 BCD5 error: 410.6\n",
            "Found AL142017 at 2017-09-24 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 73.00220966339111, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 30.8, 49.7; AI forecast: 32.1243514418602, 50.36628556922078 ; AI error: 86.52655782888755 BCD5 error: 44.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 72.63626128435135, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 29.9, 53.2; AI forecast: 33.63130691051483, 49.96774438917637 ; AI error: 278.1913214342268 BCD5 error: 292.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 66.78080141544342, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 30.3, 56.6; AI forecast: 34.59387083053589, 48.40065743923187 ; AI error: 488.6621227082284 BCD5 error: 538.0\n",
            "\tIntensity Truth: 85.0, AI forecast: 64.17445421218872, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 36.185579490661624, 45.64433696717023 ; AI error: 601.8557157501837 BCD5 error: 613.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 57.356446236371994, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 38.44554743766784, 42.51932606399059 ; AI error: 407.8282849537217 BCD5 error: 322.5\n",
            "Found AL142017 at 2017-09-24 18:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 78.63694101572037, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 30.6, 50.2; AI forecast: 31.64107177257538, 50.72198950648308 ; AI error: 68.02105552792898 BCD5 error: 85.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 77.17064410448074, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 29.9, 54.2; AI forecast: 33.11948928833008, 50.24922779873013 ; AI error: 279.7045131387721 BCD5 error: 366.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 73.11617881059647, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 30.8, 57.0; AI forecast: 34.18449213504791, 48.667094863951206 ; AI error: 468.1528123258704 BCD5 error: 577.2\n",
            "\tIntensity Truth: 80.0, AI forecast: 64.67506438493729, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 35.99053547382355, 45.993243905901906 ; AI error: 530.2480440114498 BCD5 error: 594.5\n",
            "\tIntensity Truth: 55.0, AI forecast: 56.10703095793724, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 38.471465086936945, 42.88684473037719 ; AI error: 288.4081887076743 BCD5 error: 215.7\n",
            "Found AL142017 at 2017-09-25 00:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 83.35415720939636, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 30.3, 51.0; AI forecast: 31.159813845157622, 50.13519143462181 ; AI error: 68.24215673757197 BCD5 error: 125.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 83.9371645450592, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 29.9, 55.1; AI forecast: 32.629687023162845, 49.3106408573687 ; AI error: 339.2398359326093 BCD5 error: 388.4\n",
            "\tIntensity Truth: 95.0, AI forecast: 73.00283551216125, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 33.77670109272003, 47.537110073864454 ; AI error: 508.99052327970503 BCD5 error: 545.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 62.238464057445526, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 35.72787876129151, 44.692037326097484 ; AI error: 526.6171270097661 BCD5 error: 481.2\n",
            "\tIntensity Truth: 50.0, AI forecast: 54.193317890167236, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 38.31240537166595, 42.1956866800785 ; AI error: 311.5703104707339 BCD5 error: 157.0\n",
            "Found AL142017 at 2017-09-25 06:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 90.06978154182434, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 30.1, 52.0; AI forecast: 31.09313391447067, 49.6311626009643 ; AI error: 136.1687379630522 BCD5 error: 133.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 85.98301410675049, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 30.1, 56.0; AI forecast: 32.70849064588547, 48.35186725854874 ; AI error: 421.9238414033928 BCD5 error: 353.6\n",
            "\tIntensity Truth: 90.0, AI forecast: 73.75756114721298, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 34.085431289672854, 46.14791600257158 ; AI error: 573.1678394027092 BCD5 error: 443.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 63.51491183042526, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 36.21822326183319, 43.731463262438766 ; AI error: 488.41130951125143 BCD5 error: 291.1\n",
            "Found AL142017 at 2017-09-25 12:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 85.21623253822327, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 29.9, 53.2; AI forecast: 30.857000172138214, 49.245484972745174 ; AI error: 212.72431089560448 BCD5 error: 133.3\n",
            "\tIntensity Truth: 100.0, AI forecast: 81.39928102493286, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 30.3, 56.6; AI forecast: 32.428192508220675, 47.62684758156538 ; AI error: 477.267599497731 BCD5 error: 278.4\n",
            "\tIntensity Truth: 85.0, AI forecast: 71.54814600944519, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 33.90484437942505, 45.85293737351894 ; AI error: 570.7193045293127 BCD5 error: 308.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 59.83344718813896, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 36.15623269081116, 42.85029024928808 ; AI error: 432.2176563513436 BCD5 error: 116.7\n",
            "Found AL142017 at 2017-09-25 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 78.34015727043152, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 29.9, 54.2; AI forecast: 30.509524178504943, 49.31950158625841 ; AI error: 255.85300617205837 BCD5 error: 95.2\n",
            "\tIntensity Truth: 100.0, AI forecast: 79.30379539728165, BCD5 forecast: -35.0\n",
            "\tTrajectory Truth: 30.8, 57.0; AI forecast: 32.09274407625198, 47.976838105916976 ; AI error: 468.509795522319 BCD5 error: 165.1\n",
            "\tIntensity Truth: 80.0, AI forecast: 68.58465611934662, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 33.48338630199432, 45.51396160721779 ; AI error: 554.4117583099674 BCD5 error: 119.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 62.489738166332245, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 35.86388821601868, 42.63754203319549 ; AI error: 404.2088885903057 BCD5 error: 348.2\n",
            "Found AL142017 at 2017-09-26 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 78.8352245092392, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 29.9, 55.1; AI forecast: 30.38420246839523, 50.562318332493305 ; AI error: 237.37536814451576 BCD5 error: 50.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 73.47761064767838, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 31.90354673862457, 48.650041532516475 ; AI error: 437.91538253527693 BCD5 error: 46.5\n",
            "\tIntensity Truth: 75.0, AI forecast: 67.61376231908798, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 33.514233636856076, 46.50074182450771 ; AI error: 463.4734486310029 BCD5 error: 128.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 58.42135712504387, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 35.863954305648804, 44.34542813152075 ; AI error: 440.51761175627547 BCD5 error: 662.6\n",
            "Found AL142017 at 2017-09-26 06:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 71.60887628793716, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 30.1, 56.0; AI forecast: 29.5484645485878, 51.716108091175556 ; AI error: 225.57051870996727 BCD5 error: 12.0\n",
            "\tIntensity Truth: 90.0, AI forecast: 72.66529440879822, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 31.029756212234496, 50.614475005120035 ; AI error: 347.935218363943 BCD5 error: 54.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 63.26303273439407, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 32.39379135370255, 49.443339397758244 ; AI error: 364.38886202777525 BCD5 error: 314.3\n",
            "Found AL142017 at 2017-09-26 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 82.13444471359253, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 30.3, 56.6; AI forecast: 29.38905711174011, 53.35496329925954 ; AI error: 177.61598199076488 BCD5 error: 16.7\n",
            "\tIntensity Truth: 85.0, AI forecast: 77.94055908918381, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 30.90611227750778, 53.0371840108186 ; AI error: 246.49792684587098 BCD5 error: 113.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 75.29760807752609, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 32.41316162347793, 52.4786344140768 ; AI error: 400.34622314947336 BCD5 error: 477.5\n",
            "Found AL142017 at 2017-09-26 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 82.84074425697327, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 30.8, 57.0; AI forecast: 28.91383136510849, 55.04352873954922 ; AI error: 152.32006819184232 BCD5 error: 16.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 87.75606036186218, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 30.327267003059386, 55.59173710979521 ; AI error: 243.9451890944848 BCD5 error: 154.3\n",
            "\tIntensity Truth: 55.0, AI forecast: 81.4993667602539, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 31.73771809339523, 55.94982079672627 ; AI error: 669.8939098303019 BCD5 error: 605.6\n",
            "Found AL142017 at 2017-09-27 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 96.92271709442139, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 28.873568511009218, 56.840600749198344 ; AI error: 152.83142007570962 BCD5 error: 43.2\n",
            "\tIntensity Truth: 75.0, AI forecast: 96.84108138084412, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 30.349613559246063, 58.019280704855916 ; AI error: 345.32763564531956 BCD5 error: 233.2\n",
            "\tIntensity Truth: 50.0, AI forecast: 88.56822848320007, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 31.735670065879823, 58.76217254959047 ; AI error: 964.6531832590344 BCD5 error: 781.8\n",
            "Found AL142017 at 2017-09-27 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 98.02613854408264, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 28.827510797977446, 58.48811081126332 ; AI error: 205.8728366706735 BCD5 error: 50.6\n",
            "\tIntensity Truth: 70.0, AI forecast: 98.34408640861511, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 30.072782123088835, 60.01069698557257 ; AI error: 533.139283084265 BCD5 error: 314.7\n",
            "Found AL142017 at 2017-09-27 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 99.47699785232544, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 28.868089830875398, 59.552867859229444 ; AI error: 276.06853313938944 BCD5 error: 49.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 96.80975556373596, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 30.277648711204527, 61.03158386573195 ; AI error: 713.3222374452575 BCD5 error: 378.5\n",
            "Found AL142017 at 2017-09-27 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 96.9677197933197, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 29.630450987815856, 59.59502319768071 ; AI error: 319.1343720929639 BCD5 error: 54.1\n",
            "\tIntensity Truth: 55.0, AI forecast: 91.06974601745605, BCD5 forecast: 27.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 31.146913599967956, 60.07369375377893 ; AI error: 824.289982364637 BCD5 error: 435.4\n",
            "Found AL142017 at 2017-09-28 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 89.54145669937134, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 30.42617013454437, 59.563958290219304 ; AI error: 376.8383710443987 BCD5 error: 94.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 86.02320194244385, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 32.4524609208107, 59.055542757734656 ; AI error: 942.333997130586 BCD5 error: 559.8\n",
            "Found AL142017 at 2017-09-28 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 86.65430784225464, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 31.89252253770828, 59.238580972328776 ; AI error: 421.32170351539867 BCD5 error: 143.8\n",
            "Found AL142017 at 2017-09-28 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 84.92752432823181, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 33.12878839969635, 58.27061724476516 ; AI error: 491.63738820679674 BCD5 error: 167.6\n",
            "Found AL142017 at 2017-09-28 18:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 73.19387793540955, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 35.080861282348636, 56.12698455494828 ; AI error: 516.1055081493549 BCD5 error: 169.9\n",
            "Found AL142017 at 2017-09-29 00:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 62.57941395044327, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 36.48968641757965, 54.10505318790674 ; AI error: 597.9028733060181 BCD5 error: 190.8\n",
            "Found AL142017 at 2017-09-29 06:00:00\n",
            "Found AL152017 at 2017-09-17 18:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 89.81069684028625, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 14.9, 60.4; AI forecast: 14.953586238622666, 60.91107100024819 ; AI error: 29.82354895366067 BCD5 error: 66.2\n",
            "\tIntensity Truth: 145.0, AI forecast: 101.5017020702362, BCD5 forecast: -61.0\n",
            "\tTrajectory Truth: 16.6, 63.5; AI forecast: 16.089997053146362, 65.70251322090625 ; AI error: 130.53654663376608 BCD5 error: 114.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 107.6116132736206, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 18.6, 67.0; AI forecast: 18.328312009572983, 70.51955493986607 ; AI error: 201.09692264552987 BCD5 error: 94.9\n",
            "\tIntensity Truth: 105.0, AI forecast: 105.26978731155396, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 20.5, 69.5; AI forecast: 20.07999033778906, 74.56485137939453 ; AI error: 286.32736253818763 BCD5 error: 115.8\n",
            "\tIntensity Truth: 110.0, AI forecast: 101.72451257705688, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 21.340582253038882, 77.18004203140735 ; AI error: 344.0457621714872 BCD5 error: 139.4\n",
            "Found AL152017 at 2017-09-18 00:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 89.28465843200684, BCD5 forecast: -57.0\n",
            "\tTrajectory Truth: 15.3, 61.1; AI forecast: 15.779335612058638, 60.90858206003904 ; AI error: 30.836112890865852 BCD5 error: 57.3\n",
            "\tIntensity Truth: 150.0, AI forecast: 103.19541573524475, BCD5 forecast: -57.0\n",
            "\tTrajectory Truth: 17.0, 64.3; AI forecast: 17.270027023553848, 65.74553190916777 ; AI error: 84.50753207556917 BCD5 error: 85.5\n",
            "\tIntensity Truth: 95.0, AI forecast: 102.34135746955872, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 19.0, 67.6; AI forecast: 19.701675176620483, 69.8631114795804 ; AI error: 134.94489692822623 BCD5 error: 70.5\n",
            "\tIntensity Truth: 110.0, AI forecast: 100.21600842475891, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 20.8, 70.0; AI forecast: 21.765580068528653, 72.63041194677353 ; AI error: 158.1641181240712 BCD5 error: 105.9\n",
            "\tIntensity Truth: 105.0, AI forecast: 96.88986420631409, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 23.612079554796217, 74.8213745892048 ; AI error: 177.23537271815795 BCD5 error: 98.5\n",
            "Found AL152017 at 2017-09-18 06:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 100.09520292282104, BCD5 forecast: -43.0\n",
            "\tTrajectory Truth: 15.7, 61.9; AI forecast: 16.445308798551558, 61.9322529412806 ; AI error: 44.78736850222224 BCD5 error: 40.4\n",
            "\tIntensity Truth: 140.0, AI forecast: 107.09800839424133, BCD5 forecast: -45.0\n",
            "\tTrajectory Truth: 17.6, 65.1; AI forecast: 18.172691583633423, 66.51065782010555 ; AI error: 87.63016144776103 BCD5 error: 45.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 105.61091661453247, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 19.4, 68.2; AI forecast: 20.790791819989682, 69.71204116046428 ; AI error: 119.33599897548132 BCD5 error: 37.6\n",
            "\tIntensity Truth: 110.0, AI forecast: 102.48249888420105, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 21.2, 70.5; AI forecast: 23.290829503536223, 72.30671490728855 ; AI error: 160.74108882761405 BCD5 error: 71.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 95.55484533309937, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 25.209488070011137, 73.94989195168019 ; AI error: 121.83326266833015 BCD5 error: 62.2\n",
            "Found AL152017 at 2017-09-18 12:00:00\n",
            "\tIntensity Truth: 140.0, AI forecast: 103.5883903503418, BCD5 forecast: -31.0\n",
            "\tTrajectory Truth: 16.1, 62.7; AI forecast: 16.678600311279297, 62.491323769837614 ; AI error: 36.760110476143225 BCD5 error: 5.8\n",
            "\tIntensity Truth: 115.0, AI forecast: 108.7773859500885, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 18.2, 66.2; AI forecast: 18.403646677732468, 66.7911038532853 ; AI error: 35.8447307020653 BCD5 error: 22.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 104.82045292854309, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 19.9, 68.8; AI forecast: 21.313392753899098, 69.97746198177337 ; AI error: 107.60935598811078 BCD5 error: 66.8\n",
            "\tIntensity Truth: 110.0, AI forecast: 98.53318452835083, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 21.9, 70.9; AI forecast: 23.787854313850403, 71.90143014490604 ; AI error: 126.16484852423574 BCD5 error: 98.0\n",
            "\tIntensity Truth: 100.0, AI forecast: 96.30285978317261, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 25.42143676280975, 73.58295662403107 ; AI error: 82.80313735543233 BCD5 error: 66.9\n",
            "Found AL152017 at 2017-09-18 18:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 111.07915759086609, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 16.6, 63.5; AI forecast: 16.612947022914884, 62.718440517783165 ; AI error: 44.974658721492986 BCD5 error: 18.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 114.97853755950928, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 18.6, 67.0; AI forecast: 18.3900006711483, 67.23065552711486 ; AI error: 18.20602286196612 BCD5 error: 57.8\n",
            "\tIntensity Truth: 105.0, AI forecast: 107.43964672088623, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 20.5, 69.5; AI forecast: 21.004945503175257, 69.98946112394333 ; AI error: 40.91843825924716 BCD5 error: 112.3\n",
            "\tIntensity Truth: 110.0, AI forecast: 104.72299575805664, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 23.056782841682434, 72.14424659013748 ; AI error: 54.44230064596304 BCD5 error: 139.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 96.42795443534851, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 24.32749904990196, 74.50457247793675 ; AI error: 152.56592480467293 BCD5 error: 118.3\n",
            "Found AL152017 at 2017-09-19 00:00:00\n",
            "\tIntensity Truth: 150.0, AI forecast: 119.83171939849854, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 17.0, 64.3; AI forecast: 17.205517536401747, 62.740960106253624 ; AI error: 90.31279014791596 BCD5 error: 24.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 116.42900466918945, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 19.0, 67.6; AI forecast: 18.871048595011235, 66.40436214059591 ; AI error: 68.3417096525173 BCD5 error: 64.1\n",
            "\tIntensity Truth: 110.0, AI forecast: 110.69755673408508, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 20.8, 70.0; AI forecast: 21.161222225427625, 68.75411247014999 ; AI error: 73.13377411796321 BCD5 error: 139.3\n",
            "\tIntensity Truth: 105.0, AI forecast: 102.94309854507446, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 22.86491938829422, 70.88430953025818 ; AI error: 63.81076285853187 BCD5 error: 163.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 99.6117889881134, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 23.029022568464278, 74.28834203481674 ; AI error: 237.80915986380802 BCD5 error: 176.0\n",
            "Found AL152017 at 2017-09-19 06:00:00\n",
            "\tIntensity Truth: 140.0, AI forecast: 132.60031938552856, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 17.6, 65.1; AI forecast: 17.704776626825332, 64.44714621305465 ; AI error: 37.87800910762512 BCD5 error: 13.3\n",
            "\tIntensity Truth: 100.0, AI forecast: 130.76604843139648, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 19.4, 68.2; AI forecast: 18.988227762281895, 69.28225193172693 ; AI error: 66.15943888882285 BCD5 error: 54.3\n",
            "\tIntensity Truth: 110.0, AI forecast: 124.32061553001404, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 21.2, 70.5; AI forecast: 19.85610401779413, 74.56660648286342 ; AI error: 242.46332462665416 BCD5 error: 127.9\n",
            "\tIntensity Truth: 100.0, AI forecast: 110.54961442947388, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 20.65061641857028, 78.80917146503926 ; AI error: 444.29728339269184 BCD5 error: 121.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 97.47113585472107, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 23.270395416021348, 78.72142646610737 ; AI error: 417.9007539031237 BCD5 error: 126.1\n",
            "Found AL152017 at 2017-09-19 12:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 130.24046063423157, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 18.2, 66.2; AI forecast: 18.275691521167754, 65.74957373440266 ; AI error: 26.084209849651835 BCD5 error: 12.9\n",
            "\tIntensity Truth: 100.0, AI forecast: 135.23812174797058, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 19.9, 68.8; AI forecast: 18.69783462136984, 73.03048227131367 ; AI error: 250.34604261695333 BCD5 error: 72.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 120.45115947723389, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 21.9, 70.9; AI forecast: 20.082677998393773, 77.89213583767415 ; AI error: 406.8016013380456 BCD5 error: 139.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 106.90983653068542, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 23.319911193847656, 77.99648394584656 ; AI error: 340.0766704342123 BCD5 error: 141.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 99.08091902732849, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 26.754200994968414, 77.89750628471374 ; AI error: 288.68732143340725 BCD5 error: 134.6\n",
            "Found AL152017 at 2017-09-19 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 139.02703523635864, BCD5 forecast: 40.0\n",
            "\tTrajectory Truth: 18.6, 67.0; AI forecast: 17.239381712675094, 69.59247028529644 ; AI error: 169.1335901053083 BCD5 error: 24.8\n",
            "\tIntensity Truth: 105.0, AI forecast: 131.96921348571777, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 20.5, 69.5; AI forecast: 17.910918444395065, 76.6073102235794 ; AI error: 431.8503172314742 BCD5 error: 72.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 116.23263001441956, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 21.18162749260664, 77.80297149419785 ; AI error: 380.1818697715142 BCD5 error: 126.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 108.17045450210571, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 24.83161798119545, 78.51656264960766 ; AI error: 343.2693475649089 BCD5 error: 134.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 98.21224093437195, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 28.02117074728012, 79.26646664142609 ; AI error: 341.87437788646514 BCD5 error: 140.4\n",
            "Found AL152017 at 2017-09-20 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 124.79466676712036, BCD5 forecast: 42.0\n",
            "\tTrajectory Truth: 19.0, 67.6; AI forecast: 18.254992699623106, 69.90586986988782 ; AI error: 138.6069693412693 BCD5 error: 30.5\n",
            "\tIntensity Truth: 110.0, AI forecast: 117.1030604839325, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 20.8, 70.0; AI forecast: 21.180446422100065, 72.80535591244697 ; AI error: 158.90571782211768 BCD5 error: 96.0\n",
            "\tIntensity Truth: 105.0, AI forecast: 110.98631501197815, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 25.284969186782835, 74.39850148558617 ; AI error: 180.08955690948153 BCD5 error: 121.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 103.78626704216003, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 29.18599747419357, 75.75424345433711 ; AI error: 236.1712248094788 BCD5 error: 152.3\n",
            "\tIntensity Truth: 85.0, AI forecast: 96.64016723632812, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 32.60145173072815, 76.02969464957714 ; AI error: 237.05480893857018 BCD5 error: 165.9\n",
            "Found AL152017 at 2017-09-20 06:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 127.86270260810852, BCD5 forecast: 30.0\n",
            "\tTrajectory Truth: 19.4, 68.2; AI forecast: 19.232720716297624, 70.86953444182873 ; AI error: 151.58891556148365 BCD5 error: 60.9\n",
            "\tIntensity Truth: 110.0, AI forecast: 126.41271114349365, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 21.2, 70.5; AI forecast: 21.45705236494541, 76.18296351134777 ; AI error: 318.1954850280545 BCD5 error: 143.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 115.89858531951904, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 24.423253536224365, 78.7389113664627 ; AI error: 373.8674144047482 BCD5 error: 145.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 104.26454901695251, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 28.1593213558197, 79.15134711563587 ; AI error: 350.0520356596931 BCD5 error: 156.1\n",
            "\tIntensity Truth: 75.0, AI forecast: 91.09576463699341, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 33.447047519683835, 75.58869734704494 ; AI error: 233.41925725364686 BCD5 error: 188.5\n",
            "Found AL152017 at 2017-09-20 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 114.83325719833374, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 19.9, 68.8; AI forecast: 20.54068500250578, 74.42746323645115 ; AI error: 319.36002727864917 BCD5 error: 62.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 119.8470151424408, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 21.9, 70.9; AI forecast: 23.990440809726714, 76.54828787446021 ; AI error: 336.5288422402051 BCD5 error: 107.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 106.06355547904968, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 29.90529147386551, 71.25062421262264 ; AI error: 292.0326750180483 BCD5 error: 105.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 96.4122998714447, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 37.00737099647522, 63.81021662056446 ; AI error: 687.2888711328139 BCD5 error: 76.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 67.13629394769669, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 46.18428564071655, 56.332889357919335 ; AI error: 1205.9196650121933 BCD5 error: 92.3\n",
            "Found AL152017 at 2017-09-20 18:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 95.13211369514465, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 20.5, 69.5; AI forecast: 21.56708446443081, 73.80630922615528 ; AI error: 249.6734463547429 BCD5 error: 8.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 90.05155682563782, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 26.648838353157043, 72.14237447977067 ; AI error: 236.72870333650198 BCD5 error: 28.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 83.6148190498352, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 33.73040981292725, 65.83664212822914 ; AI error: 577.942235797671 BCD5 error: 21.6\n",
            "\tIntensity Truth: 90.0, AI forecast: 54.44282293319702, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 42.49927206039429, 58.96812482699752 ; AI error: 1049.4417985450684 BCD5 error: 24.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 39.41922411322594, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 50.14379048347473, 51.38445627838373 ; AI error: 1485.9873793845802 BCD5 error: 25.6\n",
            "Found AL152017 at 2017-09-21 00:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 65.3460493683815, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 20.8, 70.0; AI forecast: 21.734860219061375, 72.62494653761387 ; AI error: 157.22623737989116 BCD5 error: 16.4\n",
            "\tIntensity Truth: 105.0, AI forecast: 68.47962409257889, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 26.813206267356872, 71.0928158223629 ; AI error: 188.93559389154052 BCD5 error: 13.2\n",
            "\tIntensity Truth: 100.0, AI forecast: 43.36912989616394, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 34.34142796993255, 65.04953248649835 ; AI error: 600.1848024635905 BCD5 error: 12.0\n",
            "\tIntensity Truth: 85.0, AI forecast: 35.80532193183899, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 41.7815582036972, 57.56211949288845 ; AI error: 1038.4748445777552 BCD5 error: 39.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 34.80173170566559, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 48.19728865623474, 51.3695175498724 ; AI error: 1384.1786612490325 BCD5 error: 62.3\n",
            "Found AL152017 at 2017-09-21 06:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 75.49361139535904, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 21.2, 70.5; AI forecast: 21.26788919866085, 73.67461881041527 ; AI error: 177.7090580313588 BCD5 error: 24.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 50.683338940143585, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 26.395098108053205, 72.29391876161098 ; AI error: 121.67677744525463 BCD5 error: 12.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 42.70813789218664, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 32.85206210613251, 66.1209544762969 ; AI error: 464.923368279271 BCD5 error: 18.8\n",
            "\tIntensity Truth: 75.0, AI forecast: 41.73898655921221, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 39.32239761352539, 60.650104495882985 ; AI error: 809.8132523035351 BCD5 error: 33.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 38.26531857252121, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 45.25533280372619, 55.020697662606835 ; AI error: 1131.143418419843 BCD5 error: 76.7\n",
            "Found AL152017 at 2017-09-21 12:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 42.60237492620945, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 21.9, 70.9; AI forecast: 21.514830285310744, 72.68736717402935 ; AI error: 102.35015204585305 BCD5 error: 30.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 42.86638740450144, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 25.409170752763746, 71.25713929235935 ; AI error: 49.38981770212925 BCD5 error: 20.2\n",
            "\tIntensity Truth: 95.0, AI forecast: 44.420808516442776, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 30.420442116260528, 67.6662026822567 ; AI error: 294.5984573787125 BCD5 error: 12.1\n",
            "\tIntensity Truth: 70.0, AI forecast: 42.58348993957043, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 35.79568972587585, 63.295562627911565 ; AI error: 571.5082626631834 BCD5 error: 52.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 36.93172037601471, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 41.65465710163116, 57.03852141452953 ; AI error: 912.6863446557934 BCD5 error: 111.0\n",
            "Found AL152017 at 2017-09-21 18:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 78.89469474554062, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 21.383168570697308, 71.49937579035759 ; AI error: 86.68220076403637 BCD5 error: 22.9\n",
            "\tIntensity Truth: 100.0, AI forecast: 87.61049628257751, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 24.693779045343398, 72.76864796578884 ; AI error: 76.75995322379654 BCD5 error: 44.2\n",
            "\tIntensity Truth: 90.0, AI forecast: 86.035076379776, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 28.853402161598204, 71.97252455353737 ; AI error: 50.915183003412146 BCD5 error: 37.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 79.71137940883636, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 33.95230424404144, 69.44558169096709 ; AI error: 239.95117781620695 BCD5 error: 30.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 75.5077263712883, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 39.43378567695618, 65.37204165160657 ; AI error: 498.54203486729256 BCD5 error: 85.4\n",
            "Found AL152017 at 2017-09-22 00:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 106.2014091014862, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 22.3050166785717, 71.44931718707085 ; AI error: 84.16842510027381 BCD5 error: 55.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 104.85750317573547, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 25.300013214349747, 73.9936474442482 ; AI error: 116.16230303550458 BCD5 error: 84.5\n",
            "\tIntensity Truth: 85.0, AI forecast: 92.375408411026, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 29.376542890071867, 73.60035164952278 ; AI error: 41.419134573896535 BCD5 error: 80.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 83.24423313140869, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 33.96072165966034, 72.08970102667809 ; AI error: 128.24673998363906 BCD5 error: 46.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 74.78014379739761, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 38.153782749176024, 69.28932576179504 ; AI error: 288.1319107925259 BCD5 error: 73.5\n",
            "Found AL152017 at 2017-09-22 06:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 109.48345184326172, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 22.778261613845824, 72.13145892322063 ; AI error: 98.19922130239655 BCD5 error: 36.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 104.48859453201294, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 26.11379510760307, 74.61739765107632 ; AI error: 136.42994484837328 BCD5 error: 42.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 93.45168471336365, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 30.240142095088956, 74.23133423924446 ; AI error: 69.12875237781432 BCD5 error: 40.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 83.21476817131042, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 34.15567705631256, 72.51352035999298 ; AI error: 97.92209700400058 BCD5 error: 108.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.40631192922592, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 38.93344511985779, 68.96445950120687 ; AI error: 307.0689367797428 BCD5 error: 178.0\n",
            "Found AL152017 at 2017-09-22 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 105.31304597854614, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 23.69153318405151, 72.23680582940578 ; AI error: 84.89520818091412 BCD5 error: 24.8\n",
            "\tIntensity Truth: 95.0, AI forecast: 103.62112641334534, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 27.27685055732727, 74.53557252287865 ; AI error: 114.1815448942841 BCD5 error: 69.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 92.37793684005737, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 30.888392746448517, 73.6049301803112 ; AI error: 31.631672544244513 BCD5 error: 173.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 80.31219005584717, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 35.644053077697755, 70.65010241866112 ; AI error: 185.75915044357782 BCD5 error: 275.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 69.93732184171677, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 41.46485068798065, 65.28048714548349 ; AI error: 507.21129718094176 BCD5 error: 366.8\n",
            "Found AL152017 at 2017-09-22 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 107.35487341880798, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 24.39888636469841, 72.75129363834859 ; AI error: 93.40508444167114 BCD5 error: 41.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 104.1330873966217, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 27.29924292564392, 75.03599067628383 ; AI error: 156.40235464758328 BCD5 error: 114.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 90.03199696540833, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 31.431925880908963, 73.39743116497993 ; AI error: 15.360109590433584 BCD5 error: 244.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 77.80540078878403, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 37.09185757637024, 69.09942102283239 ; AI error: 273.74942824225246 BCD5 error: 358.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 66.99651062488556, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 43.49203443527222, 61.40917487591505 ; AI error: 683.8987900708855 BCD5 error: 458.9\n",
            "Found AL152017 at 2017-09-23 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 104.7882091999054, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 24.72200307250023, 73.03166590631008 ; AI error: 117.8232302067531 BCD5 error: 31.9\n",
            "\tIntensity Truth: 85.0, AI forecast: 99.51353073120117, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 28.352797996997833, 74.37200874984265 ; AI error: 111.86654719773644 BCD5 error: 90.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 86.4425539970398, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 33.6166275024414, 70.84677744805813 ; AI error: 149.4913546421974 BCD5 error: 213.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 74.27500247955322, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 40.13120100498199, 63.46065596938133 ; AI error: 570.5458605696135 BCD5 error: 324.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 63.72162967920303, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 46.85107593536377, 52.90527823455631 ; AI error: 1056.625656633164 BCD5 error: 411.8\n",
            "Found AL152017 at 2017-09-23 06:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 96.95654630661011, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 26.638882100582123, 72.56820667088031 ; AI error: 51.72986407596307 BCD5 error: 21.3\n",
            "\tIntensity Truth: 75.0, AI forecast: 93.91234278678894, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 31.974327981472015, 71.63698438107967 ; AI error: 119.6579465769186 BCD5 error: 83.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 81.81031346321106, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 38.42599542140961, 64.58193307220935 ; AI error: 543.2979483838084 BCD5 error: 203.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.16376435756683, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 45.23614127635956, 54.17242817617952 ; AI error: 1056.6663462949864 BCD5 error: 313.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 61.520713567733765, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 51.26887631416321, 42.84881155341863 ; AI error: 1493.250211322757 BCD5 error: 381.1\n",
            "Found AL152017 at 2017-09-23 12:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 94.28191065788269, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 28.924441754817963, 71.68706502914429 ; AI error: 66.55255634083393 BCD5 error: 36.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 89.75231766700745, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 35.28924489021301, 68.48183487504721 ; AI error: 352.54729565166537 BCD5 error: 123.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 76.725212931633, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 41.950442743301394, 59.1177213255316 ; AI error: 841.83631345358 BCD5 error: 250.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 66.70442909002304, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 48.26923623085022, 47.6995851740241 ; AI error: 1353.0327724019473 BCD5 error: 375.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 59.869772493839264, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 53.594651508331296, 36.9040768533945 ; AI error: 1700.8371523313167 BCD5 error: 409.8\n",
            "Found AL152017 at 2017-09-23 18:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 91.88606977462769, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 30.3700945854187, 71.09432504177093 ; AI error: 121.14585496944882 BCD5 error: 47.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 85.61320424079895, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 36.888842248916625, 66.70400832742453 ; AI error: 457.6020885936788 BCD5 error: 150.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 74.15361285209656, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 43.22431585788726, 56.54732812629081 ; AI error: 954.8787884614567 BCD5 error: 284.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 65.93522131443024, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 49.21343474388122, 45.34849859178066 ; AI error: 1429.759609569384 BCD5 error: 408.1\n",
            "\tIntensity Truth: 55.0, AI forecast: 58.71613711118698, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 54.218459510803214, 34.95162005424499 ; AI error: 1733.4836840315259 BCD5 error: 420.2\n",
            "Found AL152017 at 2017-09-24 00:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 88.51996302604675, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 30.981821703910825, 71.25643894672393 ; AI error: 114.7835831247097 BCD5 error: 52.6\n",
            "\tIntensity Truth: 70.0, AI forecast: 82.834552526474, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 37.124068760871886, 66.61631420105695 ; AI error: 444.09402999877 BCD5 error: 175.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.81362682580948, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 43.222502899169925, 56.855609957408156 ; AI error: 919.9283218726471 BCD5 error: 321.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 63.790623247623444, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 49.04021382331848, 45.85714453458786 ; AI error: 1368.8561857990207 BCD5 error: 437.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 56.878986209630966, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 53.7873658657074, 35.49289565980434 ; AI error: 1643.072573780858 BCD5 error: 428.5\n",
            "Found AL152017 at 2017-09-24 06:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 88.87668013572693, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 31.208737444877624, 71.68690223693848 ; AI error: 83.03286776823741 BCD5 error: 72.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 84.95567917823792, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 37.146593308448786, 67.49121717065573 ; AI error: 388.23920694353876 BCD5 error: 211.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.39067190885544, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 43.06964509487152, 57.81598800271749 ; AI error: 855.8909069495717 BCD5 error: 368.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 63.91527146100998, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 48.64111957550048, 46.73163882791996 ; AI error: 1291.0682741759397 BCD5 error: 476.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 55.835566967725754, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 53.74506850242615, 35.873951515555376 ; AI error: 1555.742788891113 BCD5 error: 453.5\n",
            "Found AL152017 at 2017-09-24 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 91.81041717529297, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 32.255975699424745, 71.59873329997063 ; AI error: 113.06624707022661 BCD5 error: 77.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 85.04598498344421, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 38.29638764858245, 67.05251930803061 ; AI error: 420.1815043307467 BCD5 error: 202.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.46613264083862, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 43.89109563827515, 56.892132855113594 ; AI error: 892.374583357028 BCD5 error: 361.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 61.77407279610634, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 49.64376835823059, 45.13513312190771 ; AI error: 1323.3521726748857 BCD5 error: 432.0\n",
            "\tIntensity Truth: 50.0, AI forecast: 55.10711759328842, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 54.315785503387445, 34.16659194827079 ; AI error: 1545.3694684206425 BCD5 error: 399.7\n",
            "Found AL152017 at 2017-09-24 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 89.9176836013794, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 33.44384217262268, 71.38528897762299 ; AI error: 150.36273490329924 BCD5 error: 70.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 84.10625219345093, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 39.45591669082641, 66.04037148654461 ; AI error: 475.9639931913776 BCD5 error: 185.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 71.16673916578293, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 45.38114793300629, 54.89473740532994 ; AI error: 979.8248650040388 BCD5 error: 322.9\n",
            "\tIntensity Truth: 55.0, AI forecast: 62.43047133088112, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 50.70946063995361, 42.9659532636404 ; AI error: 1378.601996636619 BCD5 error: 354.7\n",
            "\tIntensity Truth: 50.0, AI forecast: 54.86674681305885, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 55.08216085433959, 32.52457479834556 ; AI error: 1511.0784141287959 BCD5 error: 312.0\n",
            "Found AL152017 at 2017-09-25 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 87.80924916267395, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 34.043314170837405, 71.40544299185277 ; AI error: 149.4194007831695 BCD5 error: 72.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 80.74667096138, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 40.52668285369873, 65.0968151435256 ; AI error: 526.0355420103732 BCD5 error: 187.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.38518130779266, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 46.14410314559936, 53.48317212387919 ; AI error: 1015.5231032620488 BCD5 error: 313.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 60.76012164354324, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 51.0897433757782, 41.88376324772834 ; AI error: 1361.126690405362 BCD5 error: 301.8\n",
            "\tIntensity Truth: 50.0, AI forecast: 53.7021554261446, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 55.35296311378478, 31.62496811151504 ; AI error: 1416.590646849954 BCD5 error: 302.8\n",
            "Found AL152017 at 2017-09-25 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 81.1765205860138, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 35.06276774406433, 71.04497017562389 ; AI error: 179.90283223506367 BCD5 error: 58.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 79.54828321933746, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 41.357868099212645, 63.90059342682361 ; AI error: 574.7218064187692 BCD5 error: 167.1\n",
            "\tIntensity Truth: 60.0, AI forecast: 69.39837634563446, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 46.442542886734, 52.222115443646906 ; AI error: 1033.2270833372277 BCD5 error: 255.5\n",
            "\tIntensity Truth: 50.0, AI forecast: 60.64291059970856, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 51.24348888397216, 40.6161037325859 ; AI error: 1330.8759247930598 BCD5 error: 217.1\n",
            "\tIntensity Truth: 50.0, AI forecast: 52.5733495503664, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 55.49518799781799, 30.67856375277042 ; AI error: 1307.006938616091 BCD5 error: 364.1\n",
            "Found AL152017 at 2017-09-25 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 81.20223045349121, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 35.07298760414123, 71.40162585377693 ; AI error: 135.81438207819286 BCD5 error: 45.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 77.86133497953415, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 40.60329275131225, 64.53440453112125 ; AI error: 500.12646802884836 BCD5 error: 150.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.54855298995972, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 45.55340304374695, 52.88263739980757 ; AI error: 932.1085965906445 BCD5 error: 197.7\n",
            "\tIntensity Truth: 50.0, AI forecast: 58.81915807723999, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 50.3977819442749, 41.56270310282707 ; AI error: 1190.2310930596313 BCD5 error: 212.5\n",
            "\tIntensity Truth: 50.0, AI forecast: 52.86311857402325, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 54.590033435821525, 32.03013077974319 ; AI error: 1104.7440375019457 BCD5 error: 482.5\n",
            "Found AL152017 at 2017-09-25 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 75.30402094125748, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 34.373694729804996, 71.4801340907812 ; AI error: 85.37512065159693 BCD5 error: 56.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.08977842330933, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 39.43536131381988, 64.66631709039211 ; AI error: 429.3253457279776 BCD5 error: 175.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 63.15892696380615, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 44.03295402526855, 53.50831779651344 ; AI error: 813.2292077225187 BCD5 error: 208.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 57.03731968998909, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 48.64595313072205, 42.732930634915824 ; AI error: 978.8921061202586 BCD5 error: 255.8\n",
            "Found AL152017 at 2017-09-26 00:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.84367382526398, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 34.41839084625244, 71.15963354408741 ; AI error: 91.16755461997418 BCD5 error: 66.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 67.83575713634491, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 38.990557074546814, 64.44632546901703 ; AI error: 394.70866384105403 BCD5 error: 179.1\n",
            "\tIntensity Truth: 55.0, AI forecast: 61.76250711083412, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 43.056739592552184, 53.80391028784215 ; AI error: 701.7521880273864 BCD5 error: 192.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 56.901566833257675, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 47.49850497245788, 43.260284115374084 ; AI error: 781.6913046946775 BCD5 error: 311.7\n",
            "Found AL152017 at 2017-09-26 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 66.16608649492264, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 34.536052918434144, 71.0768520116806 ; AI error: 92.58838688257435 BCD5 error: 70.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 66.88866019248962, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 38.589589786529544, 64.82008450329303 ; AI error: 328.1932039258194 BCD5 error: 170.0\n",
            "\tIntensity Truth: 50.0, AI forecast: 63.18872153759003, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 42.40038695335388, 54.566963565722105 ; AI error: 565.0046970780965 BCD5 error: 195.6\n",
            "\tIntensity Truth: 50.0, AI forecast: 57.39168360829353, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 46.607067012786864, 44.08559142202139 ; AI error: 574.4256483941273 BCD5 error: 386.2\n",
            "Found AL152017 at 2017-09-26 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.97345411777496, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 34.97471232414246, 71.13442788422108 ; AI error: 85.62328473916185 BCD5 error: 76.0\n",
            "\tIntensity Truth: 60.0, AI forecast: 72.03342080116272, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 39.052082014083865, 65.15497350692749 ; AI error: 266.27990693975585 BCD5 error: 155.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 65.2049371600151, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 42.658500003814694, 54.92722080945968 ; AI error: 470.5860831162959 BCD5 error: 230.2\n",
            "\tIntensity Truth: 50.0, AI forecast: 56.830033361911774, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 46.716217041015625, 44.40168640464544 ; AI error: 446.439514446688 BCD5 error: 489.8\n",
            "Found AL152017 at 2017-09-26 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.57716858386993, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 35.88964364528656, 70.6237877368927 ; AI error: 96.28576426943533 BCD5 error: 62.9\n",
            "\tIntensity Truth: 55.0, AI forecast: 72.50245064496994, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 39.94796750545501, 64.22683748602867 ; AI error: 279.3867518965166 BCD5 error: 131.6\n",
            "\tIntensity Truth: 50.0, AI forecast: 63.647028505802155, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 43.3748379945755, 53.77866265811026 ; AI error: 430.7253546455791 BCD5 error: 277.6\n",
            "Found AL152017 at 2017-09-27 00:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 71.60088837146759, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 36.415153837203974, 70.13427496552467 ; AI error: 100.30414870393766 BCD5 error: 48.5\n",
            "\tIntensity Truth: 55.0, AI forecast: 68.15044224262238, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 40.25346531867981, 63.46865991950035 ; AI error: 254.88590336646203 BCD5 error: 136.1\n",
            "\tIntensity Truth: 50.0, AI forecast: 62.588588893413544, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 43.611150479316706, 52.79522709995508 ; AI error: 354.5010621308769 BCD5 error: 379.2\n",
            "Found AL152017 at 2017-09-27 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 67.00137972831726, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 36.54940891265869, 69.98509964942932 ; AI error: 64.00126136241924 BCD5 error: 33.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 67.80503004789352, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 40.34543805122375, 63.27381587848067 ; AI error: 210.2538159527819 BCD5 error: 198.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 61.37384667992592, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 43.16044774055481, 52.81158432513475 ; AI error: 245.3529494700162 BCD5 error: 487.7\n",
            "Found AL152017 at 2017-09-27 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 70.98082035779953, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 37.22989027500152, 69.61849329769612 ; AI error: 31.6340217418809 BCD5 error: 71.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 69.31638807058334, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 40.63557302951813, 62.86958544999361 ; AI error: 222.0360223241103 BCD5 error: 285.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 61.96524769067764, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 43.32338271141052, 52.23284192383289 ; AI error: 251.9915357413211 BCD5 error: 577.1\n",
            "Found AL152017 at 2017-09-27 18:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 69.67369556427002, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 37.01210391521454, 69.53869798481465 ; AI error: 46.83105091405583 BCD5 error: 125.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 68.56014162302017, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 40.0915652513504, 62.97075232714414 ; AI error: 246.30288962651596 BCD5 error: 356.4\n",
            "Found AL152017 at 2017-09-28 00:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 68.62835079431534, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 37.236774110794066, 69.03022500872612 ; AI error: 108.73771157366816 BCD5 error: 122.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 66.99601829051971, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 40.22222294807434, 62.35480427592992 ; AI error: 339.17888960067836 BCD5 error: 398.4\n",
            "Found AL152017 at 2017-09-28 06:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 65.93865931034088, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 37.460175085067746, 68.55437660813331 ; AI error: 191.03229452936114 BCD5 error: 91.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 65.57976633310318, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 40.35111725330353, 61.86738533452153 ; AI error: 452.45584607443925 BCD5 error: 427.9\n",
            "Found AL152017 at 2017-09-28 12:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 64.12383764982224, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 37.53477225303649, 67.50740906000138 ; AI error: 265.06330461487136 BCD5 error: 111.0\n",
            "\tIntensity Truth: 50.0, AI forecast: 63.33384335041046, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 40.38754315376282, 60.57125117331743 ; AI error: 539.9656147442485 BCD5 error: 469.4\n",
            "Found AL152017 at 2017-09-28 18:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 60.31935513019562, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 37.518706464767455, 65.89282324463129 ; AI error: 328.51048391356255 BCD5 error: 143.3\n",
            "Found AL152017 at 2017-09-29 00:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 57.68855080008507, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 37.081443953514096, 63.867534738779064 ; AI error: 397.91699599742503 BCD5 error: 151.8\n",
            "Found AL152017 at 2017-09-29 06:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 50.44926628470421, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 35.46126117706299, 61.345657511800525 ; AI error: 488.1593658800164 BCD5 error: 162.4\n",
            "Found AL152017 at 2017-09-29 12:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 52.519061863422394, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 35.87186102867126, 57.665301135554905 ; AI error: 487.1561847772638 BCD5 error: 146.9\n",
            "Found AL152017 at 2017-09-29 18:00:00\n",
            "Found AL152017 at 2017-09-30 00:00:00\n",
            "Found AL152017 at 2017-09-30 06:00:00\n",
            "Found AL152017 at 2017-09-30 12:00:00\n",
            "Found AL162017 at 2017-10-04 18:00:00\n",
            "Found AL162017 at 2017-10-05 00:00:00\n",
            "Found AL162017 at 2017-10-05 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 47.27183625102043, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 16.3, 84.7; AI forecast: 14.799071311950684, 82.30415224432946 ; AI error: 165.30166132349822 BCD5 error: 54.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 53.85469824075699, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 23.5, 86.5; AI forecast: 15.277038276195526, 85.48185949325561 ; AI error: 497.0586368589359 BCD5 error: 374.1\n",
            "\tIntensity Truth: 60.0, AI forecast: 64.16936188936234, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 30.5, 88.9; AI forecast: 16.88178123831749, 85.6082133769989 ; AI error: 837.2887441623957 BCD5 error: 684.3\n",
            "Found AL162017 at 2017-10-05 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 49.80317234992981, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 17.9, 84.7; AI forecast: 15.1654842376709, 82.90751442313194 ; AI error: 193.90077842416142 BCD5 error: 98.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 62.69024133682251, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 25.7, 87.9; AI forecast: 15.704627293348313, 86.09475519061088 ; AI error: 608.6006424465056 BCD5 error: 455.2\n",
            "\tIntensity Truth: 35.0, AI forecast: 67.4500361084938, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 32.2, 88.0; AI forecast: 17.326224613189698, 85.90054069757461 ; AI error: 900.266850194348 BCD5 error: 727.0\n",
            "Found AL162017 at 2017-10-05 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 57.26841226220131, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 19.5, 85.2; AI forecast: 15.766142094135283, 83.23854219913483 ; AI error: 250.69568703315477 BCD5 error: 156.9\n",
            "\tIntensity Truth: 80.0, AI forecast: 65.18536269664764, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 27.6, 88.9; AI forecast: 16.633139657974244, 86.06856260299682 ; AI error: 676.9441315228253 BCD5 error: 538.5\n",
            "Found AL162017 at 2017-10-06 00:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 51.235617101192474, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 21.3, 85.9; AI forecast: 16.71272020936012, 83.21136268377305 ; AI error: 314.8571836482006 BCD5 error: 182.2\n",
            "\tIntensity Truth: 75.0, AI forecast: 64.81392562389374, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 29.1, 89.2; AI forecast: 18.37164840698242, 85.53235220909119 ; AI error: 674.7998510888704 BCD5 error: 548.2\n",
            "Found AL162017 at 2017-10-06 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 61.44388437271118, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 23.5, 86.5; AI forecast: 18.220302778482438, 83.06334217488765 ; AI error: 370.97716619440183 BCD5 error: 216.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.70437890291214, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 30.5, 88.9; AI forecast: 20.924894203990696, 84.35858988165856 ; AI error: 624.9897047026157 BCD5 error: 516.8\n",
            "Found AL162017 at 2017-10-06 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 55.10680049657822, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 25.7, 87.9; AI forecast: 20.14312583655119, 82.37078512907028 ; AI error: 452.417425328267 BCD5 error: 275.0\n",
            "\tIntensity Truth: 35.0, AI forecast: 62.860246896743774, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 32.2, 88.0; AI forecast: 23.07057454586029, 82.65947675704956 ; AI error: 617.1079875494413 BCD5 error: 480.5\n",
            "Found AL162017 at 2017-10-06 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 58.165999352931976, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 27.6, 88.9; AI forecast: 21.349523597955702, 82.15788514316083 ; AI error: 525.6697196718893 BCD5 error: 273.0\n",
            "Found AL162017 at 2017-10-07 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 64.76377636194229, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 29.1, 89.2; AI forecast: 24.285090535879135, 80.74576280713082 ; AI error: 537.5390430303679 BCD5 error: 249.7\n",
            "Found AL162017 at 2017-10-07 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 74.84834045171738, BCD5 forecast: 30.0\n",
            "\tTrajectory Truth: 30.5, 88.9; AI forecast: 27.32905685901642, 79.04885246455669 ; AI error: 551.3724809838103 BCD5 error: 73.4\n",
            "Found AL162017 at 2017-10-07 12:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 74.087675511837, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 32.2, 88.0; AI forecast: 30.815241289138793, 76.85336894690991 ; AI error: 576.3346424200397 BCD5 error: 66.0\n",
            "Found AL162017 at 2017-10-07 18:00:00\n",
            "Found AL162017 at 2017-10-08 00:00:00\n",
            "Found AL162017 at 2017-10-08 06:00:00\n",
            "Found AL162017 at 2017-10-08 12:00:00\n",
            "Found AL162017 at 2017-10-08 18:00:00\n",
            "Found AL172017 at 2017-10-09 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 46.3100566342473, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.9, 38.8; AI forecast: 28.91464171409607, 42.38178615123033 ; AI error: 257.89300757174783 BCD5 error: 59.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 48.15097697079182, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 30.4, 37.2; AI forecast: 29.27964948415756, 41.6107352823019 ; AI error: 239.338184430209 BCD5 error: 272.3\n",
            "\tIntensity Truth: 75.0, AI forecast: 50.2330407500267, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 30.2, 35.7; AI forecast: 28.941532683372497, 42.378387016057964 ; AI error: 356.7763858466172 BCD5 error: 411.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 50.14522008597851, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 30.9, 34.4; AI forecast: 28.92443950176239, 42.61758387833834 ; AI error: 443.6791989315622 BCD5 error: 504.4\n",
            "\tIntensity Truth: 95.0, AI forecast: 47.31183059513569, BCD5 forecast: -43.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 29.09183176755905, 42.63343746513128 ; AI error: 711.841898248605 BCD5 error: 494.6\n",
            "Found AL172017 at 2017-10-09 12:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 39.30174447596073, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.6, 38.5; AI forecast: 28.65862851142883, 42.196539643406865 ; AI error: 260.79844997266724 BCD5 error: 97.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 41.95690006017685, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 30.0, 36.7; AI forecast: 28.578747928142548, 42.195957152545446 ; AI error: 300.1404936403457 BCD5 error: 318.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 44.87533209845424, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 30.4, 35.7; AI forecast: 27.699052894115447, 43.124746885895725 ; AI error: 421.97636647372735 BCD5 error: 420.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 44.83891148120165, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 31.4, 33.4; AI forecast: 27.004976594448088, 43.3217610657215 ; AI error: 582.7582857409009 BCD5 error: 492.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 43.2806408777833, BCD5 forecast: -48.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 26.67195996046066, 42.78695560246706 ; AI error: 900.8644122325387 BCD5 error: 481.2\n",
            "Found AL172017 at 2017-10-09 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 42.25574415177107, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 31.3, 38.2; AI forecast: 29.220231151580812, 42.061912184953684 ; AI error: 235.98949886645124 BCD5 error: 135.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 44.62829824537039, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 29.8, 36.2; AI forecast: 29.07725900411606, 41.89343751370906 ; AI error: 300.81252715211735 BCD5 error: 349.8\n",
            "\tIntensity Truth: 85.0, AI forecast: 45.68427136167884, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 30.5, 35.6; AI forecast: 27.699673235416412, 42.646189521253106 ; AI error: 405.98121074534436 BCD5 error: 431.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 45.55169768631458, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 32.0, 32.5; AI forecast: 26.542308628559113, 42.294137810170646 ; AI error: 608.246627727013 BCD5 error: 468.0\n",
            "\tIntensity Truth: 100.0, AI forecast: 47.544664070010185, BCD5 forecast: -47.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 26.020876091718673, 40.6020340770483 ; AI error: 969.1827804838445 BCD5 error: 453.2\n",
            "Found AL172017 at 2017-10-10 00:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 43.94871944561601, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 30.9, 37.8; AI forecast: 30.409027385711667, 41.055766478180885 ; AI error: 170.71853352547453 BCD5 error: 121.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 45.60133205726743, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 29.9, 35.8; AI forecast: 30.19019410610199, 40.34706078469753 ; AI error: 236.94949359090646 BCD5 error: 282.2\n",
            "\tIntensity Truth: 90.0, AI forecast: 46.83977438136935, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 30.5, 35.1; AI forecast: 28.53093658685684, 40.494557306170464 ; AI error: 305.59964370813253 BCD5 error: 366.0\n",
            "\tIntensity Truth: 85.0, AI forecast: 49.916771203279495, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 32.6, 31.5; AI forecast: 27.337031173706052, 39.04488253891468 ; AI error: 503.56890229756084 BCD5 error: 392.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 47.672937251627445, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 26.734715819358826, 37.04327605962753 ; AI error: 937.6083624095227 BCD5 error: 496.8\n",
            "Found AL172017 at 2017-10-10 06:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 47.278710678219795, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 30.4, 37.2; AI forecast: 31.39209110736847, 40.47985683083534 ; AI error: 179.1619531915351 BCD5 error: 91.4\n",
            "\tIntensity Truth: 75.0, AI forecast: 48.868355602025986, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 30.2, 35.7; AI forecast: 31.261986911296844, 39.64667275547981 ; AI error: 213.41639688234295 BCD5 error: 192.2\n",
            "\tIntensity Truth: 90.0, AI forecast: 52.27427676320076, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 30.9, 34.4; AI forecast: 29.918807554244992, 39.05565904378891 ; AI error: 248.14488735226456 BCD5 error: 283.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 50.52044354379177, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 28.900113260746004, 37.52289750874042 ; AI error: 484.07839899514823 BCD5 error: 386.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 50.77104277908802, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 28.623008453845976, 34.512201496958724 ; AI error: 923.1237725059816 BCD5 error: 685.7\n",
            "Found AL172017 at 2017-10-10 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 48.97328305989504, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 30.0, 36.7; AI forecast: 31.79897116422653, 40.13516977131366 ; AI error: 207.31413903964804 BCD5 error: 88.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 54.34929087758064, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 30.4, 35.7; AI forecast: 32.19033592939377, 38.794755706191054 ; AI error: 191.7247915862031 BCD5 error: 143.5\n",
            "\tIntensity Truth: 80.0, AI forecast: 53.50297383964062, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 31.4, 33.4; AI forecast: 31.31246061325073, 38.28061552643776 ; AI error: 250.27103246011026 BCD5 error: 262.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 54.25268694758415, BCD5 forecast: -41.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 30.90619113445282, 35.968654185533524 ; AI error: 462.6242784571039 BCD5 error: 450.9\n",
            "\tIntensity Truth: 85.0, AI forecast: 50.185611844062805, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 31.39917621612549, 32.882314062118525 ; AI error: 925.644698744386 BCD5 error: 833.1\n",
            "Found AL172017 at 2017-10-10 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 53.93942981958389, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 29.8, 36.2; AI forecast: 32.01599223613739, 39.02040096521377 ; AI error: 196.98985262032033 BCD5 error: 78.5\n",
            "\tIntensity Truth: 85.0, AI forecast: 55.16302779316902, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 30.5, 35.6; AI forecast: 32.69866882562637, 37.68360563516616 ; AI error: 169.63637925830503 BCD5 error: 106.1\n",
            "\tIntensity Truth: 80.0, AI forecast: 57.25300386548042, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 32.0, 32.5; AI forecast: 32.352743685245514, 36.54065342843532 ; AI error: 206.41868774348876 BCD5 error: 255.7\n",
            "\tIntensity Truth: 100.0, AI forecast: 53.793821930885315, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 32.7085289478302, 34.214169666171074 ; AI error: 474.6560290473726 BCD5 error: 538.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 50.25435507297516, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 33.65975098609924, 31.94359654784202 ; AI error: 1002.1190224453705 BCD5 error: 977.9\n",
            "Found AL172017 at 2017-10-11 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 51.64267539978027, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 29.9, 35.8; AI forecast: 31.445199382305145, 39.15507420897484 ; AI error: 196.51840221851768 BCD5 error: 36.4\n",
            "\tIntensity Truth: 90.0, AI forecast: 55.755460411310196, BCD5 forecast: -35.0\n",
            "\tTrajectory Truth: 30.5, 35.1; AI forecast: 32.11563662290573, 37.59777344465255 ; AI error: 160.69881321216386 BCD5 error: 61.6\n",
            "\tIntensity Truth: 85.0, AI forecast: 52.93210484087467, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 32.6, 31.5; AI forecast: 32.21261864900589, 36.52037562429905 ; AI error: 255.52068017340937 BCD5 error: 243.8\n",
            "\tIntensity Truth: 95.0, AI forecast: 50.09204588830471, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 32.822306752204895, 34.93489145934581 ; AI error: 645.6836311879567 BCD5 error: 636.9\n",
            "Found AL172017 at 2017-10-11 06:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 58.57835233211517, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 30.2, 35.7; AI forecast: 30.559967839717864, 38.8497862547636 ; AI error: 164.56698485648084 BCD5 error: 42.3\n",
            "\tIntensity Truth: 90.0, AI forecast: 57.148720771074295, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 30.9, 34.4; AI forecast: 31.609535002708434, 37.23653075695037 ; AI error: 151.68880974120842 BCD5 error: 56.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 53.89886647462845, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 32.08920377492905, 36.73908356130123 ; AI error: 363.9968636549375 BCD5 error: 299.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 52.63405114412308, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 33.03278720378876, 35.43131272494793 ; AI error: 817.0416270278921 BCD5 error: 746.9\n",
            "Found AL172017 at 2017-10-11 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 56.20971500873566, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 30.4, 35.7; AI forecast: 29.849520838260652, 38.183560848236084 ; AI error: 133.1381038947293 BCD5 error: 52.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 55.512043833732605, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 31.4, 33.4; AI forecast: 30.863055634498593, 36.94917368590831 ; AI error: 185.22281745424232 BCD5 error: 77.9\n",
            "\tIntensity Truth: 100.0, AI forecast: 54.6901760995388, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 31.3739675283432, 36.62674506902694 ; AI error: 481.2487451381449 BCD5 error: 364.4\n",
            "\tIntensity Truth: 85.0, AI forecast: 50.92812091112137, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 32.457457447052, 34.807654088735575 ; AI error: 969.3553531296714 BCD5 error: 851.3\n",
            "Found AL172017 at 2017-10-11 18:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 60.51342353224754, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 30.5, 35.6; AI forecast: 29.46426260471344, 37.48248095214366 ; AI error: 115.97771393505357 BCD5 error: 43.1\n",
            "\tIntensity Truth: 80.0, AI forecast: 61.94632098078728, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 32.0, 32.5; AI forecast: 30.477451181411745, 36.091570779681206 ; AI error: 205.77540737470318 BCD5 error: 71.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 57.11818665266037, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 31.069092309474946, 35.094709315896026 ; AI error: 557.9347286261267 BCD5 error: 416.1\n",
            "\tIntensity Truth: 80.0, AI forecast: 52.11996279656887, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 32.19019623994827, 33.818031790852544 ; AI error: 1131.2254499008395 BCD5 error: 919.3\n",
            "Found AL172017 at 2017-10-12 00:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 68.62479388713837, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 30.5, 35.1; AI forecast: 29.17596011161804, 36.82616043388843 ; AI error: 120.00361182212839 BCD5 error: 50.4\n",
            "\tIntensity Truth: 85.0, AI forecast: 65.71473270654678, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 32.6, 31.5; AI forecast: 30.07090232372284, 34.57243630886077 ; AI error: 218.8010784556215 BCD5 error: 127.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 58.60421344637871, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 30.48803378343582, 34.03960894048214 ; AI error: 673.3875644616743 BCD5 error: 514.4\n",
            "Found AL172017 at 2017-10-12 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 70.85904061794281, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 30.9, 34.4; AI forecast: 28.87268981933594, 35.964157050848 ; AI error: 146.4386518646734 BCD5 error: 71.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 67.06629484891891, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 29.401502990722655, 34.210769683122635 ; AI error: 333.1934640517486 BCD5 error: 238.2\n",
            "\tIntensity Truth: 90.0, AI forecast: 58.43514248728752, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 29.42425960302353, 34.29392936825752 ; AI error: 883.4513000905314 BCD5 error: 679.1\n",
            "Found AL172017 at 2017-10-12 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 73.98000240325928, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 31.4, 33.4; AI forecast: 29.030137097835542, 35.8997405141592 ; AI error: 192.51275828191174 BCD5 error: 103.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 68.64862620830536, BCD5 forecast: -31.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 29.444612205028534, 34.5236020386219 ; AI error: 450.0113993179077 BCD5 error: 348.5\n",
            "\tIntensity Truth: 85.0, AI forecast: 58.98616477847099, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 29.221759474277498, 35.38056395053863 ; AI error: 1108.5564843907728 BCD5 error: 847.0\n",
            "Found AL172017 at 2017-10-12 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 76.5432557463646, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 32.0, 32.5; AI forecast: 29.59412722587585, 35.82727424204349 ; AI error: 224.2797393841317 BCD5 error: 106.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 70.11496484279633, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 30.121605086326596, 35.13588217794894 ; AI error: 589.9566802816414 BCD5 error: 429.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 59.58121031522751, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 29.834364533424377, 35.902207833528514 ; AI error: 1305.7027544863247 BCD5 error: 975.9\n",
            "Found AL172017 at 2017-10-13 00:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 78.95699173212051, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 32.6, 31.5; AI forecast: 30.40527980327606, 36.29577494859695 ; AI error: 278.5860984219071 BCD5 error: 79.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 70.91475784778595, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 31.07927612066269, 35.55062415003776 ; AI error: 720.3384823561452 BCD5 error: 430.8\n",
            "Found AL172017 at 2017-10-13 06:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 79.84504818916321, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 30.654955160617828, 36.56137043237686 ; AI error: 386.0729992278327 BCD5 error: 75.1\n",
            "\tIntensity Truth: 90.0, AI forecast: 71.85111939907074, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 31.089597368240355, 35.659213334321976 ; AI error: 883.2061574908994 BCD5 error: 417.9\n",
            "Found AL172017 at 2017-10-13 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 77.70119071006775, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 30.624645555019377, 36.20855563282966 ; AI error: 481.50366429317324 BCD5 error: 99.2\n",
            "\tIntensity Truth: 85.0, AI forecast: 71.81697934865952, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 31.002781879901885, 35.08451445400715 ; AI error: 1030.3104746367667 BCD5 error: 469.9\n",
            "Found AL172017 at 2017-10-13 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 75.3415134549141, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 30.778021562099454, 35.115304225683204 ; AI error: 567.7073323151908 BCD5 error: 178.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 69.35590833425522, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 31.134136521816252, 33.75318283736705 ; AI error: 1171.2876195100307 BCD5 error: 580.2\n",
            "Found AL172017 at 2017-10-14 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 72.74726182222366, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 31.460324895381927, 33.68325341045856 ; AI error: 626.1520035696151 BCD5 error: 175.9\n",
            "Found AL172017 at 2017-10-14 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 68.3923265337944, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 32.6909761428833, 31.98986175358295 ; AI error: 673.4815501852567 BCD5 error: 136.4\n",
            "Found AL172017 at 2017-10-14 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 63.636689484119415, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 33.24605243206024, 30.312069320678706 ; AI error: 754.8075369283993 BCD5 error: 122.4\n",
            "Found AL172017 at 2017-10-14 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 69.38977718353271, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 34.50687885284424, 28.4554699987173 ; AI error: 837.596432897159 BCD5 error: 155.3\n",
            "Found AL172017 at 2017-10-15 00:00:00\n",
            "Found AL172017 at 2017-10-15 06:00:00\n",
            "Found AL172017 at 2017-10-15 12:00:00\n",
            "Found AL172017 at 2017-10-15 18:00:00\n",
            "Found AL192017 at 2017-11-05 18:00:00\n",
            "Found AL192017 at 2017-11-06 00:00:00\n",
            "Found AL192017 at 2017-11-06 06:00:00\n",
            "Found AL192017 at 2017-11-06 12:00:00\n",
            "Found AL192017 at 2017-11-06 18:00:00\n",
            "Found AL192017 at 2017-11-07 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 40.56955814361572, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 34.6, 48.7; AI forecast: 28.02756642103195, 50.17483091279864 ; AI error: 401.7815281260897 BCD5 error: 108.5\n",
            "\tIntensity Truth: 45.0, AI forecast: 42.412456162273884, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 41.8, 48.8; AI forecast: 27.555660176277158, 49.23057973533869 ; AI error: 855.4978643551539 BCD5 error: 416.2\n",
            "Found AL192017 at 2017-11-07 06:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 43.6477230861783, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 36.4, 48.7; AI forecast: 28.377333021163942, 49.751007340103385 ; AI error: 484.61260056906673 BCD5 error: 133.5\n",
            "Found AL192017 at 2017-11-07 12:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 45.197215573862195, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 38.3, 48.8; AI forecast: 29.361884510517122, 49.54400687590241 ; AI error: 537.9240012442095 BCD5 error: 178.4\n",
            "Found AL192017 at 2017-11-07 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 47.20358170568943, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 40.1, 49.0; AI forecast: 30.673031425476076, 48.8337881475687 ; AI error: 566.0578476949846 BCD5 error: 198.7\n",
            "Found AL192017 at 2017-11-08 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 54.59008410573006, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 41.8, 48.8; AI forecast: 32.39729861021042, 47.95118222385645 ; AI error: 565.9952688303031 BCD5 error: 181.9\n",
            "Found AL192017 at 2017-11-08 06:00:00\n",
            "Found AL192017 at 2017-11-08 12:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btLAyw8CTA7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "a4f528f6-03bb-42da-a40f-74f13454ca5a"
      },
      "source": [
        "pd.DataFrame(ai_wind_errors).describe()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>997.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>20.176159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>16.140353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.018525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.775216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15.990373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>29.126221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>85.252114</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  997.000000\n",
              "mean    20.176159\n",
              "std     16.140353\n",
              "min      0.018525\n",
              "25%      7.775216\n",
              "50%     15.990373\n",
              "75%     29.126221\n",
              "max     85.252114"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRxsLuy4TA7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "605f0567-accb-4cae-cf96-0db3313ee416"
      },
      "source": [
        "pd.DataFrame(bcd5_wind_errors).describe()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>997.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>17.656971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>15.179471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>26.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>79.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  997.000000\n",
              "mean    17.656971\n",
              "std     15.179471\n",
              "min      0.000000\n",
              "25%      6.000000\n",
              "50%     13.000000\n",
              "75%     26.000000\n",
              "max     79.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aam7310tTA7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "62f962c8-bd49-4541-e2c6-31a6c5b2fb5b"
      },
      "source": [
        "pd.DataFrame(ai_track_errors).describe()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>997.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>433.329269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>373.859492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>15.360110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>179.161953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>321.229931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>561.164143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2070.891886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   997.000000\n",
              "mean    433.329269\n",
              "std     373.859492\n",
              "min      15.360110\n",
              "25%     179.161953\n",
              "50%     321.229931\n",
              "75%     561.164143\n",
              "max    2070.891886"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbC4Y2CfTA7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "7e1a5a14-13d7-4e12-81ad-76d829bcb9ea"
      },
      "source": [
        "pd.DataFrame(bcd5_track_errors).describe()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>997.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>249.861785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>225.954794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>72.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>176.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>366.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1164.700000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   997.000000\n",
              "mean    249.861785\n",
              "std     225.954794\n",
              "min       5.800000\n",
              "25%      72.800000\n",
              "50%     176.900000\n",
              "75%     366.100000\n",
              "max    1164.700000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    }
  ]
}