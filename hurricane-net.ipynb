{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Copy of hurricane-net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hammad93/hurricane-net/blob/master/hurricane_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibY3ir9MTEao",
        "colab_type": "code",
        "outputId": "fbc12ba1-a12d-46b9-8009-a94b22f87e56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!rm -rf sample_data\n",
        "!git init\n",
        "!git remote add origin https://github.com/hammad93/hurricane-net.git\n",
        "!git pull origin master"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n",
            "fatal: remote origin already exists.\n",
            "From https://github.com/hammad93/hurricane-net\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa7Z-ReQTA5t",
        "colab_type": "text"
      },
      "source": [
        "# hurricane-net\n",
        "Hammad Usmani\n",
        "### A machine learning algorithm to forecast the intensity and trajectory of Atlantic tropical storms\n",
        "[https://github.com/hammad93/hurricane-net](https://github.com/hammad93/hurricane-net)\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "1. [Background](#Background)\n",
        "2. [Problem](#Problem)\n",
        "3. [Datasets](#Datasets)\n",
        "4. [Workflow Diagram](#Workflow)\n",
        "5. [Data Extraction](#Extract)\n",
        "6. [Data Transformation](#Transform)\n",
        "7. [Data Loading](#Load)\n",
        "8. [Feature Engineering](#FeatureEngineering)\n",
        "9. [Model Architecture](#ModelArchitecture)\n",
        "11. [Model Selection](#ModelSelection)\n",
        "12. [Paramater Optimization](#Optimization)\n",
        "13. [Model Evaluation & Benchmarks](#Benchmarks)\n",
        "14. [Visualizations](#Visualizations)\n",
        "\n",
        "![Hurricane Maria 2017](img/hurricane-maria.png \"Hurricane Maria. Source: NOAA\")\n",
        "\n",
        "## Background<a id=\"Background\"></a>\n",
        "\n",
        "The National Hurricane Center (NHC) and National Oceanic and Atmospheric Administration (NOAA) provide predictions for storms trajectories, intensity, and size. They create these predictions based on models that can be classified into 3 groups: dynamical, statistical, and ensemble [1]. The most accurate models are based on computational fluid dynamics and achieve more precision than their statistical and ensemble counterparts [1][4]. The current statistical models (OCD5) are based on multiple regression methods that can explain a significant amount of variance [1]. In this project, we research and implement the domain of machine learning and deep learning into predictive hurricane models for both trajectory and intensity and evaluate them against the NHC standards. \n",
        "Previous research into machine learning to forecast tropical Atlantic storms include a sparse recurrent neural network (Kordmahalleh, Sefidmazgi, & Homaifar, 2016) and an artificial neural network (Jung & Das, 2013); both achieved favorable results. The hurricane models created can be utilized to develop more precise emergency planning. There is a necessity for more accurate and timely models that can help reduce the amount of loss caused by hurricanes. \n",
        "\n",
        "## Problem<a id=\"Problem\"></a>\n",
        "\n",
        "The NOAA and NHC have several different classifications for Atlantic hurricane models that describe feature prediction and model architecture. The 3 main classifications for hurricane model architecture include dynamical, statistical, and ensemble. Classifications also include relative compute time required to create an output grouped as either early or late and forecast parameters such as trajectory, intensity, and wind radii. The most accurate models are late models that take upwards of 6 hours to produce an output whereas models that can produce an output in seconds to minutes are called early. Early models tend to be statistical which include the baseline model for trajectory named CLIPER5 Climatology and Persistence (CLP5) utilizing multivariate regression. The performance for these methods can be augmented by incorporating more advanced statistical methods from deep learning such as recurrent neural networks. Kordmahalleh et al., 2016 created a sparse recurrent neural network augmented by a genetic algorithm but there are factors requiring improvement. The training set utilized an older version of the NHC Hurricane Database format known as HURDAT while a new format has been released called HURDAT2 with additional information on wind radii. Kordmahalleh et al., 2016 also utilized benchmarks different from the standard applied within the NHC. Other than improving their methodology, we can expand the scope by creating separate models for both intensity and trajectory. These models can be used to predict the trajectory and intensity for future Atlantic storms.\n",
        "\n",
        "## Datasets<a id=\"Datasets\"></a>\n",
        "\n",
        "The following datasets and inputs including their sources will be used to create machine learning models:\n",
        "- NHC Hurricane Database (HURDAT2)\n",
        "    - http://www.nhc.noaa.gov/data/#hurdat\n",
        "    - https://www.kaggle.com/noaa/hurricane-database\n",
        "- NHC Forecast Error Database\n",
        "    - http://www.nhc.noaa.gov/verification/verify7.shtml\n",
        "- NHC GIS\n",
        "    - http://www.nhc.noaa.gov/gis/\n",
        "\n",
        "*In the future, the IBTrACS database will be used to extend the hurricane-ai to additional regions.*\n",
        "\n",
        "The NHC HURDAT2 database contains the tracking information for Atlantic tropical and subtropical cyclones which includes hurricanes and tropical storms from 1851 to 2016. The most updated version of the dataset is included on the noaa.gov site and includes 2 additional years of cyclone data compared to the data set available on Kaggle and is potentially more descriptive. To match the inputs of the baseline model used by the NHC, we are calculating the forward motion of the storm by applying a vector based on previous and current geographical location.\n",
        "\n",
        "*Table 1. This table contains the tentative features as input to the model*\n",
        "\n",
        "| **Name**         | **Data Type** | **Description**                                                     |\n",
        "|------------------|---------------|---------------------------------------------------------------------|\n",
        "| Time             | Date Time     | The date and time of the measurement.                               |\n",
        "| Latitude         | Float         | The geographical latitude of the storm eye to 1 decimal precision.  |\n",
        "| Longitude        | Float         | The geographical longitude of the storm eye to 1 decimal precision. |\n",
        "| Maximum Winds    | Integer       | The maximum sustained winds within the storm.                       |\n",
        "| Minimum Pressure | Integer       | The minimum barometric pressure within the storm.                   |\n",
        "| Forward Motion   | String        | Calculated vector of motion based on location in time series.       |\n",
        "\n",
        "The Forecast Error Database contains information on the accuracy of predicted models from the NHC. The two model forecast errors available are labeled OFCL and BCD5. The OFCL is the official NHC forecast and the BCD5 is the real track available. This data set can be used to benchmark and evaluate the deep learning model. \n",
        "The NOAA and NHC also hosts a geographical information system (GIS) that contains raw and processed data on hurricanes. The server hosting the GIS is publicly accessible and can be used to evaluate our model by comparing the 2017 Atlantic tropical season. The preliminary best tracks can be found here before they are finalized and available in the HURDAT2 data set. With the GIS, we can construct a final evaluation data set.\n",
        "\n",
        "*Diagram 1. This graphic describes the workflow for the deep learning models*.<a id=\"Workflow\"></a>\n",
        "![Data Pipeline](img/Deep Learning Workflow.png \"hurricane-net Data Pipeline\")\n",
        "\n",
        "## Extract Data<a id=\"Extract\"></a>\n",
        "\n",
        "*The following code uses the hurdat2 and models modules created to provide a class interface for the HURDAT2 and error forecast database located in the data and models folder. *\n",
        "\n",
        "We will begin our steps to perform extraction, transformation, and loading of our data for analysis or broadly known as ETL. Although we're dividing these steps into disctinct procedures, they are often more fluid and often have overlaps. The extraction phase consists of collecting and parsing the HURDAT2 and error forecast databases for analysis and benchmarking. The HURDAT2 database is our core foundation for creating the deep learning model. We store the database in its raw .txt format but it can be directly linked to the database hosted by the NHC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F02MSMmiTA5x",
        "colab_type": "code",
        "outputId": "8cdb9dbf-2f1c-4a93-dae9-ff43c54629c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# Import various libraries throughout the software\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import datetime\n",
        "import dateutil\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Import from hurdat2 class in data folder and models class from hurricane-models folder\n",
        "from data.hurdat2 import hurdat2\n",
        "from errors.models import models\n",
        "\n",
        "# Initialize Dataframe for hurricanes and error database\n",
        "dataset = hurdat2(\"data/hurdat2.txt\") # Note that this data includes up to and including 2016\n",
        "errors = models(\"errors/1970-present_OFCL_v_BCD5_ind_ATL_TI_errors_noTDs.txt\")\n",
        "\n",
        "# Show the first 5 records from Hurricane Katrina 2005 (AL122005)\n",
        "dataset.hurricanes.query('storm_id == \"AL122005\"').head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>storm_id</th>\n",
              "      <th>storm_name</th>\n",
              "      <th>entry_time</th>\n",
              "      <th>entry_id</th>\n",
              "      <th>entry_status</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>max_wind</th>\n",
              "      <th>min_pressure</th>\n",
              "      <th>34kt_ne</th>\n",
              "      <th>34kt_se</th>\n",
              "      <th>34kt_sw</th>\n",
              "      <th>34kt_nw</th>\n",
              "      <th>50kt_ne</th>\n",
              "      <th>50kt_se</th>\n",
              "      <th>50kt_sw</th>\n",
              "      <th>50kt_nw</th>\n",
              "      <th>64kt_ne</th>\n",
              "      <th>64kt_se</th>\n",
              "      <th>64kt_sw</th>\n",
              "      <th>64kt_nw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14663</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-23 18:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TD</td>\n",
              "      <td>23.1N</td>\n",
              "      <td>75.1W</td>\n",
              "      <td>30</td>\n",
              "      <td>1008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14664</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-24 00:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TD</td>\n",
              "      <td>23.4N</td>\n",
              "      <td>75.7W</td>\n",
              "      <td>30</td>\n",
              "      <td>1007</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14665</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-24 06:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TD</td>\n",
              "      <td>23.8N</td>\n",
              "      <td>76.2W</td>\n",
              "      <td>30</td>\n",
              "      <td>1007</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14666</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-24 12:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TS</td>\n",
              "      <td>24.5N</td>\n",
              "      <td>76.5W</td>\n",
              "      <td>35</td>\n",
              "      <td>1006</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14667</th>\n",
              "      <td>AL122005</td>\n",
              "      <td>KATRINA</td>\n",
              "      <td>2005-08-24 18:00:00</td>\n",
              "      <td></td>\n",
              "      <td>TS</td>\n",
              "      <td>25.4N</td>\n",
              "      <td>76.9W</td>\n",
              "      <td>40</td>\n",
              "      <td>1003</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       storm_id storm_name          entry_time  ... 64kt_se 64kt_sw 64kt_nw\n",
              "14663  AL122005    KATRINA 2005-08-23 18:00:00  ...       0       0       0\n",
              "14664  AL122005    KATRINA 2005-08-24 00:00:00  ...       0       0       0\n",
              "14665  AL122005    KATRINA 2005-08-24 06:00:00  ...       0       0       0\n",
              "14666  AL122005    KATRINA 2005-08-24 12:00:00  ...       0       0       0\n",
              "14667  AL122005    KATRINA 2005-08-24 18:00:00  ...       0       0       0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk5uZFS0TA51",
        "colab_type": "code",
        "outputId": "5a7df954-2e03-47b8-a47f-6dfc3d125818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "# Show the first 3 OFCL hurricane model errors for Hurricane Katrina 2005 on 28-08-2005/18:00:00\n",
        "pprint(errors.models['OFCL'].storm['AL122005'][datetime.datetime(2005, 8, 28, 18, 0)], indent = 8)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{       'intensity_forecast': {       datetime.datetime(2005, 8, 28, 18, 0): 0.0,\n",
            "                                      datetime.datetime(2005, 8, 29, 6, 0): 20.9,\n",
            "                                      datetime.datetime(2005, 8, 29, 18, 0): 93.6,\n",
            "                                      datetime.datetime(2005, 8, 30, 6, 0): 170.2,\n",
            "                                      datetime.datetime(2005, 8, 30, 18, 0): None,\n",
            "                                      datetime.datetime(2005, 8, 31, 18, 0): None,\n",
            "                                      datetime.datetime(2005, 9, 1, 18, 0): None,\n",
            "                                      datetime.datetime(2005, 9, 2, 18, 0): None,\n",
            "                                      datetime.datetime(2005, 9, 3, 18, 0): None,\n",
            "                                      datetime.datetime(2005, 9, 4, 18, 0): None},\n",
            "        'lat': 26.3,\n",
            "        'long': 88.6,\n",
            "        'sample_sizes': {       'F012': 0.33,\n",
            "                                'F024': 0.33,\n",
            "                                'F036': 0.33,\n",
            "                                'F048': 0.0,\n",
            "                                'F072': 0.0,\n",
            "                                'F096': 0.0,\n",
            "                                'F120': 0.0,\n",
            "                                'F144': 0.0,\n",
            "                                'F168': 0.0},\n",
            "        'track_forecast': {       datetime.datetime(2005, 8, 28, 18, 0): 0.0,\n",
            "                                  datetime.datetime(2005, 8, 29, 6, 0): 28.0,\n",
            "                                  datetime.datetime(2005, 8, 29, 18, 0): 32.0,\n",
            "                                  datetime.datetime(2005, 8, 30, 6, 0): 17.0,\n",
            "                                  datetime.datetime(2005, 8, 30, 18, 0): None,\n",
            "                                  datetime.datetime(2005, 8, 31, 18, 0): None,\n",
            "                                  datetime.datetime(2005, 9, 1, 18, 0): None,\n",
            "                                  datetime.datetime(2005, 9, 2, 18, 0): None,\n",
            "                                  datetime.datetime(2005, 9, 3, 18, 0): None,\n",
            "                                  datetime.datetime(2005, 9, 4, 18, 0): None},\n",
            "        'wind_speed': 150}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vtb4-c_TA55",
        "colab_type": "text"
      },
      "source": [
        "## Transform Data<a id=\"Transform\"></a>\n",
        "\n",
        "The following code will tranform the hurricane best path data into objects that can be better manipulated for processing. to match between datasets, we will also create a `storm_id` dictionary to store storm names matched with ID's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TXygtaGkTA56",
        "colab_type": "code",
        "outputId": "911d0ff7-1c18-4782-8f5a-aeb6058aa033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Create hurricane class\n",
        "class hurricane(object) : \n",
        "    def __init__(self, name, id) :\n",
        "        # Set instance variables\n",
        "        self.name = name\n",
        "        self.id = id\n",
        "        self.entries = dict()\n",
        "        self.models = dict()\n",
        "        \n",
        "        return\n",
        "    # Add hurricane track entry based on standard HURDAT2 format\n",
        "    def add_entry(self, array) :\n",
        "        entry = {\n",
        "            array[0] : { # dateteime of entry\n",
        "                'entry_time' : array[0], \n",
        "                'entry_id' : array[1],\n",
        "                'entry_status' : array[2],\n",
        "                'lat' : float(array[3][:-1]), # Convert to number from format '#.#N'\n",
        "                'long' : float(array[4][:-1]), # Convert to number from format '#.#W'\n",
        "                'max_wind' : float(array[5]),\n",
        "                'min_pressure' : None if array[6] is None else float(array[6]), # Early records are -999 or None\n",
        "                'wind_radii' :  array[7:], # Array based on HURDAT2 format\n",
        "            }\n",
        "        }\n",
        "        self.entries.update(entry)\n",
        "        \n",
        "        return\n",
        "    # Add hurricane model errors\n",
        "    def add_model(self, name, model) :\n",
        "        self.models[name] = model\n",
        "        \n",
        "        return\n",
        "# Storm ID Key for matching between datasets\n",
        "storm_ids = dict()\n",
        "\n",
        "# Parse in hurricanes\n",
        "hurricanes = dict()\n",
        "print(\"Transforming HURDAT2 into objects . . .\")\n",
        "for index, entry in dataset.hurricanes.iterrows() :\n",
        "    print(\"Transforming {}/{} entries from HURDAT2\".format(index + 1, len(dataset.hurricanes)), end = \"\\r\")\n",
        "    # New hurricane\n",
        "    if entry['storm_id'] not in hurricanes :\n",
        "        hurricanes[entry['storm_id']] = hurricane(entry['storm_name'], entry['storm_id'])\n",
        "        storm_ids[entry['storm_id']] = entry['storm_name']\n",
        "    # Add entry to hurricane\n",
        "    hurricanes[entry['storm_id']].add_entry(entry[2:])\n",
        "print(\"\\nDone!\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transforming HURDAT2 into objects . . .\n",
            "Transforming 20291/20291 entries from HURDAT2\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC2DpnqwTA58",
        "colab_type": "text"
      },
      "source": [
        "## Load Data<a id=\"Load\"></a>\n",
        "\n",
        "The following will finalize our preliminary data preparation by loading some of the errors into each hurricane object. Note that models start from the year 1970 and any hurricane before that has no previous model data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKKXK1yXTA59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all available model errors\n",
        "models = errors.models.keys()\n",
        "# Load model errors into hurricanes\n",
        "for id in storm_ids :\n",
        "    for model in models :\n",
        "        # Skip if this hurricane does not have the model\n",
        "        if id not in errors.models[model].storm :\n",
        "            continue\n",
        "        hurricanes[id].add_model(model, errors.models[model].storm[id])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7baivH-TA6D",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering & Data Augmentation<a id=\"FeatureEngineering\"></a>\n",
        "\n",
        "The following section will extract the relevant features and engineer each data point so that we can fit it into the model. Because the type of inputs are important, the features will be transformed based on the model architecture. This will also include data augmentation methods. The higher level architecture will be a deep learning recurrent neural network with LSTM and time distributed layers.\n",
        "\n",
        "The current statistical baseline model using multivariate regression uses multiple predictors as input. According to Knaff 2013, the following predictors were calculated for their intensity model that were not included in the HURDAT2 database. These features can be calculated from the data loaded into our current object model.\n",
        "\n",
        "1. Date Information\n",
        "2. Zonal Speed Of The Storm (U) (kt)\n",
        "3. Meridional Speed Of The Storm (V) (kt)\n",
        "4. 12-h Change In Intensity (DVMX) (kt)\n",
        "\n",
        "The shape on the input to the LSTM will be in a 3D array with the format [samples, timestamps, features]. We will intitially begin with 1 time step and evaluate more can benefit our model. The output requires a 5 day forecast and observations without track data 5 days in the future will not be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GsKxUTh7TA6E",
        "colab_type": "code",
        "outputId": "04a9600c-a1ac-4543-ce5e-de3894a66c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "def feature_extraction(timestep, previous) :\n",
        "    '''\n",
        "    PURPOSE: Calculate the features for a machine learning model within the context of hurricane-net\n",
        "    METHOD: Use the predictors and the calculation methodology defined in Knaff 2013\n",
        "    INPUT:  timestep - current dictionary of features in the hurricane object format\n",
        "            previous - previous timestep dictionary of features in the hurricane object format\n",
        "    OUTPUT: Dictionary of features\n",
        "    \n",
        "    timestep = {\n",
        "      'lat' : float,\n",
        "      'long' : float,\n",
        "      'max-wind' : float,\n",
        "      'entry-time' : datetime\n",
        "    }\n",
        "    '''\n",
        "    features = {\n",
        "        'lat' : timestep['lat'],\n",
        "        'long' : timestep['long'],\n",
        "        'max_wind' : timestep['max_wind'],\n",
        "        'delta_wind' : (timestep['max_wind'] - previous['max_wind']) / # Calculated from track (12h)\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 43200),\n",
        "        'min_pressure' : timestep['min_pressure'], \n",
        "        'zonal_speed' : (timestep['lat'] - previous['lat'])/ # Calculated from track (per hour)\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 3600),\n",
        "        'meridonal_speed' : (timestep['long'] - previous['long'])/# Calculated from track (per hour)\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 3600),\n",
        "        'year' : timestep['entry_time'].year,\n",
        "        'month' : timestep['entry_time'].month,\n",
        "        'day' : timestep['entry_time'].day,\n",
        "        'hour' : timestep['entry_time'].hour,\n",
        "    }\n",
        "    return features\n",
        "    \n",
        "def storm_x_y(storm, timesteps = 1, lag = 24) :\n",
        "    '''\n",
        "    PURPOSE: Create independent and dependent samples for a machine learning model based on the timesteps\n",
        "    METHOD: Use the HURDAT2 database and a hurricane object as defined in hurricane-net for feature extraction\n",
        "    INPUT:  storm - hurricane object\n",
        "            timesteps - (default = 1) number of timesteps to calculate\n",
        "            include_none - (default = False) Boolean for including None in test data. Imputing function unavailable.\n",
        "            lag - (default = 24) lag in hours for the dependent variables up to 5 days\n",
        "    OUTPUT: Dictionary with independent (x) and dependent (y) values.\n",
        "    '''\n",
        "    x = []\n",
        "    # Create testing data structure with a dictionary\n",
        "    times = [time * lag for time in range(1, (120 // lag) + 1)] # Begin at lag hours with lag increments up to 120h inclusive\n",
        "    y = dict([(time,[]) for time in times])\n",
        "    \n",
        "    # Sort by entry time\n",
        "    entries = [entry[1] for entry in sorted(storm.entries.items())]\n",
        "    \n",
        "    for index in range(len(entries)) :\n",
        "        if index < timesteps : # Flag for insufficient initial time steps\n",
        "            continue\n",
        "\n",
        "        # If we're not including None values, check to see if there will be any\n",
        "        if None in [storm.entries.get(entries[index]['entry_time'] +\n",
        "                                         datetime.timedelta(hours = future)) for future in times] : break\n",
        "            \n",
        "        # Calculate time steps and their features for independent values\n",
        "        sample = []\n",
        "        for step in range(timesteps) :\n",
        "            # Training sample\n",
        "            timestep = entries[index - step]\n",
        "            previous = entries[index - step - 1]\n",
        "            sample.append([timestep['entry_time']] + [[feature_extraction(timestep, previous)]])\n",
        "        x.append(sample) # Add our constructed sample\n",
        "        \n",
        "        # Calculate time steps and their features for dependent values\n",
        "        for future in times :\n",
        "            timestep = storm.entries.get(entries[index]['entry_time'] + datetime.timedelta(hours = future))\n",
        "            previous = storm.entries.get(entries[index]['entry_time'] + datetime.timedelta(hours = future - lag))\n",
        "            \n",
        "            if timestep and previous: \n",
        "                y[future].append(feature_extraction(timestep, previous))\n",
        "            else :\n",
        "                y[future].append(None)\n",
        "    \n",
        "    # Return output, if there is no output, return None.\n",
        "    if len(x) is 0 :\n",
        "        return None\n",
        "    else:\n",
        "        return {'x': x, 'y': y}\n",
        "def shape(hurricanes, timesteps, remove_missing = True) :\n",
        "    '''\n",
        "    PURPOSE: Shape our data for input into machine learning models\n",
        "    METHOD: Use a numpy array to shape into (samples, timesteps, features)\n",
        "    INPUT:  hurricanes - dictionary of hurricane objects\n",
        "            timesteps - number of timesteps for the shape\n",
        "            remove_missing - boolean indicating whether the algorithm will disregard missing values\n",
        "    OUTPUT: numpy array of shape (samples, timesteps, 11) where 11 is the number of predictors in a hurricane object\n",
        "    '''\n",
        "    x = []\n",
        "    y = []\n",
        "    lag = 24 # lag time in hours\n",
        "    precision = np.float64 # defines the precision of our data type\n",
        "    times = [time * lag for time in range(1, (120 // lag) + 1)] # Begin at lag hours with lag increments up to 120h inclusive\n",
        "    count = 0\n",
        "    for hurricane in hurricanes.values() :\n",
        "        count += 1\n",
        "        result = storm_x_y(hurricane, timesteps, lag)\n",
        "        if result is None :\n",
        "            continue\n",
        "        # Extract only the values from the strom features using our specified precision\n",
        "        hurricane_x = np.array(\n",
        "            [[list(sample[1][0].values()) for sample in x] for x in result['x']],\n",
        "            dtype = precision)\n",
        "        hurricane_y = np.array(\n",
        "            [[list(result['y'][time][index].values()) for time in times] for index in range(len(result['y'][lag]))],\n",
        "            dtype = precision)\n",
        "        # Disregard if algorithm requires no missing values\n",
        "        if remove_missing :\n",
        "            if (len(np.where(np.isnan(hurricane_x))[0]) > 0) or (len(np.where(np.isnan(hurricane_y))[0]) > 0) :\n",
        "                continue\n",
        "        # Add to our results\n",
        "        x.extend(hurricane_x)\n",
        "        y.extend(hurricane_y)\n",
        "        print(\"Feature engineered {}/{} hurricanes for {} timestep(s)\".format(count, len(hurricanes), timesteps), end = \"\\r\")\n",
        "    print(\"\\nDone feature engineering hurricanes.\")\n",
        "    \n",
        "    return {'x': np.array(x), 'y': np.array(y)}\n",
        "def scaler(processed_data, hurricanes) :\n",
        "    '''\n",
        "    PURPOSE: Scale our data using the RobustScaler method from the sklearn library\n",
        "    METHOD: Generate data using 1 timesteps and then remove the NaN or None types to use the scaler methods\n",
        "    INPUT:  hurricanes - dictionary of hurricane objects\n",
        "            processed_data - dictionary of x and y values of data produced by shape() function with no missing values\n",
        "    OUTPUT: 1) Scaled processed_data using RobustScaler\n",
        "            2) RobustScaler object fit with appropriate data\n",
        "    '''\n",
        "    print(\"Scaling Data . . . (1 timestep for unqiue data)\")\n",
        "    # Create our scaler\n",
        "    unqiue_data = shape(hurricanes, timesteps = 1)\n",
        "    x = np.reshape(unqiue_data['x'], (unqiue_data['x'].shape[0], -1))\n",
        "    x = np.delete(x, np.where(np.isnan(x))[0], 0)\n",
        "    scaler = RobustScaler()\n",
        "    scaler.fit(x)\n",
        "    \n",
        "    # Scale our data\n",
        "    for index in range(len(processed_data['x'])) :\n",
        "        # Scale our x\n",
        "        processed_data['x'][index] = scaler.transform(processed_data['x'][index])\n",
        "        # Scale our y\n",
        "        processed_data['y'][index] = scaler.transform(processed_data['y'][index])\n",
        "    print(\"Done scaling.\")\n",
        "    return processed_data, scaler\n",
        "# Finalize and scale procesed data into a dictionary\n",
        "preprocessed_data = shape(hurricanes, timesteps = 5)\n",
        "processed_data, scaler = scaler(preprocessed_data, hurricanes)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature engineered 764/764 hurricanes for 5 timestep(s)\n",
            "Done feature engineering hurricanes.\n",
            "Scaling Data . . . (1 timestep for unqiue data)\n",
            "Feature engineered 764/764 hurricanes for 1 timestep(s)\n",
            "Done feature engineering hurricanes.\n",
            "Done scaling.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk1fyYBXTA6G",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture<a id=\"ModelArchitecture\"></a>\n",
        "\n",
        "Following feature engineering, we are now ready to input our data into a machine learning algorithm. The scope of this project will attempt a deep learning approach to forecasting Atlantic tropical cyclones. We will experiment with nunermous different architectures but we will focus around a Recurrent Neural Network utilizing LSTM cells.\n",
        "\n",
        "Notes:\n",
        "- We will use 500 epochs for wind intensity because the validation loss is not decresing\n",
        "- We will use 1,000 epochs for latitute and longitude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0ETjdPZJTA6H",
        "colab_type": "code",
        "outputId": "29a25cab-dfb4-40ca-b759-041d981e03a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import RepeatVector\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Create our cross validation data structure\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(processed_data['x'], processed_data['y'],\n",
        "                                                                    test_size = 0.2)\n",
        "\n",
        "# Train for wind intensity\n",
        "y_train_wind = np.array([[[features[2]] for features in y] for y in y_train], dtype = np.float64)\n",
        "y_test_wind = np.array([[[features[2]] for features in y] for y in y_test], dtype = np.float64)\n",
        "\n",
        "# Train for latitude and longitude location\n",
        "y_train_lat = np.array([[[features[0]] for features in y] for y in y_train], dtype = np.float64)\n",
        "y_test_lat = np.array([[[features[0]] for features in y] for y in y_test], dtype = np.float64)\n",
        "y_train_long = np.array([[[features[1]] for features in y] for y in y_train], dtype = np.float64)\n",
        "y_test_long = np.array([[[features[1]] for features in y] for y in y_test], dtype = np.float64)\n",
        "\n",
        "\n",
        "def bd_lstm_td(X_train, y_train, X_test, y_test, n_epochs = 500) :    \n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units = 512, return_sequences = True, dropout = 0.05),\n",
        "                            input_shape = (X_train.shape[1],X_train.shape[2])))\n",
        "    model.add(LSTM(units = 256, return_sequences = True, dropout = 0.05))\n",
        "    model.add(TimeDistributed(Dense(1)))\n",
        "    model.compile(loss='mse', optimizer='adadelta')\n",
        "    print(model.summary())\n",
        "    history = model.fit(X_train, y_train, batch_size = len(X_train), epochs = n_epochs,\n",
        "                        validation_data = (X_test, y_test))\n",
        "    return model, history\n",
        "\n",
        "def lstm_td(X_train, X_test, y_train, y_test) :\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units = 1024, input_shape = (5,8), return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(8)))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    print(model.summary())\n",
        "    model.fit(X_train, y_train, batch_size = len(X_train), epochs = 300)\n",
        "    \n",
        "    return model\n",
        "\n",
        "model_wind, model_wind_history = bd_lstm_td(X_train, y_train_wind, X_test, y_test_wind, n_epochs = 500)\n",
        "model_lat, model_lat_history = bd_lstm_td(X_train, y_train_lat, X_test, y_test_lat, n_epochs = 1000)\n",
        "model_long, model_long_history = bd_lstm_td(X_train, y_train_long, X_test, y_test_long, n_epochs = 1000)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_4 (Bidirection (None, 5, 1024)           2146304   \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 5, 256)            1311744   \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 5, 1)              257       \n",
            "=================================================================\n",
            "Total params: 3,458,305\n",
            "Trainable params: 3,458,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 3298 samples, validate on 825 samples\n",
            "Epoch 1/500\n",
            "3298/3298 [==============================] - 4s 1ms/step - loss: 0.8942 - val_loss: 0.7212\n",
            "Epoch 2/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.6967 - val_loss: 0.6160\n",
            "Epoch 3/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.6077 - val_loss: 0.5693\n",
            "Epoch 4/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.5704 - val_loss: 0.5460\n",
            "Epoch 5/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.5452 - val_loss: 0.5309\n",
            "Epoch 6/500\n",
            "3298/3298 [==============================] - 0s 65us/step - loss: 0.5312 - val_loss: 0.5186\n",
            "Epoch 7/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.5200 - val_loss: 0.5083\n",
            "Epoch 8/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.5098 - val_loss: 0.4989\n",
            "Epoch 9/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.5025 - val_loss: 0.4905\n",
            "Epoch 10/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4946 - val_loss: 0.4824\n",
            "Epoch 11/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4866 - val_loss: 0.4752\n",
            "Epoch 12/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4811 - val_loss: 0.4685\n",
            "Epoch 13/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4770 - val_loss: 0.4632\n",
            "Epoch 14/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4710 - val_loss: 0.4577\n",
            "Epoch 15/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4641 - val_loss: 0.4528\n",
            "Epoch 16/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4614 - val_loss: 0.4488\n",
            "Epoch 17/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4595 - val_loss: 0.4453\n",
            "Epoch 18/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4542 - val_loss: 0.4423\n",
            "Epoch 19/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4534 - val_loss: 0.4393\n",
            "Epoch 20/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4512 - val_loss: 0.4372\n",
            "Epoch 21/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4444 - val_loss: 0.4347\n",
            "Epoch 22/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4455 - val_loss: 0.4335\n",
            "Epoch 23/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4448 - val_loss: 0.4314\n",
            "Epoch 24/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4453 - val_loss: 0.4304\n",
            "Epoch 25/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4430 - val_loss: 0.4291\n",
            "Epoch 26/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4398 - val_loss: 0.4277\n",
            "Epoch 27/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4414 - val_loss: 0.4272\n",
            "Epoch 28/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4387 - val_loss: 0.4259\n",
            "Epoch 29/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4402 - val_loss: 0.4249\n",
            "Epoch 30/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4369 - val_loss: 0.4245\n",
            "Epoch 31/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4374 - val_loss: 0.4233\n",
            "Epoch 32/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4343 - val_loss: 0.4227\n",
            "Epoch 33/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4333 - val_loss: 0.4220\n",
            "Epoch 34/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4333 - val_loss: 0.4219\n",
            "Epoch 35/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4350 - val_loss: 0.4204\n",
            "Epoch 36/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4344 - val_loss: 0.4208\n",
            "Epoch 37/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4339 - val_loss: 0.4190\n",
            "Epoch 38/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4333 - val_loss: 0.4194\n",
            "Epoch 39/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4338 - val_loss: 0.4179\n",
            "Epoch 40/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4330 - val_loss: 0.4180\n",
            "Epoch 41/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4305 - val_loss: 0.4167\n",
            "Epoch 42/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4300 - val_loss: 0.4167\n",
            "Epoch 43/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4300 - val_loss: 0.4157\n",
            "Epoch 44/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4279 - val_loss: 0.4156\n",
            "Epoch 45/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4312 - val_loss: 0.4145\n",
            "Epoch 46/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4283 - val_loss: 0.4144\n",
            "Epoch 47/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4265 - val_loss: 0.4134\n",
            "Epoch 48/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4298 - val_loss: 0.4136\n",
            "Epoch 49/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4267 - val_loss: 0.4122\n",
            "Epoch 50/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4256 - val_loss: 0.4130\n",
            "Epoch 51/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4241 - val_loss: 0.4117\n",
            "Epoch 52/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4233 - val_loss: 0.4123\n",
            "Epoch 53/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4261 - val_loss: 0.4104\n",
            "Epoch 54/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4259 - val_loss: 0.4121\n",
            "Epoch 55/500\n",
            "3298/3298 [==============================] - 0s 65us/step - loss: 0.4248 - val_loss: 0.4097\n",
            "Epoch 56/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4240 - val_loss: 0.4118\n",
            "Epoch 57/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4255 - val_loss: 0.4099\n",
            "Epoch 58/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4245 - val_loss: 0.4133\n",
            "Epoch 59/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4252 - val_loss: 0.4114\n",
            "Epoch 60/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4248 - val_loss: 0.4142\n",
            "Epoch 61/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4247 - val_loss: 0.4115\n",
            "Epoch 62/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4279 - val_loss: 0.4191\n",
            "Epoch 63/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4296 - val_loss: 0.4139\n",
            "Epoch 64/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4306 - val_loss: 0.4225\n",
            "Epoch 65/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4309 - val_loss: 0.4156\n",
            "Epoch 66/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4314 - val_loss: 0.4223\n",
            "Epoch 67/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4308 - val_loss: 0.4150\n",
            "Epoch 68/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4324 - val_loss: 0.4218\n",
            "Epoch 69/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4304 - val_loss: 0.4136\n",
            "Epoch 70/500\n",
            "3298/3298 [==============================] - 0s 65us/step - loss: 0.4295 - val_loss: 0.4206\n",
            "Epoch 71/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4314 - val_loss: 0.4124\n",
            "Epoch 72/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4260 - val_loss: 0.4164\n",
            "Epoch 73/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4271 - val_loss: 0.4097\n",
            "Epoch 74/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4238 - val_loss: 0.4145\n",
            "Epoch 75/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4242 - val_loss: 0.4079\n",
            "Epoch 76/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4214 - val_loss: 0.4123\n",
            "Epoch 77/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4227 - val_loss: 0.4073\n",
            "Epoch 78/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4236 - val_loss: 0.4122\n",
            "Epoch 79/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4234 - val_loss: 0.4064\n",
            "Epoch 80/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4227 - val_loss: 0.4112\n",
            "Epoch 81/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4211 - val_loss: 0.4049\n",
            "Epoch 82/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4210 - val_loss: 0.4107\n",
            "Epoch 83/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4215 - val_loss: 0.4056\n",
            "Epoch 84/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4198 - val_loss: 0.4096\n",
            "Epoch 85/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4201 - val_loss: 0.4034\n",
            "Epoch 86/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4185 - val_loss: 0.4084\n",
            "Epoch 87/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4218 - val_loss: 0.4027\n",
            "Epoch 88/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4172 - val_loss: 0.4068\n",
            "Epoch 89/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4177 - val_loss: 0.4014\n",
            "Epoch 90/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4182 - val_loss: 0.4051\n",
            "Epoch 91/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4170 - val_loss: 0.3996\n",
            "Epoch 92/500\n",
            "3298/3298 [==============================] - 1s 187us/step - loss: 0.4139 - val_loss: 0.4051\n",
            "Epoch 93/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4169 - val_loss: 0.4007\n",
            "Epoch 94/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4165 - val_loss: 0.4071\n",
            "Epoch 95/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4169 - val_loss: 0.4014\n",
            "Epoch 96/500\n",
            "3298/3298 [==============================] - 0s 65us/step - loss: 0.4171 - val_loss: 0.4077\n",
            "Epoch 97/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4229 - val_loss: 0.4030\n",
            "Epoch 98/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4169 - val_loss: 0.4094\n",
            "Epoch 99/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4196 - val_loss: 0.4017\n",
            "Epoch 100/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4187 - val_loss: 0.4086\n",
            "Epoch 101/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4205 - val_loss: 0.4017\n",
            "Epoch 102/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4164 - val_loss: 0.4078\n",
            "Epoch 103/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4191 - val_loss: 0.4004\n",
            "Epoch 104/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4159 - val_loss: 0.4057\n",
            "Epoch 105/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4178 - val_loss: 0.3989\n",
            "Epoch 106/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4132 - val_loss: 0.4031\n",
            "Epoch 107/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4158 - val_loss: 0.3969\n",
            "Epoch 108/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4116 - val_loss: 0.4023\n",
            "Epoch 109/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4133 - val_loss: 0.3957\n",
            "Epoch 110/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4123 - val_loss: 0.4034\n",
            "Epoch 111/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4145 - val_loss: 0.3963\n",
            "Epoch 112/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4134 - val_loss: 0.4027\n",
            "Epoch 113/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4161 - val_loss: 0.3949\n",
            "Epoch 114/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4116 - val_loss: 0.4018\n",
            "Epoch 115/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4103 - val_loss: 0.3955\n",
            "Epoch 116/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4110 - val_loss: 0.3997\n",
            "Epoch 117/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4133 - val_loss: 0.3929\n",
            "Epoch 118/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4114 - val_loss: 0.3997\n",
            "Epoch 119/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4099 - val_loss: 0.3919\n",
            "Epoch 120/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4097 - val_loss: 0.3995\n",
            "Epoch 121/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4096 - val_loss: 0.3929\n",
            "Epoch 122/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4112 - val_loss: 0.4006\n",
            "Epoch 123/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4107 - val_loss: 0.3935\n",
            "Epoch 124/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4113 - val_loss: 0.4005\n",
            "Epoch 125/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4117 - val_loss: 0.3935\n",
            "Epoch 126/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4109 - val_loss: 0.3990\n",
            "Epoch 127/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4097 - val_loss: 0.3922\n",
            "Epoch 128/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4126 - val_loss: 0.3980\n",
            "Epoch 129/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4110 - val_loss: 0.3910\n",
            "Epoch 130/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4116 - val_loss: 0.3973\n",
            "Epoch 131/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4080 - val_loss: 0.3902\n",
            "Epoch 132/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.4097 - val_loss: 0.3968\n",
            "Epoch 133/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4075 - val_loss: 0.3888\n",
            "Epoch 134/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4087 - val_loss: 0.3972\n",
            "Epoch 135/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4098 - val_loss: 0.3895\n",
            "Epoch 136/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4088 - val_loss: 0.3969\n",
            "Epoch 137/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.4095 - val_loss: 0.3886\n",
            "Epoch 138/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4081 - val_loss: 0.3917\n",
            "Epoch 139/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4067 - val_loss: 0.3847\n",
            "Epoch 140/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4040 - val_loss: 0.3900\n",
            "Epoch 141/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4015 - val_loss: 0.3839\n",
            "Epoch 142/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4037 - val_loss: 0.3894\n",
            "Epoch 143/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4027 - val_loss: 0.3835\n",
            "Epoch 144/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4017 - val_loss: 0.3895\n",
            "Epoch 145/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4020 - val_loss: 0.3843\n",
            "Epoch 146/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4039 - val_loss: 0.3918\n",
            "Epoch 147/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4070 - val_loss: 0.3866\n",
            "Epoch 148/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4045 - val_loss: 0.3967\n",
            "Epoch 149/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4095 - val_loss: 0.3888\n",
            "Epoch 150/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4063 - val_loss: 0.3969\n",
            "Epoch 151/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4091 - val_loss: 0.3879\n",
            "Epoch 152/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4066 - val_loss: 0.3937\n",
            "Epoch 153/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4065 - val_loss: 0.3849\n",
            "Epoch 154/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.4060 - val_loss: 0.3921\n",
            "Epoch 155/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.4067 - val_loss: 0.3829\n",
            "Epoch 156/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4031 - val_loss: 0.3878\n",
            "Epoch 157/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4013 - val_loss: 0.3810\n",
            "Epoch 158/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4012 - val_loss: 0.3879\n",
            "Epoch 159/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4028 - val_loss: 0.3798\n",
            "Epoch 160/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3997 - val_loss: 0.3875\n",
            "Epoch 161/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4030 - val_loss: 0.3802\n",
            "Epoch 162/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3996 - val_loss: 0.3851\n",
            "Epoch 163/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.4011 - val_loss: 0.3783\n",
            "Epoch 164/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3990 - val_loss: 0.3842\n",
            "Epoch 165/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3998 - val_loss: 0.3774\n",
            "Epoch 166/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3984 - val_loss: 0.3849\n",
            "Epoch 167/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3976 - val_loss: 0.3769\n",
            "Epoch 168/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3977 - val_loss: 0.3831\n",
            "Epoch 169/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3978 - val_loss: 0.3761\n",
            "Epoch 170/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3971 - val_loss: 0.3824\n",
            "Epoch 171/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3961 - val_loss: 0.3754\n",
            "Epoch 172/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3965 - val_loss: 0.3810\n",
            "Epoch 173/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3975 - val_loss: 0.3742\n",
            "Epoch 174/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3958 - val_loss: 0.3800\n",
            "Epoch 175/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3938 - val_loss: 0.3741\n",
            "Epoch 176/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3949 - val_loss: 0.3814\n",
            "Epoch 177/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3961 - val_loss: 0.3750\n",
            "Epoch 178/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3960 - val_loss: 0.3849\n",
            "Epoch 179/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4020 - val_loss: 0.3767\n",
            "Epoch 180/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3967 - val_loss: 0.3849\n",
            "Epoch 181/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3994 - val_loss: 0.3768\n",
            "Epoch 182/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3993 - val_loss: 0.3840\n",
            "Epoch 183/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3989 - val_loss: 0.3759\n",
            "Epoch 184/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4004 - val_loss: 0.3838\n",
            "Epoch 185/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3992 - val_loss: 0.3753\n",
            "Epoch 186/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3992 - val_loss: 0.3818\n",
            "Epoch 187/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3968 - val_loss: 0.3739\n",
            "Epoch 188/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3942 - val_loss: 0.3789\n",
            "Epoch 189/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3979 - val_loss: 0.3711\n",
            "Epoch 190/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3927 - val_loss: 0.3763\n",
            "Epoch 191/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3939 - val_loss: 0.3700\n",
            "Epoch 192/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3931 - val_loss: 0.3765\n",
            "Epoch 193/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3921 - val_loss: 0.3701\n",
            "Epoch 194/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3944 - val_loss: 0.3746\n",
            "Epoch 195/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3902 - val_loss: 0.3685\n",
            "Epoch 196/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3946 - val_loss: 0.3754\n",
            "Epoch 197/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3913 - val_loss: 0.3688\n",
            "Epoch 198/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3906 - val_loss: 0.3753\n",
            "Epoch 199/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3922 - val_loss: 0.3684\n",
            "Epoch 200/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3924 - val_loss: 0.3758\n",
            "Epoch 201/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3926 - val_loss: 0.3693\n",
            "Epoch 202/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3939 - val_loss: 0.3773\n",
            "Epoch 203/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3947 - val_loss: 0.3703\n",
            "Epoch 204/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3912 - val_loss: 0.3762\n",
            "Epoch 205/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3937 - val_loss: 0.3684\n",
            "Epoch 206/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3896 - val_loss: 0.3747\n",
            "Epoch 207/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3904 - val_loss: 0.3668\n",
            "Epoch 208/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3937 - val_loss: 0.3723\n",
            "Epoch 209/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3916 - val_loss: 0.3654\n",
            "Epoch 210/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3898 - val_loss: 0.3712\n",
            "Epoch 211/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3893 - val_loss: 0.3633\n",
            "Epoch 212/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3864 - val_loss: 0.3682\n",
            "Epoch 213/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3859 - val_loss: 0.3616\n",
            "Epoch 214/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3861 - val_loss: 0.3678\n",
            "Epoch 215/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3881 - val_loss: 0.3626\n",
            "Epoch 216/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3858 - val_loss: 0.3683\n",
            "Epoch 217/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3847 - val_loss: 0.3619\n",
            "Epoch 218/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3867 - val_loss: 0.3680\n",
            "Epoch 219/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3845 - val_loss: 0.3617\n",
            "Epoch 220/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3873 - val_loss: 0.3680\n",
            "Epoch 221/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3891 - val_loss: 0.3617\n",
            "Epoch 222/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3877 - val_loss: 0.3690\n",
            "Epoch 223/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3863 - val_loss: 0.3623\n",
            "Epoch 224/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3864 - val_loss: 0.3696\n",
            "Epoch 225/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3868 - val_loss: 0.3622\n",
            "Epoch 226/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3860 - val_loss: 0.3696\n",
            "Epoch 227/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3844 - val_loss: 0.3622\n",
            "Epoch 228/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3867 - val_loss: 0.3696\n",
            "Epoch 229/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3871 - val_loss: 0.3617\n",
            "Epoch 230/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3842 - val_loss: 0.3684\n",
            "Epoch 231/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3858 - val_loss: 0.3606\n",
            "Epoch 232/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3855 - val_loss: 0.3658\n",
            "Epoch 233/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3808 - val_loss: 0.3591\n",
            "Epoch 234/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3832 - val_loss: 0.3647\n",
            "Epoch 235/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3832 - val_loss: 0.3580\n",
            "Epoch 236/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3802 - val_loss: 0.3637\n",
            "Epoch 237/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3816 - val_loss: 0.3564\n",
            "Epoch 238/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3835 - val_loss: 0.3631\n",
            "Epoch 239/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3809 - val_loss: 0.3569\n",
            "Epoch 240/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3822 - val_loss: 0.3644\n",
            "Epoch 241/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3835 - val_loss: 0.3576\n",
            "Epoch 242/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3816 - val_loss: 0.3638\n",
            "Epoch 243/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3824 - val_loss: 0.3556\n",
            "Epoch 244/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3806 - val_loss: 0.3614\n",
            "Epoch 245/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3785 - val_loss: 0.3547\n",
            "Epoch 246/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3785 - val_loss: 0.3603\n",
            "Epoch 247/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3776 - val_loss: 0.3546\n",
            "Epoch 248/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3783 - val_loss: 0.3618\n",
            "Epoch 249/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3805 - val_loss: 0.3552\n",
            "Epoch 250/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3817 - val_loss: 0.3646\n",
            "Epoch 251/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3822 - val_loss: 0.3566\n",
            "Epoch 252/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3819 - val_loss: 0.3645\n",
            "Epoch 253/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3825 - val_loss: 0.3573\n",
            "Epoch 254/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3801 - val_loss: 0.3633\n",
            "Epoch 255/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3792 - val_loss: 0.3564\n",
            "Epoch 256/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3804 - val_loss: 0.3620\n",
            "Epoch 257/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3802 - val_loss: 0.3547\n",
            "Epoch 258/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3812 - val_loss: 0.3602\n",
            "Epoch 259/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3766 - val_loss: 0.3527\n",
            "Epoch 260/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3795 - val_loss: 0.3580\n",
            "Epoch 261/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3777 - val_loss: 0.3531\n",
            "Epoch 262/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3800 - val_loss: 0.3605\n",
            "Epoch 263/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3793 - val_loss: 0.3523\n",
            "Epoch 264/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3799 - val_loss: 0.3592\n",
            "Epoch 265/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3778 - val_loss: 0.3515\n",
            "Epoch 266/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3756 - val_loss: 0.3563\n",
            "Epoch 267/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3741 - val_loss: 0.3503\n",
            "Epoch 268/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3765 - val_loss: 0.3558\n",
            "Epoch 269/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3782 - val_loss: 0.3488\n",
            "Epoch 270/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3730 - val_loss: 0.3537\n",
            "Epoch 271/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3714 - val_loss: 0.3485\n",
            "Epoch 272/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3725 - val_loss: 0.3526\n",
            "Epoch 273/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3746 - val_loss: 0.3472\n",
            "Epoch 274/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3722 - val_loss: 0.3521\n",
            "Epoch 275/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3691 - val_loss: 0.3465\n",
            "Epoch 276/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3724 - val_loss: 0.3519\n",
            "Epoch 277/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3733 - val_loss: 0.3462\n",
            "Epoch 278/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3701 - val_loss: 0.3510\n",
            "Epoch 279/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3741 - val_loss: 0.3445\n",
            "Epoch 280/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3691 - val_loss: 0.3500\n",
            "Epoch 281/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3708 - val_loss: 0.3445\n",
            "Epoch 282/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3709 - val_loss: 0.3516\n",
            "Epoch 283/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3689 - val_loss: 0.3462\n",
            "Epoch 284/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3722 - val_loss: 0.3526\n",
            "Epoch 285/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3713 - val_loss: 0.3462\n",
            "Epoch 286/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3735 - val_loss: 0.3538\n",
            "Epoch 287/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3723 - val_loss: 0.3468\n",
            "Epoch 288/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3757 - val_loss: 0.3540\n",
            "Epoch 289/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3708 - val_loss: 0.3467\n",
            "Epoch 290/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3716 - val_loss: 0.3537\n",
            "Epoch 291/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3741 - val_loss: 0.3473\n",
            "Epoch 292/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3742 - val_loss: 0.3546\n",
            "Epoch 293/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3749 - val_loss: 0.3473\n",
            "Epoch 294/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3731 - val_loss: 0.3522\n",
            "Epoch 295/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3725 - val_loss: 0.3449\n",
            "Epoch 296/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3698 - val_loss: 0.3506\n",
            "Epoch 297/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3688 - val_loss: 0.3433\n",
            "Epoch 298/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3679 - val_loss: 0.3475\n",
            "Epoch 299/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3671 - val_loss: 0.3413\n",
            "Epoch 300/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3665 - val_loss: 0.3472\n",
            "Epoch 301/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3680 - val_loss: 0.3411\n",
            "Epoch 302/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3643 - val_loss: 0.3455\n",
            "Epoch 303/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3636 - val_loss: 0.3404\n",
            "Epoch 304/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3624 - val_loss: 0.3448\n",
            "Epoch 305/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3650 - val_loss: 0.3394\n",
            "Epoch 306/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3649 - val_loss: 0.3441\n",
            "Epoch 307/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3615 - val_loss: 0.3394\n",
            "Epoch 308/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3634 - val_loss: 0.3438\n",
            "Epoch 309/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3668 - val_loss: 0.3389\n",
            "Epoch 310/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3639 - val_loss: 0.3419\n",
            "Epoch 311/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3630 - val_loss: 0.3389\n",
            "Epoch 312/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3629 - val_loss: 0.3433\n",
            "Epoch 313/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3640 - val_loss: 0.3390\n",
            "Epoch 314/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3671 - val_loss: 0.3450\n",
            "Epoch 315/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3630 - val_loss: 0.3391\n",
            "Epoch 316/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3633 - val_loss: 0.3440\n",
            "Epoch 317/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3640 - val_loss: 0.3397\n",
            "Epoch 318/500\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.3643 - val_loss: 0.3460\n",
            "Epoch 319/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3633 - val_loss: 0.3404\n",
            "Epoch 320/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3647 - val_loss: 0.3460\n",
            "Epoch 321/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3645 - val_loss: 0.3400\n",
            "Epoch 322/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3608 - val_loss: 0.3452\n",
            "Epoch 323/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3630 - val_loss: 0.3407\n",
            "Epoch 324/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3637 - val_loss: 0.3472\n",
            "Epoch 325/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3665 - val_loss: 0.3410\n",
            "Epoch 326/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3666 - val_loss: 0.3443\n",
            "Epoch 327/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3646 - val_loss: 0.3390\n",
            "Epoch 328/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3599 - val_loss: 0.3428\n",
            "Epoch 329/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3629 - val_loss: 0.3378\n",
            "Epoch 330/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3611 - val_loss: 0.3408\n",
            "Epoch 331/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3610 - val_loss: 0.3361\n",
            "Epoch 332/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3589 - val_loss: 0.3394\n",
            "Epoch 333/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3576 - val_loss: 0.3360\n",
            "Epoch 334/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3610 - val_loss: 0.3391\n",
            "Epoch 335/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3591 - val_loss: 0.3352\n",
            "Epoch 336/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3580 - val_loss: 0.3377\n",
            "Epoch 337/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3566 - val_loss: 0.3330\n",
            "Epoch 338/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3538 - val_loss: 0.3372\n",
            "Epoch 339/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3567 - val_loss: 0.3333\n",
            "Epoch 340/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3581 - val_loss: 0.3384\n",
            "Epoch 341/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3562 - val_loss: 0.3349\n",
            "Epoch 342/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.3594 - val_loss: 0.3393\n",
            "Epoch 343/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3626 - val_loss: 0.3351\n",
            "Epoch 344/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3570 - val_loss: 0.3380\n",
            "Epoch 345/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3575 - val_loss: 0.3333\n",
            "Epoch 346/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3551 - val_loss: 0.3366\n",
            "Epoch 347/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3567 - val_loss: 0.3335\n",
            "Epoch 348/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3559 - val_loss: 0.3384\n",
            "Epoch 349/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3557 - val_loss: 0.3336\n",
            "Epoch 350/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3555 - val_loss: 0.3375\n",
            "Epoch 351/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3570 - val_loss: 0.3335\n",
            "Epoch 352/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3569 - val_loss: 0.3390\n",
            "Epoch 353/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3575 - val_loss: 0.3337\n",
            "Epoch 354/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3550 - val_loss: 0.3387\n",
            "Epoch 355/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3574 - val_loss: 0.3324\n",
            "Epoch 356/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3557 - val_loss: 0.3379\n",
            "Epoch 357/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3565 - val_loss: 0.3321\n",
            "Epoch 358/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3564 - val_loss: 0.3386\n",
            "Epoch 359/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3545 - val_loss: 0.3337\n",
            "Epoch 360/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3542 - val_loss: 0.3399\n",
            "Epoch 361/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3569 - val_loss: 0.3329\n",
            "Epoch 362/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3535 - val_loss: 0.3380\n",
            "Epoch 363/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3547 - val_loss: 0.3320\n",
            "Epoch 364/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3548 - val_loss: 0.3366\n",
            "Epoch 365/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3522 - val_loss: 0.3316\n",
            "Epoch 366/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3527 - val_loss: 0.3371\n",
            "Epoch 367/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3558 - val_loss: 0.3315\n",
            "Epoch 368/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3549 - val_loss: 0.3360\n",
            "Epoch 369/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3533 - val_loss: 0.3286\n",
            "Epoch 370/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3507 - val_loss: 0.3327\n",
            "Epoch 371/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3497 - val_loss: 0.3276\n",
            "Epoch 372/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3522 - val_loss: 0.3317\n",
            "Epoch 373/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3519 - val_loss: 0.3271\n",
            "Epoch 374/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3508 - val_loss: 0.3322\n",
            "Epoch 375/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3489 - val_loss: 0.3268\n",
            "Epoch 376/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3483 - val_loss: 0.3312\n",
            "Epoch 377/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3461 - val_loss: 0.3261\n",
            "Epoch 378/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3485 - val_loss: 0.3302\n",
            "Epoch 379/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3459 - val_loss: 0.3249\n",
            "Epoch 380/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3482 - val_loss: 0.3297\n",
            "Epoch 381/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3450 - val_loss: 0.3238\n",
            "Epoch 382/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3441 - val_loss: 0.3286\n",
            "Epoch 383/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3449 - val_loss: 0.3241\n",
            "Epoch 384/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3463 - val_loss: 0.3298\n",
            "Epoch 385/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3461 - val_loss: 0.3249\n",
            "Epoch 386/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3471 - val_loss: 0.3301\n",
            "Epoch 387/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3428 - val_loss: 0.3245\n",
            "Epoch 388/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3441 - val_loss: 0.3289\n",
            "Epoch 389/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3443 - val_loss: 0.3233\n",
            "Epoch 390/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3449 - val_loss: 0.3297\n",
            "Epoch 391/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3451 - val_loss: 0.3229\n",
            "Epoch 392/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3439 - val_loss: 0.3302\n",
            "Epoch 393/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3462 - val_loss: 0.3239\n",
            "Epoch 394/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3472 - val_loss: 0.3302\n",
            "Epoch 395/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3451 - val_loss: 0.3238\n",
            "Epoch 396/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3462 - val_loss: 0.3298\n",
            "Epoch 397/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3457 - val_loss: 0.3257\n",
            "Epoch 398/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3462 - val_loss: 0.3330\n",
            "Epoch 399/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3491 - val_loss: 0.3255\n",
            "Epoch 400/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3490 - val_loss: 0.3299\n",
            "Epoch 401/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3486 - val_loss: 0.3237\n",
            "Epoch 402/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3430 - val_loss: 0.3286\n",
            "Epoch 403/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3437 - val_loss: 0.3228\n",
            "Epoch 404/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3447 - val_loss: 0.3285\n",
            "Epoch 405/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3415 - val_loss: 0.3230\n",
            "Epoch 406/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3430 - val_loss: 0.3277\n",
            "Epoch 407/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3444 - val_loss: 0.3237\n",
            "Epoch 408/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3454 - val_loss: 0.3289\n",
            "Epoch 409/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3462 - val_loss: 0.3228\n",
            "Epoch 410/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3444 - val_loss: 0.3248\n",
            "Epoch 411/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3410 - val_loss: 0.3193\n",
            "Epoch 412/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3408 - val_loss: 0.3226\n",
            "Epoch 413/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.3403 - val_loss: 0.3191\n",
            "Epoch 414/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3387 - val_loss: 0.3221\n",
            "Epoch 415/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3384 - val_loss: 0.3178\n",
            "Epoch 416/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3370 - val_loss: 0.3210\n",
            "Epoch 417/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3369 - val_loss: 0.3183\n",
            "Epoch 418/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3381 - val_loss: 0.3211\n",
            "Epoch 419/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3379 - val_loss: 0.3175\n",
            "Epoch 420/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3386 - val_loss: 0.3214\n",
            "Epoch 421/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3378 - val_loss: 0.3172\n",
            "Epoch 422/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3395 - val_loss: 0.3212\n",
            "Epoch 423/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.3344 - val_loss: 0.3177\n",
            "Epoch 424/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3405 - val_loss: 0.3219\n",
            "Epoch 425/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3385 - val_loss: 0.3179\n",
            "Epoch 426/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3378 - val_loss: 0.3213\n",
            "Epoch 427/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3384 - val_loss: 0.3179\n",
            "Epoch 428/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.3374 - val_loss: 0.3225\n",
            "Epoch 429/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3388 - val_loss: 0.3173\n",
            "Epoch 430/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3391 - val_loss: 0.3239\n",
            "Epoch 431/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3386 - val_loss: 0.3184\n",
            "Epoch 432/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3383 - val_loss: 0.3214\n",
            "Epoch 433/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.3389 - val_loss: 0.3172\n",
            "Epoch 434/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3382 - val_loss: 0.3224\n",
            "Epoch 435/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3397 - val_loss: 0.3161\n",
            "Epoch 436/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3358 - val_loss: 0.3194\n",
            "Epoch 437/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3349 - val_loss: 0.3152\n",
            "Epoch 438/500\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.3360 - val_loss: 0.3202\n",
            "Epoch 439/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3345 - val_loss: 0.3150\n",
            "Epoch 440/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3364 - val_loss: 0.3210\n",
            "Epoch 441/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3347 - val_loss: 0.3162\n",
            "Epoch 442/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3364 - val_loss: 0.3207\n",
            "Epoch 443/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3369 - val_loss: 0.3151\n",
            "Epoch 444/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3376 - val_loss: 0.3199\n",
            "Epoch 445/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3361 - val_loss: 0.3143\n",
            "Epoch 446/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3365 - val_loss: 0.3203\n",
            "Epoch 447/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3356 - val_loss: 0.3141\n",
            "Epoch 448/500\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.3357 - val_loss: 0.3221\n",
            "Epoch 449/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3353 - val_loss: 0.3146\n",
            "Epoch 450/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3350 - val_loss: 0.3213\n",
            "Epoch 451/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3353 - val_loss: 0.3141\n",
            "Epoch 452/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3353 - val_loss: 0.3215\n",
            "Epoch 453/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3361 - val_loss: 0.3146\n",
            "Epoch 454/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3347 - val_loss: 0.3220\n",
            "Epoch 455/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3355 - val_loss: 0.3131\n",
            "Epoch 456/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3343 - val_loss: 0.3209\n",
            "Epoch 457/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3330 - val_loss: 0.3123\n",
            "Epoch 458/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3346 - val_loss: 0.3186\n",
            "Epoch 459/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3326 - val_loss: 0.3110\n",
            "Epoch 460/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3313 - val_loss: 0.3172\n",
            "Epoch 461/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3323 - val_loss: 0.3107\n",
            "Epoch 462/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3317 - val_loss: 0.3163\n",
            "Epoch 463/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3324 - val_loss: 0.3101\n",
            "Epoch 464/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3318 - val_loss: 0.3165\n",
            "Epoch 465/500\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3309 - val_loss: 0.3098\n",
            "Epoch 466/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3288 - val_loss: 0.3169\n",
            "Epoch 467/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3320 - val_loss: 0.3101\n",
            "Epoch 468/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3332 - val_loss: 0.3183\n",
            "Epoch 469/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3350 - val_loss: 0.3099\n",
            "Epoch 470/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3318 - val_loss: 0.3176\n",
            "Epoch 471/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3306 - val_loss: 0.3095\n",
            "Epoch 472/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3315 - val_loss: 0.3164\n",
            "Epoch 473/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3294 - val_loss: 0.3086\n",
            "Epoch 474/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3312 - val_loss: 0.3159\n",
            "Epoch 475/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3294 - val_loss: 0.3082\n",
            "Epoch 476/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3285 - val_loss: 0.3138\n",
            "Epoch 477/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3273 - val_loss: 0.3065\n",
            "Epoch 478/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3293 - val_loss: 0.3133\n",
            "Epoch 479/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3257 - val_loss: 0.3058\n",
            "Epoch 480/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3251 - val_loss: 0.3116\n",
            "Epoch 481/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3282 - val_loss: 0.3048\n",
            "Epoch 482/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3250 - val_loss: 0.3111\n",
            "Epoch 483/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3231 - val_loss: 0.3047\n",
            "Epoch 484/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3262 - val_loss: 0.3120\n",
            "Epoch 485/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3267 - val_loss: 0.3047\n",
            "Epoch 486/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3258 - val_loss: 0.3130\n",
            "Epoch 487/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3280 - val_loss: 0.3059\n",
            "Epoch 488/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3270 - val_loss: 0.3139\n",
            "Epoch 489/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3279 - val_loss: 0.3058\n",
            "Epoch 490/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3277 - val_loss: 0.3138\n",
            "Epoch 491/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3252 - val_loss: 0.3048\n",
            "Epoch 492/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3227 - val_loss: 0.3118\n",
            "Epoch 493/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3260 - val_loss: 0.3036\n",
            "Epoch 494/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3222 - val_loss: 0.3110\n",
            "Epoch 495/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3260 - val_loss: 0.3035\n",
            "Epoch 496/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3217 - val_loss: 0.3103\n",
            "Epoch 497/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3249 - val_loss: 0.3032\n",
            "Epoch 498/500\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3206 - val_loss: 0.3095\n",
            "Epoch 499/500\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3220 - val_loss: 0.3029\n",
            "Epoch 500/500\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.3213 - val_loss: 0.3100\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_5 (Bidirection (None, 5, 1024)           2146304   \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 5, 256)            1311744   \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 5, 1)              257       \n",
            "=================================================================\n",
            "Total params: 3,458,305\n",
            "Trainable params: 3,458,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 3298 samples, validate on 825 samples\n",
            "Epoch 1/1000\n",
            "3298/3298 [==============================] - 4s 1ms/step - loss: 1.0034 - val_loss: 0.6959\n",
            "Epoch 2/1000\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.6851 - val_loss: 0.5176\n",
            "Epoch 3/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.4998 - val_loss: 0.4182\n",
            "Epoch 4/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.3987 - val_loss: 0.3582\n",
            "Epoch 5/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.3487 - val_loss: 0.3167\n",
            "Epoch 6/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3090 - val_loss: 0.2880\n",
            "Epoch 7/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2835 - val_loss: 0.2671\n",
            "Epoch 8/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2668 - val_loss: 0.2520\n",
            "Epoch 9/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2554 - val_loss: 0.2400\n",
            "Epoch 10/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2403 - val_loss: 0.2314\n",
            "Epoch 11/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2316 - val_loss: 0.2245\n",
            "Epoch 12/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2273 - val_loss: 0.2192\n",
            "Epoch 13/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2228 - val_loss: 0.2151\n",
            "Epoch 14/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2173 - val_loss: 0.2118\n",
            "Epoch 15/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2161 - val_loss: 0.2098\n",
            "Epoch 16/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2119 - val_loss: 0.2073\n",
            "Epoch 17/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2112 - val_loss: 0.2055\n",
            "Epoch 18/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2095 - val_loss: 0.2033\n",
            "Epoch 19/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.2078 - val_loss: 0.2019\n",
            "Epoch 20/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2083 - val_loss: 0.2006\n",
            "Epoch 21/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2050 - val_loss: 0.1997\n",
            "Epoch 22/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2037 - val_loss: 0.1985\n",
            "Epoch 23/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2059 - val_loss: 0.1984\n",
            "Epoch 24/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2068 - val_loss: 0.1979\n",
            "Epoch 25/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2072 - val_loss: 0.1993\n",
            "Epoch 26/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2071 - val_loss: 0.1986\n",
            "Epoch 27/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2051 - val_loss: 0.2019\n",
            "Epoch 28/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2059 - val_loss: 0.1995\n",
            "Epoch 29/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2064 - val_loss: 0.2047\n",
            "Epoch 30/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2096 - val_loss: 0.2005\n",
            "Epoch 31/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.2089 - val_loss: 0.2049\n",
            "Epoch 32/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2105 - val_loss: 0.2019\n",
            "Epoch 33/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2086 - val_loss: 0.2063\n",
            "Epoch 34/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2126 - val_loss: 0.2013\n",
            "Epoch 35/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2113 - val_loss: 0.2066\n",
            "Epoch 36/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2148 - val_loss: 0.2006\n",
            "Epoch 37/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.2109 - val_loss: 0.2030\n",
            "Epoch 38/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2092 - val_loss: 0.1969\n",
            "Epoch 39/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2050 - val_loss: 0.1989\n",
            "Epoch 40/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.2057 - val_loss: 0.1938\n",
            "Epoch 41/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2037 - val_loss: 0.1960\n",
            "Epoch 42/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2038 - val_loss: 0.1902\n",
            "Epoch 43/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.2007 - val_loss: 0.1928\n",
            "Epoch 44/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2022 - val_loss: 0.1890\n",
            "Epoch 45/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2005 - val_loss: 0.1914\n",
            "Epoch 46/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1979 - val_loss: 0.1876\n",
            "Epoch 47/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1986 - val_loss: 0.1914\n",
            "Epoch 48/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1979 - val_loss: 0.1864\n",
            "Epoch 49/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1957 - val_loss: 0.1882\n",
            "Epoch 50/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1987 - val_loss: 0.1851\n",
            "Epoch 51/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1946 - val_loss: 0.1869\n",
            "Epoch 52/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1964 - val_loss: 0.1830\n",
            "Epoch 53/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1907 - val_loss: 0.1840\n",
            "Epoch 54/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1943 - val_loss: 0.1814\n",
            "Epoch 55/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1946 - val_loss: 0.1834\n",
            "Epoch 56/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1922 - val_loss: 0.1809\n",
            "Epoch 57/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1904 - val_loss: 0.1826\n",
            "Epoch 58/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1946 - val_loss: 0.1791\n",
            "Epoch 59/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1887 - val_loss: 0.1801\n",
            "Epoch 60/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1911 - val_loss: 0.1784\n",
            "Epoch 61/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1911 - val_loss: 0.1812\n",
            "Epoch 62/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1875 - val_loss: 0.1784\n",
            "Epoch 63/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1891 - val_loss: 0.1802\n",
            "Epoch 64/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1885 - val_loss: 0.1780\n",
            "Epoch 65/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1870 - val_loss: 0.1790\n",
            "Epoch 66/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1896 - val_loss: 0.1763\n",
            "Epoch 67/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1888 - val_loss: 0.1792\n",
            "Epoch 68/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1853 - val_loss: 0.1765\n",
            "Epoch 69/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1874 - val_loss: 0.1789\n",
            "Epoch 70/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1857 - val_loss: 0.1760\n",
            "Epoch 71/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1888 - val_loss: 0.1792\n",
            "Epoch 72/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1871 - val_loss: 0.1752\n",
            "Epoch 73/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1865 - val_loss: 0.1779\n",
            "Epoch 74/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1859 - val_loss: 0.1746\n",
            "Epoch 75/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1865 - val_loss: 0.1762\n",
            "Epoch 76/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1825 - val_loss: 0.1722\n",
            "Epoch 77/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1812 - val_loss: 0.1750\n",
            "Epoch 78/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1852 - val_loss: 0.1722\n",
            "Epoch 79/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1832 - val_loss: 0.1733\n",
            "Epoch 80/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1813 - val_loss: 0.1698\n",
            "Epoch 81/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1809 - val_loss: 0.1731\n",
            "Epoch 82/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1823 - val_loss: 0.1708\n",
            "Epoch 83/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1805 - val_loss: 0.1742\n",
            "Epoch 84/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1829 - val_loss: 0.1709\n",
            "Epoch 85/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1831 - val_loss: 0.1743\n",
            "Epoch 86/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1830 - val_loss: 0.1708\n",
            "Epoch 87/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1823 - val_loss: 0.1742\n",
            "Epoch 88/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1851 - val_loss: 0.1703\n",
            "Epoch 89/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1798 - val_loss: 0.1721\n",
            "Epoch 90/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1781 - val_loss: 0.1687\n",
            "Epoch 91/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1796 - val_loss: 0.1716\n",
            "Epoch 92/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1793 - val_loss: 0.1672\n",
            "Epoch 93/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1799 - val_loss: 0.1695\n",
            "Epoch 94/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1780 - val_loss: 0.1660\n",
            "Epoch 95/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1755 - val_loss: 0.1696\n",
            "Epoch 96/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1766 - val_loss: 0.1659\n",
            "Epoch 97/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1775 - val_loss: 0.1687\n",
            "Epoch 98/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1769 - val_loss: 0.1649\n",
            "Epoch 99/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1775 - val_loss: 0.1671\n",
            "Epoch 100/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1760 - val_loss: 0.1639\n",
            "Epoch 101/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1763 - val_loss: 0.1678\n",
            "Epoch 102/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1743 - val_loss: 0.1637\n",
            "Epoch 103/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1782 - val_loss: 0.1669\n",
            "Epoch 104/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1762 - val_loss: 0.1632\n",
            "Epoch 105/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1717 - val_loss: 0.1675\n",
            "Epoch 106/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1769 - val_loss: 0.1646\n",
            "Epoch 107/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1753 - val_loss: 0.1675\n",
            "Epoch 108/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1749 - val_loss: 0.1633\n",
            "Epoch 109/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1756 - val_loss: 0.1666\n",
            "Epoch 110/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1742 - val_loss: 0.1628\n",
            "Epoch 111/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1746 - val_loss: 0.1674\n",
            "Epoch 112/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1740 - val_loss: 0.1627\n",
            "Epoch 113/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1746 - val_loss: 0.1678\n",
            "Epoch 114/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1775 - val_loss: 0.1630\n",
            "Epoch 115/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1751 - val_loss: 0.1673\n",
            "Epoch 116/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1755 - val_loss: 0.1615\n",
            "Epoch 117/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1713 - val_loss: 0.1641\n",
            "Epoch 118/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1723 - val_loss: 0.1602\n",
            "Epoch 119/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1696 - val_loss: 0.1637\n",
            "Epoch 120/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1731 - val_loss: 0.1589\n",
            "Epoch 121/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1692 - val_loss: 0.1624\n",
            "Epoch 122/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1733 - val_loss: 0.1581\n",
            "Epoch 123/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1730 - val_loss: 0.1619\n",
            "Epoch 124/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1716 - val_loss: 0.1579\n",
            "Epoch 125/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1698 - val_loss: 0.1628\n",
            "Epoch 126/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1720 - val_loss: 0.1577\n",
            "Epoch 127/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1707 - val_loss: 0.1619\n",
            "Epoch 128/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1698 - val_loss: 0.1570\n",
            "Epoch 129/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1697 - val_loss: 0.1623\n",
            "Epoch 130/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1725 - val_loss: 0.1578\n",
            "Epoch 131/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1680 - val_loss: 0.1631\n",
            "Epoch 132/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1720 - val_loss: 0.1573\n",
            "Epoch 133/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1695 - val_loss: 0.1622\n",
            "Epoch 134/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1684 - val_loss: 0.1560\n",
            "Epoch 135/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1675 - val_loss: 0.1610\n",
            "Epoch 136/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1678 - val_loss: 0.1550\n",
            "Epoch 137/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1655 - val_loss: 0.1576\n",
            "Epoch 138/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1675 - val_loss: 0.1539\n",
            "Epoch 139/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1658 - val_loss: 0.1578\n",
            "Epoch 140/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1661 - val_loss: 0.1533\n",
            "Epoch 141/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1644 - val_loss: 0.1566\n",
            "Epoch 142/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1640 - val_loss: 0.1514\n",
            "Epoch 143/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1651 - val_loss: 0.1541\n",
            "Epoch 144/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1626 - val_loss: 0.1507\n",
            "Epoch 145/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1616 - val_loss: 0.1557\n",
            "Epoch 146/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1674 - val_loss: 0.1509\n",
            "Epoch 147/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1622 - val_loss: 0.1543\n",
            "Epoch 148/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1648 - val_loss: 0.1507\n",
            "Epoch 149/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1638 - val_loss: 0.1574\n",
            "Epoch 150/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1661 - val_loss: 0.1519\n",
            "Epoch 151/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1661 - val_loss: 0.1569\n",
            "Epoch 152/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1642 - val_loss: 0.1503\n",
            "Epoch 153/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1636 - val_loss: 0.1557\n",
            "Epoch 154/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1631 - val_loss: 0.1508\n",
            "Epoch 155/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1630 - val_loss: 0.1549\n",
            "Epoch 156/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1638 - val_loss: 0.1503\n",
            "Epoch 157/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1631 - val_loss: 0.1558\n",
            "Epoch 158/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1654 - val_loss: 0.1509\n",
            "Epoch 159/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1634 - val_loss: 0.1564\n",
            "Epoch 160/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1653 - val_loss: 0.1516\n",
            "Epoch 161/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1648 - val_loss: 0.1572\n",
            "Epoch 162/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1642 - val_loss: 0.1508\n",
            "Epoch 163/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1630 - val_loss: 0.1562\n",
            "Epoch 164/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1649 - val_loss: 0.1499\n",
            "Epoch 165/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1645 - val_loss: 0.1558\n",
            "Epoch 166/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1639 - val_loss: 0.1498\n",
            "Epoch 167/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1613 - val_loss: 0.1548\n",
            "Epoch 168/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1629 - val_loss: 0.1487\n",
            "Epoch 169/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1608 - val_loss: 0.1554\n",
            "Epoch 170/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1634 - val_loss: 0.1498\n",
            "Epoch 171/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1633 - val_loss: 0.1556\n",
            "Epoch 172/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1651 - val_loss: 0.1497\n",
            "Epoch 173/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1623 - val_loss: 0.1562\n",
            "Epoch 174/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1631 - val_loss: 0.1488\n",
            "Epoch 175/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1617 - val_loss: 0.1546\n",
            "Epoch 176/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1636 - val_loss: 0.1483\n",
            "Epoch 177/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1605 - val_loss: 0.1532\n",
            "Epoch 178/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1616 - val_loss: 0.1476\n",
            "Epoch 179/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1601 - val_loss: 0.1519\n",
            "Epoch 180/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1603 - val_loss: 0.1470\n",
            "Epoch 181/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1604 - val_loss: 0.1531\n",
            "Epoch 182/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1616 - val_loss: 0.1470\n",
            "Epoch 183/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1595 - val_loss: 0.1524\n",
            "Epoch 184/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1643 - val_loss: 0.1465\n",
            "Epoch 185/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1596 - val_loss: 0.1519\n",
            "Epoch 186/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1597 - val_loss: 0.1462\n",
            "Epoch 187/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1587 - val_loss: 0.1514\n",
            "Epoch 188/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1620 - val_loss: 0.1458\n",
            "Epoch 189/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1573 - val_loss: 0.1502\n",
            "Epoch 190/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1588 - val_loss: 0.1448\n",
            "Epoch 191/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1581 - val_loss: 0.1496\n",
            "Epoch 192/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1599 - val_loss: 0.1447\n",
            "Epoch 193/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1573 - val_loss: 0.1496\n",
            "Epoch 194/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1562 - val_loss: 0.1432\n",
            "Epoch 195/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1568 - val_loss: 0.1481\n",
            "Epoch 196/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1579 - val_loss: 0.1428\n",
            "Epoch 197/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1561 - val_loss: 0.1496\n",
            "Epoch 198/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1573 - val_loss: 0.1430\n",
            "Epoch 199/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1578 - val_loss: 0.1496\n",
            "Epoch 200/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1562 - val_loss: 0.1432\n",
            "Epoch 201/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1572 - val_loss: 0.1508\n",
            "Epoch 202/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1587 - val_loss: 0.1441\n",
            "Epoch 203/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1574 - val_loss: 0.1504\n",
            "Epoch 204/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1580 - val_loss: 0.1444\n",
            "Epoch 205/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1581 - val_loss: 0.1508\n",
            "Epoch 206/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1578 - val_loss: 0.1439\n",
            "Epoch 207/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1565 - val_loss: 0.1505\n",
            "Epoch 208/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1582 - val_loss: 0.1439\n",
            "Epoch 209/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1595 - val_loss: 0.1509\n",
            "Epoch 210/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1587 - val_loss: 0.1438\n",
            "Epoch 211/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1592 - val_loss: 0.1497\n",
            "Epoch 212/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1600 - val_loss: 0.1425\n",
            "Epoch 213/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1552 - val_loss: 0.1471\n",
            "Epoch 214/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1544 - val_loss: 0.1410\n",
            "Epoch 215/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1556 - val_loss: 0.1460\n",
            "Epoch 216/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1578 - val_loss: 0.1412\n",
            "Epoch 217/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1540 - val_loss: 0.1455\n",
            "Epoch 218/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1549 - val_loss: 0.1409\n",
            "Epoch 219/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1537 - val_loss: 0.1445\n",
            "Epoch 220/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1538 - val_loss: 0.1399\n",
            "Epoch 221/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1539 - val_loss: 0.1441\n",
            "Epoch 222/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1556 - val_loss: 0.1403\n",
            "Epoch 223/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1533 - val_loss: 0.1451\n",
            "Epoch 224/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1531 - val_loss: 0.1405\n",
            "Epoch 225/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1526 - val_loss: 0.1457\n",
            "Epoch 226/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1537 - val_loss: 0.1401\n",
            "Epoch 227/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1526 - val_loss: 0.1449\n",
            "Epoch 228/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1545 - val_loss: 0.1403\n",
            "Epoch 229/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1574 - val_loss: 0.1471\n",
            "Epoch 230/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1549 - val_loss: 0.1405\n",
            "Epoch 231/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1527 - val_loss: 0.1473\n",
            "Epoch 232/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1566 - val_loss: 0.1415\n",
            "Epoch 233/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1593 - val_loss: 0.1482\n",
            "Epoch 234/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1561 - val_loss: 0.1424\n",
            "Epoch 235/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1554 - val_loss: 0.1487\n",
            "Epoch 236/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1591 - val_loss: 0.1417\n",
            "Epoch 237/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1540 - val_loss: 0.1489\n",
            "Epoch 238/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1549 - val_loss: 0.1412\n",
            "Epoch 239/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1559 - val_loss: 0.1487\n",
            "Epoch 240/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1580 - val_loss: 0.1420\n",
            "Epoch 241/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1550 - val_loss: 0.1485\n",
            "Epoch 242/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1554 - val_loss: 0.1412\n",
            "Epoch 243/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1535 - val_loss: 0.1474\n",
            "Epoch 244/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1542 - val_loss: 0.1405\n",
            "Epoch 245/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1534 - val_loss: 0.1476\n",
            "Epoch 246/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1562 - val_loss: 0.1402\n",
            "Epoch 247/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1552 - val_loss: 0.1459\n",
            "Epoch 248/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1555 - val_loss: 0.1394\n",
            "Epoch 249/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1537 - val_loss: 0.1447\n",
            "Epoch 250/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1531 - val_loss: 0.1385\n",
            "Epoch 251/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1515 - val_loss: 0.1430\n",
            "Epoch 252/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1532 - val_loss: 0.1388\n",
            "Epoch 253/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1521 - val_loss: 0.1442\n",
            "Epoch 254/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1506 - val_loss: 0.1390\n",
            "Epoch 255/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1503 - val_loss: 0.1440\n",
            "Epoch 256/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1539 - val_loss: 0.1399\n",
            "Epoch 257/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1524 - val_loss: 0.1447\n",
            "Epoch 258/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1511 - val_loss: 0.1398\n",
            "Epoch 259/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1530 - val_loss: 0.1449\n",
            "Epoch 260/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1522 - val_loss: 0.1402\n",
            "Epoch 261/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1525 - val_loss: 0.1451\n",
            "Epoch 262/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1538 - val_loss: 0.1383\n",
            "Epoch 263/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1522 - val_loss: 0.1439\n",
            "Epoch 264/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1534 - val_loss: 0.1379\n",
            "Epoch 265/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1537 - val_loss: 0.1428\n",
            "Epoch 266/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1524 - val_loss: 0.1374\n",
            "Epoch 267/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1514 - val_loss: 0.1441\n",
            "Epoch 268/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1512 - val_loss: 0.1382\n",
            "Epoch 269/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1528 - val_loss: 0.1441\n",
            "Epoch 270/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1531 - val_loss: 0.1388\n",
            "Epoch 271/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1521 - val_loss: 0.1448\n",
            "Epoch 272/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1519 - val_loss: 0.1388\n",
            "Epoch 273/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1520 - val_loss: 0.1454\n",
            "Epoch 274/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1522 - val_loss: 0.1402\n",
            "Epoch 275/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1545 - val_loss: 0.1475\n",
            "Epoch 276/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1550 - val_loss: 0.1404\n",
            "Epoch 277/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1539 - val_loss: 0.1466\n",
            "Epoch 278/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1540 - val_loss: 0.1399\n",
            "Epoch 279/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1505 - val_loss: 0.1449\n",
            "Epoch 280/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1529 - val_loss: 0.1390\n",
            "Epoch 281/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1517 - val_loss: 0.1444\n",
            "Epoch 282/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1532 - val_loss: 0.1384\n",
            "Epoch 283/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1516 - val_loss: 0.1439\n",
            "Epoch 284/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1507 - val_loss: 0.1371\n",
            "Epoch 285/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1493 - val_loss: 0.1415\n",
            "Epoch 286/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1488 - val_loss: 0.1363\n",
            "Epoch 287/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1510 - val_loss: 0.1418\n",
            "Epoch 288/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1509 - val_loss: 0.1362\n",
            "Epoch 289/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1518 - val_loss: 0.1420\n",
            "Epoch 290/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1509 - val_loss: 0.1368\n",
            "Epoch 291/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1515 - val_loss: 0.1404\n",
            "Epoch 292/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1498 - val_loss: 0.1362\n",
            "Epoch 293/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1488 - val_loss: 0.1405\n",
            "Epoch 294/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1487 - val_loss: 0.1365\n",
            "Epoch 295/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1499 - val_loss: 0.1393\n",
            "Epoch 296/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1475 - val_loss: 0.1353\n",
            "Epoch 297/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1491 - val_loss: 0.1405\n",
            "Epoch 298/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1486 - val_loss: 0.1363\n",
            "Epoch 299/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1509 - val_loss: 0.1414\n",
            "Epoch 300/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1477 - val_loss: 0.1365\n",
            "Epoch 301/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1490 - val_loss: 0.1427\n",
            "Epoch 302/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1496 - val_loss: 0.1375\n",
            "Epoch 303/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1503 - val_loss: 0.1439\n",
            "Epoch 304/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1536 - val_loss: 0.1384\n",
            "Epoch 305/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1524 - val_loss: 0.1448\n",
            "Epoch 306/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1530 - val_loss: 0.1373\n",
            "Epoch 307/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1498 - val_loss: 0.1432\n",
            "Epoch 308/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1531 - val_loss: 0.1373\n",
            "Epoch 309/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1500 - val_loss: 0.1413\n",
            "Epoch 310/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1494 - val_loss: 0.1364\n",
            "Epoch 311/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1501 - val_loss: 0.1424\n",
            "Epoch 312/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1509 - val_loss: 0.1366\n",
            "Epoch 313/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1504 - val_loss: 0.1431\n",
            "Epoch 314/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1507 - val_loss: 0.1364\n",
            "Epoch 315/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1499 - val_loss: 0.1412\n",
            "Epoch 316/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1488 - val_loss: 0.1352\n",
            "Epoch 317/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1483 - val_loss: 0.1421\n",
            "Epoch 318/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1495 - val_loss: 0.1363\n",
            "Epoch 319/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1490 - val_loss: 0.1417\n",
            "Epoch 320/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1497 - val_loss: 0.1369\n",
            "Epoch 321/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1507 - val_loss: 0.1432\n",
            "Epoch 322/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1513 - val_loss: 0.1364\n",
            "Epoch 323/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1453 - val_loss: 0.1415\n",
            "Epoch 324/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1496 - val_loss: 0.1363\n",
            "Epoch 325/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1492 - val_loss: 0.1418\n",
            "Epoch 326/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1490 - val_loss: 0.1365\n",
            "Epoch 327/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1490 - val_loss: 0.1409\n",
            "Epoch 328/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1492 - val_loss: 0.1362\n",
            "Epoch 329/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1500 - val_loss: 0.1410\n",
            "Epoch 330/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1488 - val_loss: 0.1353\n",
            "Epoch 331/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1488 - val_loss: 0.1398\n",
            "Epoch 332/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1471 - val_loss: 0.1357\n",
            "Epoch 333/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1454 - val_loss: 0.1398\n",
            "Epoch 334/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1457 - val_loss: 0.1357\n",
            "Epoch 335/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1462 - val_loss: 0.1395\n",
            "Epoch 336/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1468 - val_loss: 0.1342\n",
            "Epoch 337/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1457 - val_loss: 0.1386\n",
            "Epoch 338/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1464 - val_loss: 0.1342\n",
            "Epoch 339/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1487 - val_loss: 0.1389\n",
            "Epoch 340/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1468 - val_loss: 0.1344\n",
            "Epoch 341/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1486 - val_loss: 0.1399\n",
            "Epoch 342/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1487 - val_loss: 0.1351\n",
            "Epoch 343/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1480 - val_loss: 0.1404\n",
            "Epoch 344/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1493 - val_loss: 0.1358\n",
            "Epoch 345/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1467 - val_loss: 0.1403\n",
            "Epoch 346/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1472 - val_loss: 0.1352\n",
            "Epoch 347/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1459 - val_loss: 0.1410\n",
            "Epoch 348/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1467 - val_loss: 0.1353\n",
            "Epoch 349/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1453 - val_loss: 0.1404\n",
            "Epoch 350/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1478 - val_loss: 0.1348\n",
            "Epoch 351/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1460 - val_loss: 0.1392\n",
            "Epoch 352/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1467 - val_loss: 0.1341\n",
            "Epoch 353/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1465 - val_loss: 0.1397\n",
            "Epoch 354/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1458 - val_loss: 0.1344\n",
            "Epoch 355/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1470 - val_loss: 0.1400\n",
            "Epoch 356/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1481 - val_loss: 0.1342\n",
            "Epoch 357/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1485 - val_loss: 0.1408\n",
            "Epoch 358/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1485 - val_loss: 0.1351\n",
            "Epoch 359/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1467 - val_loss: 0.1415\n",
            "Epoch 360/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1480 - val_loss: 0.1353\n",
            "Epoch 361/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1464 - val_loss: 0.1410\n",
            "Epoch 362/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1481 - val_loss: 0.1347\n",
            "Epoch 363/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1489 - val_loss: 0.1411\n",
            "Epoch 364/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1491 - val_loss: 0.1354\n",
            "Epoch 365/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1468 - val_loss: 0.1411\n",
            "Epoch 366/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1474 - val_loss: 0.1348\n",
            "Epoch 367/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1467 - val_loss: 0.1410\n",
            "Epoch 368/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1470 - val_loss: 0.1354\n",
            "Epoch 369/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1469 - val_loss: 0.1403\n",
            "Epoch 370/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1486 - val_loss: 0.1350\n",
            "Epoch 371/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1448 - val_loss: 0.1399\n",
            "Epoch 372/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1459 - val_loss: 0.1343\n",
            "Epoch 373/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1466 - val_loss: 0.1413\n",
            "Epoch 374/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1492 - val_loss: 0.1352\n",
            "Epoch 375/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1474 - val_loss: 0.1402\n",
            "Epoch 376/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1469 - val_loss: 0.1347\n",
            "Epoch 377/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1441 - val_loss: 0.1387\n",
            "Epoch 378/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1483 - val_loss: 0.1343\n",
            "Epoch 379/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1445 - val_loss: 0.1402\n",
            "Epoch 380/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1483 - val_loss: 0.1352\n",
            "Epoch 381/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1458 - val_loss: 0.1412\n",
            "Epoch 382/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1480 - val_loss: 0.1350\n",
            "Epoch 383/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1455 - val_loss: 0.1405\n",
            "Epoch 384/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1468 - val_loss: 0.1347\n",
            "Epoch 385/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1450 - val_loss: 0.1404\n",
            "Epoch 386/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1458 - val_loss: 0.1344\n",
            "Epoch 387/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1459 - val_loss: 0.1401\n",
            "Epoch 388/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1436 - val_loss: 0.1340\n",
            "Epoch 389/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1463 - val_loss: 0.1393\n",
            "Epoch 390/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1442 - val_loss: 0.1332\n",
            "Epoch 391/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1428 - val_loss: 0.1380\n",
            "Epoch 392/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1472 - val_loss: 0.1339\n",
            "Epoch 393/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1456 - val_loss: 0.1400\n",
            "Epoch 394/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1457 - val_loss: 0.1343\n",
            "Epoch 395/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1453 - val_loss: 0.1386\n",
            "Epoch 396/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1453 - val_loss: 0.1336\n",
            "Epoch 397/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1432 - val_loss: 0.1381\n",
            "Epoch 398/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1450 - val_loss: 0.1332\n",
            "Epoch 399/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1445 - val_loss: 0.1373\n",
            "Epoch 400/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1433 - val_loss: 0.1329\n",
            "Epoch 401/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1413 - val_loss: 0.1378\n",
            "Epoch 402/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1466 - val_loss: 0.1333\n",
            "Epoch 403/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1416 - val_loss: 0.1384\n",
            "Epoch 404/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1455 - val_loss: 0.1333\n",
            "Epoch 405/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1456 - val_loss: 0.1393\n",
            "Epoch 406/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1462 - val_loss: 0.1341\n",
            "Epoch 407/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1438 - val_loss: 0.1400\n",
            "Epoch 408/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1462 - val_loss: 0.1339\n",
            "Epoch 409/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1458 - val_loss: 0.1405\n",
            "Epoch 410/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1479 - val_loss: 0.1343\n",
            "Epoch 411/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1470 - val_loss: 0.1402\n",
            "Epoch 412/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1451 - val_loss: 0.1331\n",
            "Epoch 413/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1439 - val_loss: 0.1383\n",
            "Epoch 414/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1440 - val_loss: 0.1332\n",
            "Epoch 415/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1444 - val_loss: 0.1384\n",
            "Epoch 416/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1426 - val_loss: 0.1320\n",
            "Epoch 417/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1425 - val_loss: 0.1382\n",
            "Epoch 418/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1453 - val_loss: 0.1320\n",
            "Epoch 419/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1426 - val_loss: 0.1378\n",
            "Epoch 420/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1440 - val_loss: 0.1329\n",
            "Epoch 421/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1447 - val_loss: 0.1382\n",
            "Epoch 422/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1435 - val_loss: 0.1332\n",
            "Epoch 423/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1438 - val_loss: 0.1381\n",
            "Epoch 424/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1445 - val_loss: 0.1325\n",
            "Epoch 425/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1415 - val_loss: 0.1385\n",
            "Epoch 426/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1434 - val_loss: 0.1329\n",
            "Epoch 427/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1455 - val_loss: 0.1391\n",
            "Epoch 428/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1434 - val_loss: 0.1328\n",
            "Epoch 429/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1455 - val_loss: 0.1404\n",
            "Epoch 430/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1455 - val_loss: 0.1334\n",
            "Epoch 431/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1447 - val_loss: 0.1406\n",
            "Epoch 432/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1451 - val_loss: 0.1328\n",
            "Epoch 433/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1430 - val_loss: 0.1392\n",
            "Epoch 434/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1440 - val_loss: 0.1329\n",
            "Epoch 435/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1452 - val_loss: 0.1394\n",
            "Epoch 436/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1445 - val_loss: 0.1324\n",
            "Epoch 437/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1413 - val_loss: 0.1378\n",
            "Epoch 438/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1462 - val_loss: 0.1320\n",
            "Epoch 439/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1419 - val_loss: 0.1368\n",
            "Epoch 440/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1429 - val_loss: 0.1321\n",
            "Epoch 441/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1421 - val_loss: 0.1361\n",
            "Epoch 442/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1430 - val_loss: 0.1330\n",
            "Epoch 443/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1432 - val_loss: 0.1364\n",
            "Epoch 444/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1441 - val_loss: 0.1319\n",
            "Epoch 445/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1432 - val_loss: 0.1358\n",
            "Epoch 446/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1408 - val_loss: 0.1321\n",
            "Epoch 447/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1414 - val_loss: 0.1359\n",
            "Epoch 448/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1440 - val_loss: 0.1323\n",
            "Epoch 449/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1398 - val_loss: 0.1356\n",
            "Epoch 450/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1411 - val_loss: 0.1324\n",
            "Epoch 451/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1418 - val_loss: 0.1357\n",
            "Epoch 452/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1413 - val_loss: 0.1317\n",
            "Epoch 453/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1392 - val_loss: 0.1344\n",
            "Epoch 454/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1419 - val_loss: 0.1312\n",
            "Epoch 455/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1417 - val_loss: 0.1349\n",
            "Epoch 456/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1413 - val_loss: 0.1317\n",
            "Epoch 457/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1415 - val_loss: 0.1371\n",
            "Epoch 458/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1412 - val_loss: 0.1328\n",
            "Epoch 459/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1432 - val_loss: 0.1364\n",
            "Epoch 460/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1431 - val_loss: 0.1328\n",
            "Epoch 461/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1444 - val_loss: 0.1377\n",
            "Epoch 462/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1442 - val_loss: 0.1318\n",
            "Epoch 463/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1400 - val_loss: 0.1369\n",
            "Epoch 464/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1438 - val_loss: 0.1323\n",
            "Epoch 465/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1428 - val_loss: 0.1374\n",
            "Epoch 466/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1431 - val_loss: 0.1325\n",
            "Epoch 467/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1428 - val_loss: 0.1378\n",
            "Epoch 468/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1415 - val_loss: 0.1326\n",
            "Epoch 469/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1417 - val_loss: 0.1388\n",
            "Epoch 470/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1427 - val_loss: 0.1324\n",
            "Epoch 471/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1432 - val_loss: 0.1385\n",
            "Epoch 472/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1446 - val_loss: 0.1326\n",
            "Epoch 473/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1434 - val_loss: 0.1384\n",
            "Epoch 474/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1444 - val_loss: 0.1322\n",
            "Epoch 475/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1430 - val_loss: 0.1378\n",
            "Epoch 476/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1432 - val_loss: 0.1327\n",
            "Epoch 477/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1421 - val_loss: 0.1375\n",
            "Epoch 478/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1417 - val_loss: 0.1318\n",
            "Epoch 479/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1417 - val_loss: 0.1368\n",
            "Epoch 480/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1431 - val_loss: 0.1325\n",
            "Epoch 481/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1424 - val_loss: 0.1374\n",
            "Epoch 482/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1439 - val_loss: 0.1318\n",
            "Epoch 483/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1400 - val_loss: 0.1368\n",
            "Epoch 484/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1406 - val_loss: 0.1316\n",
            "Epoch 485/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1420 - val_loss: 0.1364\n",
            "Epoch 486/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1421 - val_loss: 0.1326\n",
            "Epoch 487/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1411 - val_loss: 0.1368\n",
            "Epoch 488/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1410 - val_loss: 0.1317\n",
            "Epoch 489/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1401 - val_loss: 0.1358\n",
            "Epoch 490/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1401 - val_loss: 0.1315\n",
            "Epoch 491/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1415 - val_loss: 0.1357\n",
            "Epoch 492/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1409 - val_loss: 0.1316\n",
            "Epoch 493/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1414 - val_loss: 0.1349\n",
            "Epoch 494/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1419 - val_loss: 0.1310\n",
            "Epoch 495/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1402 - val_loss: 0.1351\n",
            "Epoch 496/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1411 - val_loss: 0.1316\n",
            "Epoch 497/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1396 - val_loss: 0.1355\n",
            "Epoch 498/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1401 - val_loss: 0.1317\n",
            "Epoch 499/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1413 - val_loss: 0.1351\n",
            "Epoch 500/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1401 - val_loss: 0.1306\n",
            "Epoch 501/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1409 - val_loss: 0.1348\n",
            "Epoch 502/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1399 - val_loss: 0.1305\n",
            "Epoch 503/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1396 - val_loss: 0.1352\n",
            "Epoch 504/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1404 - val_loss: 0.1311\n",
            "Epoch 505/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1419 - val_loss: 0.1360\n",
            "Epoch 506/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1398 - val_loss: 0.1311\n",
            "Epoch 507/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1408 - val_loss: 0.1367\n",
            "Epoch 508/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1403 - val_loss: 0.1308\n",
            "Epoch 509/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1398 - val_loss: 0.1361\n",
            "Epoch 510/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1394 - val_loss: 0.1304\n",
            "Epoch 511/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1397 - val_loss: 0.1351\n",
            "Epoch 512/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1380 - val_loss: 0.1296\n",
            "Epoch 513/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1395 - val_loss: 0.1346\n",
            "Epoch 514/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1381 - val_loss: 0.1302\n",
            "Epoch 515/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1386 - val_loss: 0.1348\n",
            "Epoch 516/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1406 - val_loss: 0.1301\n",
            "Epoch 517/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1383 - val_loss: 0.1356\n",
            "Epoch 518/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1401 - val_loss: 0.1306\n",
            "Epoch 519/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1393 - val_loss: 0.1360\n",
            "Epoch 520/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1404 - val_loss: 0.1310\n",
            "Epoch 521/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1413 - val_loss: 0.1384\n",
            "Epoch 522/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1417 - val_loss: 0.1314\n",
            "Epoch 523/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1404 - val_loss: 0.1398\n",
            "Epoch 524/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1438 - val_loss: 0.1325\n",
            "Epoch 525/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1421 - val_loss: 0.1383\n",
            "Epoch 526/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1413 - val_loss: 0.1309\n",
            "Epoch 527/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1411 - val_loss: 0.1370\n",
            "Epoch 528/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1418 - val_loss: 0.1304\n",
            "Epoch 529/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1386 - val_loss: 0.1353\n",
            "Epoch 530/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1401 - val_loss: 0.1298\n",
            "Epoch 531/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1393 - val_loss: 0.1355\n",
            "Epoch 532/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1406 - val_loss: 0.1297\n",
            "Epoch 533/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1416 - val_loss: 0.1344\n",
            "Epoch 534/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1397 - val_loss: 0.1289\n",
            "Epoch 535/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1376 - val_loss: 0.1326\n",
            "Epoch 536/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1365 - val_loss: 0.1283\n",
            "Epoch 537/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1375 - val_loss: 0.1325\n",
            "Epoch 538/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1384 - val_loss: 0.1278\n",
            "Epoch 539/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1355 - val_loss: 0.1314\n",
            "Epoch 540/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1360 - val_loss: 0.1283\n",
            "Epoch 541/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1360 - val_loss: 0.1322\n",
            "Epoch 542/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1361 - val_loss: 0.1293\n",
            "Epoch 543/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1375 - val_loss: 0.1333\n",
            "Epoch 544/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1379 - val_loss: 0.1293\n",
            "Epoch 545/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1379 - val_loss: 0.1337\n",
            "Epoch 546/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1389 - val_loss: 0.1297\n",
            "Epoch 547/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1401 - val_loss: 0.1333\n",
            "Epoch 548/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1396 - val_loss: 0.1296\n",
            "Epoch 549/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1366 - val_loss: 0.1338\n",
            "Epoch 550/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1386 - val_loss: 0.1301\n",
            "Epoch 551/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1360 - val_loss: 0.1332\n",
            "Epoch 552/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1377 - val_loss: 0.1305\n",
            "Epoch 553/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1379 - val_loss: 0.1335\n",
            "Epoch 554/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1383 - val_loss: 0.1310\n",
            "Epoch 555/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1376 - val_loss: 0.1330\n",
            "Epoch 556/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1388 - val_loss: 0.1314\n",
            "Epoch 557/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1396 - val_loss: 0.1343\n",
            "Epoch 558/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1414 - val_loss: 0.1319\n",
            "Epoch 559/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1382 - val_loss: 0.1350\n",
            "Epoch 560/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1410 - val_loss: 0.1322\n",
            "Epoch 561/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1401 - val_loss: 0.1348\n",
            "Epoch 562/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1412 - val_loss: 0.1314\n",
            "Epoch 563/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1395 - val_loss: 0.1339\n",
            "Epoch 564/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1385 - val_loss: 0.1307\n",
            "Epoch 565/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1365 - val_loss: 0.1335\n",
            "Epoch 566/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1386 - val_loss: 0.1303\n",
            "Epoch 567/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1379 - val_loss: 0.1342\n",
            "Epoch 568/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1370 - val_loss: 0.1308\n",
            "Epoch 569/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1400 - val_loss: 0.1348\n",
            "Epoch 570/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1394 - val_loss: 0.1293\n",
            "Epoch 571/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1386 - val_loss: 0.1358\n",
            "Epoch 572/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1384 - val_loss: 0.1302\n",
            "Epoch 573/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1382 - val_loss: 0.1358\n",
            "Epoch 574/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1397 - val_loss: 0.1305\n",
            "Epoch 575/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1386 - val_loss: 0.1365\n",
            "Epoch 576/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1429 - val_loss: 0.1315\n",
            "Epoch 577/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1391 - val_loss: 0.1372\n",
            "Epoch 578/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1411 - val_loss: 0.1315\n",
            "Epoch 579/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1393 - val_loss: 0.1366\n",
            "Epoch 580/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1383 - val_loss: 0.1301\n",
            "Epoch 581/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1379 - val_loss: 0.1356\n",
            "Epoch 582/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1395 - val_loss: 0.1300\n",
            "Epoch 583/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1382 - val_loss: 0.1346\n",
            "Epoch 584/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1370 - val_loss: 0.1298\n",
            "Epoch 585/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1389 - val_loss: 0.1362\n",
            "Epoch 586/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1410 - val_loss: 0.1301\n",
            "Epoch 587/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1383 - val_loss: 0.1359\n",
            "Epoch 588/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1401 - val_loss: 0.1297\n",
            "Epoch 589/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1370 - val_loss: 0.1342\n",
            "Epoch 590/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1384 - val_loss: 0.1290\n",
            "Epoch 591/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1367 - val_loss: 0.1334\n",
            "Epoch 592/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1383 - val_loss: 0.1290\n",
            "Epoch 593/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1351 - val_loss: 0.1324\n",
            "Epoch 594/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1354 - val_loss: 0.1282\n",
            "Epoch 595/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1366 - val_loss: 0.1323\n",
            "Epoch 596/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1372 - val_loss: 0.1277\n",
            "Epoch 597/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1331 - val_loss: 0.1308\n",
            "Epoch 598/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1355 - val_loss: 0.1276\n",
            "Epoch 599/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1330 - val_loss: 0.1314\n",
            "Epoch 600/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1374 - val_loss: 0.1278\n",
            "Epoch 601/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1351 - val_loss: 0.1306\n",
            "Epoch 602/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1352 - val_loss: 0.1273\n",
            "Epoch 603/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1363 - val_loss: 0.1315\n",
            "Epoch 604/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1356 - val_loss: 0.1273\n",
            "Epoch 605/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1341 - val_loss: 0.1320\n",
            "Epoch 606/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1372 - val_loss: 0.1269\n",
            "Epoch 607/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1367 - val_loss: 0.1325\n",
            "Epoch 608/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1356 - val_loss: 0.1281\n",
            "Epoch 609/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1363 - val_loss: 0.1332\n",
            "Epoch 610/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1381 - val_loss: 0.1280\n",
            "Epoch 611/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1371 - val_loss: 0.1332\n",
            "Epoch 612/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1367 - val_loss: 0.1292\n",
            "Epoch 613/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1378 - val_loss: 0.1341\n",
            "Epoch 614/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1384 - val_loss: 0.1287\n",
            "Epoch 615/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1372 - val_loss: 0.1341\n",
            "Epoch 616/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1377 - val_loss: 0.1294\n",
            "Epoch 617/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1377 - val_loss: 0.1344\n",
            "Epoch 618/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1387 - val_loss: 0.1297\n",
            "Epoch 619/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1374 - val_loss: 0.1346\n",
            "Epoch 620/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1404 - val_loss: 0.1292\n",
            "Epoch 621/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1364 - val_loss: 0.1325\n",
            "Epoch 622/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1383 - val_loss: 0.1290\n",
            "Epoch 623/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1366 - val_loss: 0.1333\n",
            "Epoch 624/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1363 - val_loss: 0.1285\n",
            "Epoch 625/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1362 - val_loss: 0.1332\n",
            "Epoch 626/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1374 - val_loss: 0.1288\n",
            "Epoch 627/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1383 - val_loss: 0.1328\n",
            "Epoch 628/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1363 - val_loss: 0.1284\n",
            "Epoch 629/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1371 - val_loss: 0.1327\n",
            "Epoch 630/1000\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.1373 - val_loss: 0.1280\n",
            "Epoch 631/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1353 - val_loss: 0.1326\n",
            "Epoch 632/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1346 - val_loss: 0.1268\n",
            "Epoch 633/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1337 - val_loss: 0.1317\n",
            "Epoch 634/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1345 - val_loss: 0.1262\n",
            "Epoch 635/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1347 - val_loss: 0.1308\n",
            "Epoch 636/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1336 - val_loss: 0.1267\n",
            "Epoch 637/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1338 - val_loss: 0.1313\n",
            "Epoch 638/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1362 - val_loss: 0.1263\n",
            "Epoch 639/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1340 - val_loss: 0.1313\n",
            "Epoch 640/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1354 - val_loss: 0.1263\n",
            "Epoch 641/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1351 - val_loss: 0.1320\n",
            "Epoch 642/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1364 - val_loss: 0.1264\n",
            "Epoch 643/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1343 - val_loss: 0.1319\n",
            "Epoch 644/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1340 - val_loss: 0.1268\n",
            "Epoch 645/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1345 - val_loss: 0.1337\n",
            "Epoch 646/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1356 - val_loss: 0.1273\n",
            "Epoch 647/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1368 - val_loss: 0.1331\n",
            "Epoch 648/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1363 - val_loss: 0.1271\n",
            "Epoch 649/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1359 - val_loss: 0.1324\n",
            "Epoch 650/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1350 - val_loss: 0.1264\n",
            "Epoch 651/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1336 - val_loss: 0.1309\n",
            "Epoch 652/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1356 - val_loss: 0.1263\n",
            "Epoch 653/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1341 - val_loss: 0.1310\n",
            "Epoch 654/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1348 - val_loss: 0.1264\n",
            "Epoch 655/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1333 - val_loss: 0.1308\n",
            "Epoch 656/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1332 - val_loss: 0.1264\n",
            "Epoch 657/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1355 - val_loss: 0.1308\n",
            "Epoch 658/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1351 - val_loss: 0.1264\n",
            "Epoch 659/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1354 - val_loss: 0.1316\n",
            "Epoch 660/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1340 - val_loss: 0.1269\n",
            "Epoch 661/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1343 - val_loss: 0.1315\n",
            "Epoch 662/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1342 - val_loss: 0.1261\n",
            "Epoch 663/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1357 - val_loss: 0.1307\n",
            "Epoch 664/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1347 - val_loss: 0.1263\n",
            "Epoch 665/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1327 - val_loss: 0.1307\n",
            "Epoch 666/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1335 - val_loss: 0.1263\n",
            "Epoch 667/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1316 - val_loss: 0.1310\n",
            "Epoch 668/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1357 - val_loss: 0.1259\n",
            "Epoch 669/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1337 - val_loss: 0.1305\n",
            "Epoch 670/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1356 - val_loss: 0.1260\n",
            "Epoch 671/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1329 - val_loss: 0.1314\n",
            "Epoch 672/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1335 - val_loss: 0.1267\n",
            "Epoch 673/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1338 - val_loss: 0.1326\n",
            "Epoch 674/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1356 - val_loss: 0.1271\n",
            "Epoch 675/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1360 - val_loss: 0.1328\n",
            "Epoch 676/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1362 - val_loss: 0.1270\n",
            "Epoch 677/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1318 - val_loss: 0.1328\n",
            "Epoch 678/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1365 - val_loss: 0.1268\n",
            "Epoch 679/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1340 - val_loss: 0.1311\n",
            "Epoch 680/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1346 - val_loss: 0.1261\n",
            "Epoch 681/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1352 - val_loss: 0.1314\n",
            "Epoch 682/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1336 - val_loss: 0.1258\n",
            "Epoch 683/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1350 - val_loss: 0.1311\n",
            "Epoch 684/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1358 - val_loss: 0.1256\n",
            "Epoch 685/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1337 - val_loss: 0.1309\n",
            "Epoch 686/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1346 - val_loss: 0.1255\n",
            "Epoch 687/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1331 - val_loss: 0.1296\n",
            "Epoch 688/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1346 - val_loss: 0.1257\n",
            "Epoch 689/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1330 - val_loss: 0.1302\n",
            "Epoch 690/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1334 - val_loss: 0.1262\n",
            "Epoch 691/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1339 - val_loss: 0.1311\n",
            "Epoch 692/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1330 - val_loss: 0.1267\n",
            "Epoch 693/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1340 - val_loss: 0.1316\n",
            "Epoch 694/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1342 - val_loss: 0.1272\n",
            "Epoch 695/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1334 - val_loss: 0.1313\n",
            "Epoch 696/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1345 - val_loss: 0.1275\n",
            "Epoch 697/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1334 - val_loss: 0.1309\n",
            "Epoch 698/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1354 - val_loss: 0.1283\n",
            "Epoch 699/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1349 - val_loss: 0.1312\n",
            "Epoch 700/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1350 - val_loss: 0.1307\n",
            "Epoch 701/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1361 - val_loss: 0.1319\n",
            "Epoch 702/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1369 - val_loss: 0.1319\n",
            "Epoch 703/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1356 - val_loss: 0.1320\n",
            "Epoch 704/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1360 - val_loss: 0.1322\n",
            "Epoch 705/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1336 - val_loss: 0.1302\n",
            "Epoch 706/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1353 - val_loss: 0.1323\n",
            "Epoch 707/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1345 - val_loss: 0.1297\n",
            "Epoch 708/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1357 - val_loss: 0.1327\n",
            "Epoch 709/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1348 - val_loss: 0.1303\n",
            "Epoch 710/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1344 - val_loss: 0.1323\n",
            "Epoch 711/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1357 - val_loss: 0.1301\n",
            "Epoch 712/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1359 - val_loss: 0.1324\n",
            "Epoch 713/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1347 - val_loss: 0.1288\n",
            "Epoch 714/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1345 - val_loss: 0.1304\n",
            "Epoch 715/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1323 - val_loss: 0.1273\n",
            "Epoch 716/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1321 - val_loss: 0.1285\n",
            "Epoch 717/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1326 - val_loss: 0.1266\n",
            "Epoch 718/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1332 - val_loss: 0.1291\n",
            "Epoch 719/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1340 - val_loss: 0.1261\n",
            "Epoch 720/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1321 - val_loss: 0.1285\n",
            "Epoch 721/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1321 - val_loss: 0.1251\n",
            "Epoch 722/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1321 - val_loss: 0.1284\n",
            "Epoch 723/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1332 - val_loss: 0.1245\n",
            "Epoch 724/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1309 - val_loss: 0.1273\n",
            "Epoch 725/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1300 - val_loss: 0.1245\n",
            "Epoch 726/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1313 - val_loss: 0.1282\n",
            "Epoch 727/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1298 - val_loss: 0.1244\n",
            "Epoch 728/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1307 - val_loss: 0.1279\n",
            "Epoch 729/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1302 - val_loss: 0.1243\n",
            "Epoch 730/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1289 - val_loss: 0.1272\n",
            "Epoch 731/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1304 - val_loss: 0.1239\n",
            "Epoch 732/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1313 - val_loss: 0.1282\n",
            "Epoch 733/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1333 - val_loss: 0.1247\n",
            "Epoch 734/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1302 - val_loss: 0.1290\n",
            "Epoch 735/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1288 - val_loss: 0.1247\n",
            "Epoch 736/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1324 - val_loss: 0.1299\n",
            "Epoch 737/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1346 - val_loss: 0.1249\n",
            "Epoch 738/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1342 - val_loss: 0.1301\n",
            "Epoch 739/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1324 - val_loss: 0.1248\n",
            "Epoch 740/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1325 - val_loss: 0.1304\n",
            "Epoch 741/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1310 - val_loss: 0.1242\n",
            "Epoch 742/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1315 - val_loss: 0.1314\n",
            "Epoch 743/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1322 - val_loss: 0.1245\n",
            "Epoch 744/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1341 - val_loss: 0.1317\n",
            "Epoch 745/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1345 - val_loss: 0.1241\n",
            "Epoch 746/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1305 - val_loss: 0.1300\n",
            "Epoch 747/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1298 - val_loss: 0.1234\n",
            "Epoch 748/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1324 - val_loss: 0.1289\n",
            "Epoch 749/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1318 - val_loss: 0.1230\n",
            "Epoch 750/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1300 - val_loss: 0.1289\n",
            "Epoch 751/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1318 - val_loss: 0.1240\n",
            "Epoch 752/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1301 - val_loss: 0.1290\n",
            "Epoch 753/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1324 - val_loss: 0.1236\n",
            "Epoch 754/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1306 - val_loss: 0.1295\n",
            "Epoch 755/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1327 - val_loss: 0.1239\n",
            "Epoch 756/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1338 - val_loss: 0.1310\n",
            "Epoch 757/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1315 - val_loss: 0.1242\n",
            "Epoch 758/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1320 - val_loss: 0.1323\n",
            "Epoch 759/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1352 - val_loss: 0.1247\n",
            "Epoch 760/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1330 - val_loss: 0.1333\n",
            "Epoch 761/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1352 - val_loss: 0.1254\n",
            "Epoch 762/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1334 - val_loss: 0.1334\n",
            "Epoch 763/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1344 - val_loss: 0.1249\n",
            "Epoch 764/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1330 - val_loss: 0.1317\n",
            "Epoch 765/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1335 - val_loss: 0.1253\n",
            "Epoch 766/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1337 - val_loss: 0.1316\n",
            "Epoch 767/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1345 - val_loss: 0.1255\n",
            "Epoch 768/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1341 - val_loss: 0.1325\n",
            "Epoch 769/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1344 - val_loss: 0.1266\n",
            "Epoch 770/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1331 - val_loss: 0.1312\n",
            "Epoch 771/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1328 - val_loss: 0.1251\n",
            "Epoch 772/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1312 - val_loss: 0.1294\n",
            "Epoch 773/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1332 - val_loss: 0.1246\n",
            "Epoch 774/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1309 - val_loss: 0.1299\n",
            "Epoch 775/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1318 - val_loss: 0.1244\n",
            "Epoch 776/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1325 - val_loss: 0.1290\n",
            "Epoch 777/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1326 - val_loss: 0.1252\n",
            "Epoch 778/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1316 - val_loss: 0.1292\n",
            "Epoch 779/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1312 - val_loss: 0.1244\n",
            "Epoch 780/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1296 - val_loss: 0.1278\n",
            "Epoch 781/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1313 - val_loss: 0.1247\n",
            "Epoch 782/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1298 - val_loss: 0.1284\n",
            "Epoch 783/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1302 - val_loss: 0.1246\n",
            "Epoch 784/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1301 - val_loss: 0.1284\n",
            "Epoch 785/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1309 - val_loss: 0.1246\n",
            "Epoch 786/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1297 - val_loss: 0.1271\n",
            "Epoch 787/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1306 - val_loss: 0.1238\n",
            "Epoch 788/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1309 - val_loss: 0.1275\n",
            "Epoch 789/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1291 - val_loss: 0.1232\n",
            "Epoch 790/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1298 - val_loss: 0.1258\n",
            "Epoch 791/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1284 - val_loss: 0.1230\n",
            "Epoch 792/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1295 - val_loss: 0.1257\n",
            "Epoch 793/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1297 - val_loss: 0.1228\n",
            "Epoch 794/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1289 - val_loss: 0.1250\n",
            "Epoch 795/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1294 - val_loss: 0.1230\n",
            "Epoch 796/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1312 - val_loss: 0.1252\n",
            "Epoch 797/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1290 - val_loss: 0.1232\n",
            "Epoch 798/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1270 - val_loss: 0.1255\n",
            "Epoch 799/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1300 - val_loss: 0.1235\n",
            "Epoch 800/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1279 - val_loss: 0.1255\n",
            "Epoch 801/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1288 - val_loss: 0.1243\n",
            "Epoch 802/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1307 - val_loss: 0.1265\n",
            "Epoch 803/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1294 - val_loss: 0.1253\n",
            "Epoch 804/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1295 - val_loss: 0.1279\n",
            "Epoch 805/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1300 - val_loss: 0.1260\n",
            "Epoch 806/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1302 - val_loss: 0.1292\n",
            "Epoch 807/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1324 - val_loss: 0.1264\n",
            "Epoch 808/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1326 - val_loss: 0.1298\n",
            "Epoch 809/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1328 - val_loss: 0.1258\n",
            "Epoch 810/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1311 - val_loss: 0.1293\n",
            "Epoch 811/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1316 - val_loss: 0.1259\n",
            "Epoch 812/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1309 - val_loss: 0.1302\n",
            "Epoch 813/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1322 - val_loss: 0.1266\n",
            "Epoch 814/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1326 - val_loss: 0.1306\n",
            "Epoch 815/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1328 - val_loss: 0.1259\n",
            "Epoch 816/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1304 - val_loss: 0.1308\n",
            "Epoch 817/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1338 - val_loss: 0.1256\n",
            "Epoch 818/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1324 - val_loss: 0.1284\n",
            "Epoch 819/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1314 - val_loss: 0.1244\n",
            "Epoch 820/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1306 - val_loss: 0.1282\n",
            "Epoch 821/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1311 - val_loss: 0.1236\n",
            "Epoch 822/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1281 - val_loss: 0.1274\n",
            "Epoch 823/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1294 - val_loss: 0.1237\n",
            "Epoch 824/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1298 - val_loss: 0.1270\n",
            "Epoch 825/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1271 - val_loss: 0.1225\n",
            "Epoch 826/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1273 - val_loss: 0.1261\n",
            "Epoch 827/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1296 - val_loss: 0.1224\n",
            "Epoch 828/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1294 - val_loss: 0.1257\n",
            "Epoch 829/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1277 - val_loss: 0.1219\n",
            "Epoch 830/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1275 - val_loss: 0.1267\n",
            "Epoch 831/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1294 - val_loss: 0.1224\n",
            "Epoch 832/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1297 - val_loss: 0.1268\n",
            "Epoch 833/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1278 - val_loss: 0.1226\n",
            "Epoch 834/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1275 - val_loss: 0.1262\n",
            "Epoch 835/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1288 - val_loss: 0.1220\n",
            "Epoch 836/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1294 - val_loss: 0.1258\n",
            "Epoch 837/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1292 - val_loss: 0.1222\n",
            "Epoch 838/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1277 - val_loss: 0.1258\n",
            "Epoch 839/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1260 - val_loss: 0.1227\n",
            "Epoch 840/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1263 - val_loss: 0.1256\n",
            "Epoch 841/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1277 - val_loss: 0.1233\n",
            "Epoch 842/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1292 - val_loss: 0.1264\n",
            "Epoch 843/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1285 - val_loss: 0.1240\n",
            "Epoch 844/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1283 - val_loss: 0.1260\n",
            "Epoch 845/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1302 - val_loss: 0.1258\n",
            "Epoch 846/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1291 - val_loss: 0.1268\n",
            "Epoch 847/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1298 - val_loss: 0.1236\n",
            "Epoch 848/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1278 - val_loss: 0.1246\n",
            "Epoch 849/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1260 - val_loss: 0.1231\n",
            "Epoch 850/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1265 - val_loss: 0.1239\n",
            "Epoch 851/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1266 - val_loss: 0.1233\n",
            "Epoch 852/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1270 - val_loss: 0.1245\n",
            "Epoch 853/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1285 - val_loss: 0.1229\n",
            "Epoch 854/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1252 - val_loss: 0.1234\n",
            "Epoch 855/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1270 - val_loss: 0.1231\n",
            "Epoch 856/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1265 - val_loss: 0.1237\n",
            "Epoch 857/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1278 - val_loss: 0.1236\n",
            "Epoch 858/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1271 - val_loss: 0.1242\n",
            "Epoch 859/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1295 - val_loss: 0.1241\n",
            "Epoch 860/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1288 - val_loss: 0.1239\n",
            "Epoch 861/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1266 - val_loss: 0.1243\n",
            "Epoch 862/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1280 - val_loss: 0.1245\n",
            "Epoch 863/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1284 - val_loss: 0.1243\n",
            "Epoch 864/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1264 - val_loss: 0.1238\n",
            "Epoch 865/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1261 - val_loss: 0.1236\n",
            "Epoch 866/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1289 - val_loss: 0.1244\n",
            "Epoch 867/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1279 - val_loss: 0.1248\n",
            "Epoch 868/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1258 - val_loss: 0.1249\n",
            "Epoch 869/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1289 - val_loss: 0.1234\n",
            "Epoch 870/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1262 - val_loss: 0.1228\n",
            "Epoch 871/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1267 - val_loss: 0.1223\n",
            "Epoch 872/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1247 - val_loss: 0.1219\n",
            "Epoch 873/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1253 - val_loss: 0.1217\n",
            "Epoch 874/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1271 - val_loss: 0.1228\n",
            "Epoch 875/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1268 - val_loss: 0.1218\n",
            "Epoch 876/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1257 - val_loss: 0.1220\n",
            "Epoch 877/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1284 - val_loss: 0.1229\n",
            "Epoch 878/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1271 - val_loss: 0.1224\n",
            "Epoch 879/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1248 - val_loss: 0.1226\n",
            "Epoch 880/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1255 - val_loss: 0.1220\n",
            "Epoch 881/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1258 - val_loss: 0.1236\n",
            "Epoch 882/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1266 - val_loss: 0.1224\n",
            "Epoch 883/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1266 - val_loss: 0.1242\n",
            "Epoch 884/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1274 - val_loss: 0.1228\n",
            "Epoch 885/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1278 - val_loss: 0.1260\n",
            "Epoch 886/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1295 - val_loss: 0.1237\n",
            "Epoch 887/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1277 - val_loss: 0.1260\n",
            "Epoch 888/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1251 - val_loss: 0.1227\n",
            "Epoch 889/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1283 - val_loss: 0.1269\n",
            "Epoch 890/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1287 - val_loss: 0.1220\n",
            "Epoch 891/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1270 - val_loss: 0.1266\n",
            "Epoch 892/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1279 - val_loss: 0.1203\n",
            "Epoch 893/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1271 - val_loss: 0.1264\n",
            "Epoch 894/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1256 - val_loss: 0.1202\n",
            "Epoch 895/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1278 - val_loss: 0.1263\n",
            "Epoch 896/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1270 - val_loss: 0.1197\n",
            "Epoch 897/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1239 - val_loss: 0.1260\n",
            "Epoch 898/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1272 - val_loss: 0.1192\n",
            "Epoch 899/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1249 - val_loss: 0.1273\n",
            "Epoch 900/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1267 - val_loss: 0.1198\n",
            "Epoch 901/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1271 - val_loss: 0.1281\n",
            "Epoch 902/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1276 - val_loss: 0.1206\n",
            "Epoch 903/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1286 - val_loss: 0.1295\n",
            "Epoch 904/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1264 - val_loss: 0.1202\n",
            "Epoch 905/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1278 - val_loss: 0.1283\n",
            "Epoch 906/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1280 - val_loss: 0.1204\n",
            "Epoch 907/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1267 - val_loss: 0.1300\n",
            "Epoch 908/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1276 - val_loss: 0.1205\n",
            "Epoch 909/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1306 - val_loss: 0.1307\n",
            "Epoch 910/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1296 - val_loss: 0.1206\n",
            "Epoch 911/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1299 - val_loss: 0.1310\n",
            "Epoch 912/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1305 - val_loss: 0.1209\n",
            "Epoch 913/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1310 - val_loss: 0.1302\n",
            "Epoch 914/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1297 - val_loss: 0.1208\n",
            "Epoch 915/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1308 - val_loss: 0.1289\n",
            "Epoch 916/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1273 - val_loss: 0.1203\n",
            "Epoch 917/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1258 - val_loss: 0.1268\n",
            "Epoch 918/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1293 - val_loss: 0.1190\n",
            "Epoch 919/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1272 - val_loss: 0.1264\n",
            "Epoch 920/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1277 - val_loss: 0.1190\n",
            "Epoch 921/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1255 - val_loss: 0.1256\n",
            "Epoch 922/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1262 - val_loss: 0.1186\n",
            "Epoch 923/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1259 - val_loss: 0.1258\n",
            "Epoch 924/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1277 - val_loss: 0.1188\n",
            "Epoch 925/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1262 - val_loss: 0.1251\n",
            "Epoch 926/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1276 - val_loss: 0.1185\n",
            "Epoch 927/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1266 - val_loss: 0.1234\n",
            "Epoch 928/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1264 - val_loss: 0.1178\n",
            "Epoch 929/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1255 - val_loss: 0.1232\n",
            "Epoch 930/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1274 - val_loss: 0.1176\n",
            "Epoch 931/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1245 - val_loss: 0.1227\n",
            "Epoch 932/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1247 - val_loss: 0.1174\n",
            "Epoch 933/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1224 - val_loss: 0.1216\n",
            "Epoch 934/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1226 - val_loss: 0.1170\n",
            "Epoch 935/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1224 - val_loss: 0.1211\n",
            "Epoch 936/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1239 - val_loss: 0.1167\n",
            "Epoch 937/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1251 - val_loss: 0.1218\n",
            "Epoch 938/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1236 - val_loss: 0.1173\n",
            "Epoch 939/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1232 - val_loss: 0.1223\n",
            "Epoch 940/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1231 - val_loss: 0.1173\n",
            "Epoch 941/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1240 - val_loss: 0.1225\n",
            "Epoch 942/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1233 - val_loss: 0.1172\n",
            "Epoch 943/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1248 - val_loss: 0.1227\n",
            "Epoch 944/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1225 - val_loss: 0.1171\n",
            "Epoch 945/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1238 - val_loss: 0.1220\n",
            "Epoch 946/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1231 - val_loss: 0.1172\n",
            "Epoch 947/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1227 - val_loss: 0.1228\n",
            "Epoch 948/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1237 - val_loss: 0.1172\n",
            "Epoch 949/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1255 - val_loss: 0.1231\n",
            "Epoch 950/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1253 - val_loss: 0.1165\n",
            "Epoch 951/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1225 - val_loss: 0.1223\n",
            "Epoch 952/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1258 - val_loss: 0.1168\n",
            "Epoch 953/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1218 - val_loss: 0.1235\n",
            "Epoch 954/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1248 - val_loss: 0.1173\n",
            "Epoch 955/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1227 - val_loss: 0.1241\n",
            "Epoch 956/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1239 - val_loss: 0.1179\n",
            "Epoch 957/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1260 - val_loss: 0.1251\n",
            "Epoch 958/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1260 - val_loss: 0.1187\n",
            "Epoch 959/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1253 - val_loss: 0.1244\n",
            "Epoch 960/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1245 - val_loss: 0.1179\n",
            "Epoch 961/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1252 - val_loss: 0.1231\n",
            "Epoch 962/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1250 - val_loss: 0.1174\n",
            "Epoch 963/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1228 - val_loss: 0.1236\n",
            "Epoch 964/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1233 - val_loss: 0.1180\n",
            "Epoch 965/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1242 - val_loss: 0.1219\n",
            "Epoch 966/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1247 - val_loss: 0.1173\n",
            "Epoch 967/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1230 - val_loss: 0.1216\n",
            "Epoch 968/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1237 - val_loss: 0.1180\n",
            "Epoch 969/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1231 - val_loss: 0.1228\n",
            "Epoch 970/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1265 - val_loss: 0.1185\n",
            "Epoch 971/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1253 - val_loss: 0.1221\n",
            "Epoch 972/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1218 - val_loss: 0.1190\n",
            "Epoch 973/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1234 - val_loss: 0.1218\n",
            "Epoch 974/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1255 - val_loss: 0.1200\n",
            "Epoch 975/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1228 - val_loss: 0.1230\n",
            "Epoch 976/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1255 - val_loss: 0.1206\n",
            "Epoch 977/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1260 - val_loss: 0.1227\n",
            "Epoch 978/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1245 - val_loss: 0.1200\n",
            "Epoch 979/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1241 - val_loss: 0.1224\n",
            "Epoch 980/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1255 - val_loss: 0.1205\n",
            "Epoch 981/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1230 - val_loss: 0.1222\n",
            "Epoch 982/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1240 - val_loss: 0.1201\n",
            "Epoch 983/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1233 - val_loss: 0.1214\n",
            "Epoch 984/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1232 - val_loss: 0.1221\n",
            "Epoch 985/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1239 - val_loss: 0.1233\n",
            "Epoch 986/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1255 - val_loss: 0.1209\n",
            "Epoch 987/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1248 - val_loss: 0.1213\n",
            "Epoch 988/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1239 - val_loss: 0.1208\n",
            "Epoch 989/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.1242 - val_loss: 0.1220\n",
            "Epoch 990/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1245 - val_loss: 0.1219\n",
            "Epoch 991/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1251 - val_loss: 0.1209\n",
            "Epoch 992/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1230 - val_loss: 0.1208\n",
            "Epoch 993/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1249 - val_loss: 0.1209\n",
            "Epoch 994/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1232 - val_loss: 0.1220\n",
            "Epoch 995/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1247 - val_loss: 0.1210\n",
            "Epoch 996/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1254 - val_loss: 0.1217\n",
            "Epoch 997/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1238 - val_loss: 0.1211\n",
            "Epoch 998/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1252 - val_loss: 0.1216\n",
            "Epoch 999/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1211 - val_loss: 0.1195\n",
            "Epoch 1000/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1235 - val_loss: 0.1205\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_6 (Bidirection (None, 5, 1024)           2146304   \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 5, 256)            1311744   \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 5, 1)              257       \n",
            "=================================================================\n",
            "Total params: 3,458,305\n",
            "Trainable params: 3,458,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 3298 samples, validate on 825 samples\n",
            "Epoch 1/1000\n",
            "3298/3298 [==============================] - 5s 2ms/step - loss: 0.4062 - val_loss: 0.3254\n",
            "Epoch 2/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.3409 - val_loss: 0.2761\n",
            "Epoch 3/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.2912 - val_loss: 0.2347\n",
            "Epoch 4/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.2506 - val_loss: 0.1999\n",
            "Epoch 5/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.2147 - val_loss: 0.1717\n",
            "Epoch 6/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1888 - val_loss: 0.1511\n",
            "Epoch 7/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1678 - val_loss: 0.1370\n",
            "Epoch 8/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1545 - val_loss: 0.1280\n",
            "Epoch 9/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1466 - val_loss: 0.1229\n",
            "Epoch 10/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1420 - val_loss: 0.1194\n",
            "Epoch 11/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1403 - val_loss: 0.1176\n",
            "Epoch 12/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1380 - val_loss: 0.1181\n",
            "Epoch 13/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1391 - val_loss: 0.1226\n",
            "Epoch 14/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1420 - val_loss: 0.1252\n",
            "Epoch 15/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1443 - val_loss: 0.1280\n",
            "Epoch 16/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1467 - val_loss: 0.1271\n",
            "Epoch 17/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1463 - val_loss: 0.1255\n",
            "Epoch 18/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1432 - val_loss: 0.1217\n",
            "Epoch 19/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1420 - val_loss: 0.1186\n",
            "Epoch 20/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1381 - val_loss: 0.1150\n",
            "Epoch 21/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1339 - val_loss: 0.1128\n",
            "Epoch 22/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1323 - val_loss: 0.1092\n",
            "Epoch 23/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1298 - val_loss: 0.1076\n",
            "Epoch 24/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1272 - val_loss: 0.1049\n",
            "Epoch 25/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.1272 - val_loss: 0.1039\n",
            "Epoch 26/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1263 - val_loss: 0.1022\n",
            "Epoch 27/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1244 - val_loss: 0.1016\n",
            "Epoch 28/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1216 - val_loss: 0.0995\n",
            "Epoch 29/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1209 - val_loss: 0.0992\n",
            "Epoch 30/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1207 - val_loss: 0.0970\n",
            "Epoch 31/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1178 - val_loss: 0.0964\n",
            "Epoch 32/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1189 - val_loss: 0.0949\n",
            "Epoch 33/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1134 - val_loss: 0.0942\n",
            "Epoch 34/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1163 - val_loss: 0.0933\n",
            "Epoch 35/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1164 - val_loss: 0.0926\n",
            "Epoch 36/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1143 - val_loss: 0.0920\n",
            "Epoch 37/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1145 - val_loss: 0.0915\n",
            "Epoch 38/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1122 - val_loss: 0.0908\n",
            "Epoch 39/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1127 - val_loss: 0.0907\n",
            "Epoch 40/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1095 - val_loss: 0.0894\n",
            "Epoch 41/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1111 - val_loss: 0.0887\n",
            "Epoch 42/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1087 - val_loss: 0.0877\n",
            "Epoch 43/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1098 - val_loss: 0.0871\n",
            "Epoch 44/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1080 - val_loss: 0.0860\n",
            "Epoch 45/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1093 - val_loss: 0.0857\n",
            "Epoch 46/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1087 - val_loss: 0.0851\n",
            "Epoch 47/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1062 - val_loss: 0.0848\n",
            "Epoch 48/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1067 - val_loss: 0.0844\n",
            "Epoch 49/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1054 - val_loss: 0.0841\n",
            "Epoch 50/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1050 - val_loss: 0.0836\n",
            "Epoch 51/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1063 - val_loss: 0.0835\n",
            "Epoch 52/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1057 - val_loss: 0.0830\n",
            "Epoch 53/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1065 - val_loss: 0.0830\n",
            "Epoch 54/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1056 - val_loss: 0.0827\n",
            "Epoch 55/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1053 - val_loss: 0.0826\n",
            "Epoch 56/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1033 - val_loss: 0.0821\n",
            "Epoch 57/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1049 - val_loss: 0.0819\n",
            "Epoch 58/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.1018 - val_loss: 0.0815\n",
            "Epoch 59/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1032 - val_loss: 0.0814\n",
            "Epoch 60/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1019 - val_loss: 0.0810\n",
            "Epoch 61/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1022 - val_loss: 0.0810\n",
            "Epoch 62/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1027 - val_loss: 0.0806\n",
            "Epoch 63/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1002 - val_loss: 0.0806\n",
            "Epoch 64/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1035 - val_loss: 0.0804\n",
            "Epoch 65/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1035 - val_loss: 0.0805\n",
            "Epoch 66/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1029 - val_loss: 0.0803\n",
            "Epoch 67/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1034 - val_loss: 0.0805\n",
            "Epoch 68/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1045 - val_loss: 0.0801\n",
            "Epoch 69/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0994 - val_loss: 0.0801\n",
            "Epoch 70/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1030 - val_loss: 0.0798\n",
            "Epoch 71/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1007 - val_loss: 0.0795\n",
            "Epoch 72/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1030 - val_loss: 0.0792\n",
            "Epoch 73/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1017 - val_loss: 0.0794\n",
            "Epoch 74/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0996 - val_loss: 0.0792\n",
            "Epoch 75/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1044 - val_loss: 0.0793\n",
            "Epoch 76/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1019 - val_loss: 0.0790\n",
            "Epoch 77/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1020 - val_loss: 0.0788\n",
            "Epoch 78/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0996 - val_loss: 0.0785\n",
            "Epoch 79/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1012 - val_loss: 0.0785\n",
            "Epoch 80/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1005 - val_loss: 0.0785\n",
            "Epoch 81/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0987 - val_loss: 0.0786\n",
            "Epoch 82/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.1006 - val_loss: 0.0785\n",
            "Epoch 83/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1008 - val_loss: 0.0789\n",
            "Epoch 84/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1000 - val_loss: 0.0784\n",
            "Epoch 85/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0999 - val_loss: 0.0784\n",
            "Epoch 86/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0994 - val_loss: 0.0780\n",
            "Epoch 87/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1012 - val_loss: 0.0788\n",
            "Epoch 88/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0996 - val_loss: 0.0782\n",
            "Epoch 89/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0982 - val_loss: 0.0788\n",
            "Epoch 90/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0993 - val_loss: 0.0782\n",
            "Epoch 91/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0970 - val_loss: 0.0789\n",
            "Epoch 92/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0992 - val_loss: 0.0782\n",
            "Epoch 93/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0998 - val_loss: 0.0790\n",
            "Epoch 94/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0987 - val_loss: 0.0789\n",
            "Epoch 95/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0988 - val_loss: 0.0786\n",
            "Epoch 96/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.1004 - val_loss: 0.0782\n",
            "Epoch 97/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0999 - val_loss: 0.0785\n",
            "Epoch 98/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.1015 - val_loss: 0.0782\n",
            "Epoch 99/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0970 - val_loss: 0.0780\n",
            "Epoch 100/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0980 - val_loss: 0.0773\n",
            "Epoch 101/1000\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.0979 - val_loss: 0.0778\n",
            "Epoch 102/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0993 - val_loss: 0.0777\n",
            "Epoch 103/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0969 - val_loss: 0.0781\n",
            "Epoch 104/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0989 - val_loss: 0.0772\n",
            "Epoch 105/1000\n",
            "3298/3298 [==============================] - 0s 65us/step - loss: 0.0978 - val_loss: 0.0778\n",
            "Epoch 106/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0986 - val_loss: 0.0776\n",
            "Epoch 107/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0988 - val_loss: 0.0779\n",
            "Epoch 108/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0966 - val_loss: 0.0773\n",
            "Epoch 109/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0980 - val_loss: 0.0779\n",
            "Epoch 110/1000\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.0975 - val_loss: 0.0776\n",
            "Epoch 111/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0971 - val_loss: 0.0783\n",
            "Epoch 112/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0974 - val_loss: 0.0771\n",
            "Epoch 113/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0987 - val_loss: 0.0777\n",
            "Epoch 114/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0974 - val_loss: 0.0770\n",
            "Epoch 115/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0967 - val_loss: 0.0777\n",
            "Epoch 116/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0964 - val_loss: 0.0765\n",
            "Epoch 117/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0961 - val_loss: 0.0769\n",
            "Epoch 118/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0972 - val_loss: 0.0758\n",
            "Epoch 119/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0957 - val_loss: 0.0763\n",
            "Epoch 120/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0961 - val_loss: 0.0757\n",
            "Epoch 121/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0959 - val_loss: 0.0758\n",
            "Epoch 122/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0962 - val_loss: 0.0751\n",
            "Epoch 123/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0951 - val_loss: 0.0757\n",
            "Epoch 124/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0969 - val_loss: 0.0755\n",
            "Epoch 125/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0957 - val_loss: 0.0761\n",
            "Epoch 126/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0964 - val_loss: 0.0751\n",
            "Epoch 127/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0939 - val_loss: 0.0760\n",
            "Epoch 128/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0949 - val_loss: 0.0751\n",
            "Epoch 129/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0970 - val_loss: 0.0752\n",
            "Epoch 130/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0985 - val_loss: 0.0747\n",
            "Epoch 131/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0966 - val_loss: 0.0753\n",
            "Epoch 132/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0944 - val_loss: 0.0746\n",
            "Epoch 133/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0949 - val_loss: 0.0753\n",
            "Epoch 134/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0956 - val_loss: 0.0744\n",
            "Epoch 135/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0939 - val_loss: 0.0754\n",
            "Epoch 136/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0929 - val_loss: 0.0749\n",
            "Epoch 137/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0939 - val_loss: 0.0754\n",
            "Epoch 138/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0948 - val_loss: 0.0748\n",
            "Epoch 139/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0943 - val_loss: 0.0754\n",
            "Epoch 140/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0944 - val_loss: 0.0749\n",
            "Epoch 141/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0936 - val_loss: 0.0755\n",
            "Epoch 142/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0934 - val_loss: 0.0748\n",
            "Epoch 143/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0939 - val_loss: 0.0756\n",
            "Epoch 144/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0943 - val_loss: 0.0749\n",
            "Epoch 145/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0942 - val_loss: 0.0759\n",
            "Epoch 146/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0950 - val_loss: 0.0748\n",
            "Epoch 147/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0958 - val_loss: 0.0752\n",
            "Epoch 148/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0953 - val_loss: 0.0741\n",
            "Epoch 149/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0941 - val_loss: 0.0750\n",
            "Epoch 150/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0952 - val_loss: 0.0741\n",
            "Epoch 151/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0951 - val_loss: 0.0751\n",
            "Epoch 152/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0955 - val_loss: 0.0744\n",
            "Epoch 153/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0938 - val_loss: 0.0753\n",
            "Epoch 154/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0932 - val_loss: 0.0748\n",
            "Epoch 155/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0957 - val_loss: 0.0755\n",
            "Epoch 156/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0964 - val_loss: 0.0748\n",
            "Epoch 157/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0949 - val_loss: 0.0745\n",
            "Epoch 158/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0930 - val_loss: 0.0731\n",
            "Epoch 159/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0941 - val_loss: 0.0735\n",
            "Epoch 160/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0919 - val_loss: 0.0726\n",
            "Epoch 161/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0922 - val_loss: 0.0733\n",
            "Epoch 162/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0931 - val_loss: 0.0726\n",
            "Epoch 163/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0930 - val_loss: 0.0729\n",
            "Epoch 164/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0912 - val_loss: 0.0722\n",
            "Epoch 165/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0921 - val_loss: 0.0730\n",
            "Epoch 166/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0929 - val_loss: 0.0723\n",
            "Epoch 167/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0947 - val_loss: 0.0734\n",
            "Epoch 168/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0923 - val_loss: 0.0724\n",
            "Epoch 169/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0927 - val_loss: 0.0733\n",
            "Epoch 170/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0919 - val_loss: 0.0729\n",
            "Epoch 171/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0934 - val_loss: 0.0735\n",
            "Epoch 172/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0942 - val_loss: 0.0725\n",
            "Epoch 173/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0947 - val_loss: 0.0727\n",
            "Epoch 174/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0912 - val_loss: 0.0719\n",
            "Epoch 175/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0914 - val_loss: 0.0724\n",
            "Epoch 176/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0922 - val_loss: 0.0719\n",
            "Epoch 177/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0948 - val_loss: 0.0726\n",
            "Epoch 178/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0910 - val_loss: 0.0716\n",
            "Epoch 179/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0911 - val_loss: 0.0725\n",
            "Epoch 180/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0917 - val_loss: 0.0711\n",
            "Epoch 181/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0920 - val_loss: 0.0721\n",
            "Epoch 182/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0907 - val_loss: 0.0714\n",
            "Epoch 183/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0925 - val_loss: 0.0720\n",
            "Epoch 184/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0933 - val_loss: 0.0714\n",
            "Epoch 185/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0912 - val_loss: 0.0720\n",
            "Epoch 186/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0906 - val_loss: 0.0715\n",
            "Epoch 187/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0917 - val_loss: 0.0725\n",
            "Epoch 188/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0931 - val_loss: 0.0714\n",
            "Epoch 189/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0909 - val_loss: 0.0723\n",
            "Epoch 190/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0908 - val_loss: 0.0716\n",
            "Epoch 191/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0907 - val_loss: 0.0723\n",
            "Epoch 192/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0930 - val_loss: 0.0715\n",
            "Epoch 193/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0909 - val_loss: 0.0725\n",
            "Epoch 194/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0915 - val_loss: 0.0717\n",
            "Epoch 195/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0926 - val_loss: 0.0730\n",
            "Epoch 196/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0917 - val_loss: 0.0721\n",
            "Epoch 197/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0929 - val_loss: 0.0725\n",
            "Epoch 198/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0904 - val_loss: 0.0712\n",
            "Epoch 199/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0930 - val_loss: 0.0724\n",
            "Epoch 200/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0908 - val_loss: 0.0720\n",
            "Epoch 201/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0939 - val_loss: 0.0728\n",
            "Epoch 202/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0906 - val_loss: 0.0711\n",
            "Epoch 203/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0903 - val_loss: 0.0715\n",
            "Epoch 204/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0928 - val_loss: 0.0708\n",
            "Epoch 205/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0906 - val_loss: 0.0715\n",
            "Epoch 206/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0904 - val_loss: 0.0706\n",
            "Epoch 207/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0924 - val_loss: 0.0716\n",
            "Epoch 208/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0896 - val_loss: 0.0704\n",
            "Epoch 209/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0922 - val_loss: 0.0713\n",
            "Epoch 210/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0901 - val_loss: 0.0704\n",
            "Epoch 211/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0906 - val_loss: 0.0715\n",
            "Epoch 212/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0899 - val_loss: 0.0711\n",
            "Epoch 213/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0901 - val_loss: 0.0715\n",
            "Epoch 214/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0912 - val_loss: 0.0707\n",
            "Epoch 215/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0906 - val_loss: 0.0711\n",
            "Epoch 216/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0892 - val_loss: 0.0699\n",
            "Epoch 217/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0887 - val_loss: 0.0708\n",
            "Epoch 218/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0892 - val_loss: 0.0696\n",
            "Epoch 219/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0887 - val_loss: 0.0697\n",
            "Epoch 220/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0888 - val_loss: 0.0691\n",
            "Epoch 221/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0880 - val_loss: 0.0704\n",
            "Epoch 222/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0905 - val_loss: 0.0695\n",
            "Epoch 223/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0898 - val_loss: 0.0700\n",
            "Epoch 224/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0897 - val_loss: 0.0689\n",
            "Epoch 225/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0884 - val_loss: 0.0693\n",
            "Epoch 226/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0874 - val_loss: 0.0684\n",
            "Epoch 227/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0881 - val_loss: 0.0693\n",
            "Epoch 228/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0886 - val_loss: 0.0690\n",
            "Epoch 229/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0915 - val_loss: 0.0697\n",
            "Epoch 230/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0893 - val_loss: 0.0688\n",
            "Epoch 231/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0906 - val_loss: 0.0697\n",
            "Epoch 232/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0898 - val_loss: 0.0686\n",
            "Epoch 233/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0893 - val_loss: 0.0690\n",
            "Epoch 234/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0868 - val_loss: 0.0681\n",
            "Epoch 235/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0889 - val_loss: 0.0692\n",
            "Epoch 236/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0876 - val_loss: 0.0683\n",
            "Epoch 237/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0903 - val_loss: 0.0697\n",
            "Epoch 238/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0880 - val_loss: 0.0691\n",
            "Epoch 239/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0903 - val_loss: 0.0699\n",
            "Epoch 240/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0880 - val_loss: 0.0691\n",
            "Epoch 241/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0902 - val_loss: 0.0700\n",
            "Epoch 242/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0898 - val_loss: 0.0691\n",
            "Epoch 243/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0905 - val_loss: 0.0701\n",
            "Epoch 244/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0893 - val_loss: 0.0688\n",
            "Epoch 245/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0903 - val_loss: 0.0700\n",
            "Epoch 246/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0890 - val_loss: 0.0693\n",
            "Epoch 247/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0886 - val_loss: 0.0695\n",
            "Epoch 248/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0885 - val_loss: 0.0690\n",
            "Epoch 249/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0901 - val_loss: 0.0698\n",
            "Epoch 250/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0890 - val_loss: 0.0684\n",
            "Epoch 251/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0882 - val_loss: 0.0691\n",
            "Epoch 252/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0891 - val_loss: 0.0689\n",
            "Epoch 253/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0883 - val_loss: 0.0697\n",
            "Epoch 254/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0874 - val_loss: 0.0682\n",
            "Epoch 255/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0868 - val_loss: 0.0692\n",
            "Epoch 256/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0893 - val_loss: 0.0681\n",
            "Epoch 257/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0883 - val_loss: 0.0687\n",
            "Epoch 258/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0888 - val_loss: 0.0681\n",
            "Epoch 259/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0887 - val_loss: 0.0691\n",
            "Epoch 260/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0866 - val_loss: 0.0682\n",
            "Epoch 261/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0882 - val_loss: 0.0691\n",
            "Epoch 262/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0892 - val_loss: 0.0675\n",
            "Epoch 263/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0859 - val_loss: 0.0684\n",
            "Epoch 264/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0868 - val_loss: 0.0676\n",
            "Epoch 265/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0889 - val_loss: 0.0692\n",
            "Epoch 266/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0894 - val_loss: 0.0681\n",
            "Epoch 267/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0884 - val_loss: 0.0695\n",
            "Epoch 268/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0886 - val_loss: 0.0679\n",
            "Epoch 269/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0872 - val_loss: 0.0694\n",
            "Epoch 270/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0882 - val_loss: 0.0677\n",
            "Epoch 271/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0871 - val_loss: 0.0690\n",
            "Epoch 272/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0878 - val_loss: 0.0680\n",
            "Epoch 273/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0876 - val_loss: 0.0696\n",
            "Epoch 274/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0885 - val_loss: 0.0686\n",
            "Epoch 275/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0887 - val_loss: 0.0703\n",
            "Epoch 276/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0886 - val_loss: 0.0687\n",
            "Epoch 277/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0875 - val_loss: 0.0694\n",
            "Epoch 278/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0862 - val_loss: 0.0676\n",
            "Epoch 279/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0880 - val_loss: 0.0693\n",
            "Epoch 280/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0879 - val_loss: 0.0678\n",
            "Epoch 281/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0884 - val_loss: 0.0686\n",
            "Epoch 282/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0853 - val_loss: 0.0677\n",
            "Epoch 283/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0864 - val_loss: 0.0695\n",
            "Epoch 284/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0856 - val_loss: 0.0675\n",
            "Epoch 285/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0884 - val_loss: 0.0686\n",
            "Epoch 286/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0880 - val_loss: 0.0672\n",
            "Epoch 287/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0851 - val_loss: 0.0679\n",
            "Epoch 288/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0860 - val_loss: 0.0667\n",
            "Epoch 289/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0874 - val_loss: 0.0681\n",
            "Epoch 290/1000\n",
            "3298/3298 [==============================] - 0s 58us/step - loss: 0.0871 - val_loss: 0.0662\n",
            "Epoch 291/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0849 - val_loss: 0.0677\n",
            "Epoch 292/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0854 - val_loss: 0.0664\n",
            "Epoch 293/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0877 - val_loss: 0.0676\n",
            "Epoch 294/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0859 - val_loss: 0.0665\n",
            "Epoch 295/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0870 - val_loss: 0.0677\n",
            "Epoch 296/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0835 - val_loss: 0.0661\n",
            "Epoch 297/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0857 - val_loss: 0.0673\n",
            "Epoch 298/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0851 - val_loss: 0.0663\n",
            "Epoch 299/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0855 - val_loss: 0.0674\n",
            "Epoch 300/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0854 - val_loss: 0.0665\n",
            "Epoch 301/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0890 - val_loss: 0.0680\n",
            "Epoch 302/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0863 - val_loss: 0.0671\n",
            "Epoch 303/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0878 - val_loss: 0.0683\n",
            "Epoch 304/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0858 - val_loss: 0.0669\n",
            "Epoch 305/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0874 - val_loss: 0.0687\n",
            "Epoch 306/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0871 - val_loss: 0.0681\n",
            "Epoch 307/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0869 - val_loss: 0.0696\n",
            "Epoch 308/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0886 - val_loss: 0.0681\n",
            "Epoch 309/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0876 - val_loss: 0.0686\n",
            "Epoch 310/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0849 - val_loss: 0.0666\n",
            "Epoch 311/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0855 - val_loss: 0.0682\n",
            "Epoch 312/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0872 - val_loss: 0.0662\n",
            "Epoch 313/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0855 - val_loss: 0.0675\n",
            "Epoch 314/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0853 - val_loss: 0.0656\n",
            "Epoch 315/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0842 - val_loss: 0.0670\n",
            "Epoch 316/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0839 - val_loss: 0.0653\n",
            "Epoch 317/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0846 - val_loss: 0.0665\n",
            "Epoch 318/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0863 - val_loss: 0.0652\n",
            "Epoch 319/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0864 - val_loss: 0.0661\n",
            "Epoch 320/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0839 - val_loss: 0.0654\n",
            "Epoch 321/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0863 - val_loss: 0.0666\n",
            "Epoch 322/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0862 - val_loss: 0.0655\n",
            "Epoch 323/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0839 - val_loss: 0.0666\n",
            "Epoch 324/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0836 - val_loss: 0.0657\n",
            "Epoch 325/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0836 - val_loss: 0.0662\n",
            "Epoch 326/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0847 - val_loss: 0.0653\n",
            "Epoch 327/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0839 - val_loss: 0.0668\n",
            "Epoch 328/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0860 - val_loss: 0.0652\n",
            "Epoch 329/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0851 - val_loss: 0.0663\n",
            "Epoch 330/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0842 - val_loss: 0.0653\n",
            "Epoch 331/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0853 - val_loss: 0.0662\n",
            "Epoch 332/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0850 - val_loss: 0.0653\n",
            "Epoch 333/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0848 - val_loss: 0.0661\n",
            "Epoch 334/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0838 - val_loss: 0.0653\n",
            "Epoch 335/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0865 - val_loss: 0.0664\n",
            "Epoch 336/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0839 - val_loss: 0.0657\n",
            "Epoch 337/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0845 - val_loss: 0.0671\n",
            "Epoch 338/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0871 - val_loss: 0.0656\n",
            "Epoch 339/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0863 - val_loss: 0.0672\n",
            "Epoch 340/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0869 - val_loss: 0.0661\n",
            "Epoch 341/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0854 - val_loss: 0.0681\n",
            "Epoch 342/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0859 - val_loss: 0.0659\n",
            "Epoch 343/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0853 - val_loss: 0.0680\n",
            "Epoch 344/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0859 - val_loss: 0.0668\n",
            "Epoch 345/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0848 - val_loss: 0.0682\n",
            "Epoch 346/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0868 - val_loss: 0.0660\n",
            "Epoch 347/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0865 - val_loss: 0.0676\n",
            "Epoch 348/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0870 - val_loss: 0.0662\n",
            "Epoch 349/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0853 - val_loss: 0.0677\n",
            "Epoch 350/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0890 - val_loss: 0.0668\n",
            "Epoch 351/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0873 - val_loss: 0.0688\n",
            "Epoch 352/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0865 - val_loss: 0.0673\n",
            "Epoch 353/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0867 - val_loss: 0.0688\n",
            "Epoch 354/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0863 - val_loss: 0.0668\n",
            "Epoch 355/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0862 - val_loss: 0.0688\n",
            "Epoch 356/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0880 - val_loss: 0.0669\n",
            "Epoch 357/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0875 - val_loss: 0.0685\n",
            "Epoch 358/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0872 - val_loss: 0.0662\n",
            "Epoch 359/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0867 - val_loss: 0.0676\n",
            "Epoch 360/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0860 - val_loss: 0.0664\n",
            "Epoch 361/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0875 - val_loss: 0.0677\n",
            "Epoch 362/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0873 - val_loss: 0.0660\n",
            "Epoch 363/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0849 - val_loss: 0.0677\n",
            "Epoch 364/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0849 - val_loss: 0.0661\n",
            "Epoch 365/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0856 - val_loss: 0.0677\n",
            "Epoch 366/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0855 - val_loss: 0.0658\n",
            "Epoch 367/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0848 - val_loss: 0.0666\n",
            "Epoch 368/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0846 - val_loss: 0.0650\n",
            "Epoch 369/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0860 - val_loss: 0.0665\n",
            "Epoch 370/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0834 - val_loss: 0.0649\n",
            "Epoch 371/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0822 - val_loss: 0.0658\n",
            "Epoch 372/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0834 - val_loss: 0.0645\n",
            "Epoch 373/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0839 - val_loss: 0.0661\n",
            "Epoch 374/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0835 - val_loss: 0.0646\n",
            "Epoch 375/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0836 - val_loss: 0.0654\n",
            "Epoch 376/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0836 - val_loss: 0.0646\n",
            "Epoch 377/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0835 - val_loss: 0.0659\n",
            "Epoch 378/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0850 - val_loss: 0.0647\n",
            "Epoch 379/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0832 - val_loss: 0.0662\n",
            "Epoch 380/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0849 - val_loss: 0.0650\n",
            "Epoch 381/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0827 - val_loss: 0.0660\n",
            "Epoch 382/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0836 - val_loss: 0.0649\n",
            "Epoch 383/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0838 - val_loss: 0.0673\n",
            "Epoch 384/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0842 - val_loss: 0.0648\n",
            "Epoch 385/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0841 - val_loss: 0.0663\n",
            "Epoch 386/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0824 - val_loss: 0.0642\n",
            "Epoch 387/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0845 - val_loss: 0.0657\n",
            "Epoch 388/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0850 - val_loss: 0.0640\n",
            "Epoch 389/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0834 - val_loss: 0.0653\n",
            "Epoch 390/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0832 - val_loss: 0.0636\n",
            "Epoch 391/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0829 - val_loss: 0.0651\n",
            "Epoch 392/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0816 - val_loss: 0.0635\n",
            "Epoch 393/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0813 - val_loss: 0.0652\n",
            "Epoch 394/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0815 - val_loss: 0.0637\n",
            "Epoch 395/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0832 - val_loss: 0.0652\n",
            "Epoch 396/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0828 - val_loss: 0.0636\n",
            "Epoch 397/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0828 - val_loss: 0.0651\n",
            "Epoch 398/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0834 - val_loss: 0.0639\n",
            "Epoch 399/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0817 - val_loss: 0.0649\n",
            "Epoch 400/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0824 - val_loss: 0.0635\n",
            "Epoch 401/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0851 - val_loss: 0.0647\n",
            "Epoch 402/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0840 - val_loss: 0.0635\n",
            "Epoch 403/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0816 - val_loss: 0.0649\n",
            "Epoch 404/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0815 - val_loss: 0.0644\n",
            "Epoch 405/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0834 - val_loss: 0.0661\n",
            "Epoch 406/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0840 - val_loss: 0.0644\n",
            "Epoch 407/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0822 - val_loss: 0.0663\n",
            "Epoch 408/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0819 - val_loss: 0.0645\n",
            "Epoch 409/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0822 - val_loss: 0.0657\n",
            "Epoch 410/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0848 - val_loss: 0.0641\n",
            "Epoch 411/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0837 - val_loss: 0.0654\n",
            "Epoch 412/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0825 - val_loss: 0.0640\n",
            "Epoch 413/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0825 - val_loss: 0.0657\n",
            "Epoch 414/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0838 - val_loss: 0.0638\n",
            "Epoch 415/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0810 - val_loss: 0.0655\n",
            "Epoch 416/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0833 - val_loss: 0.0639\n",
            "Epoch 417/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0853 - val_loss: 0.0658\n",
            "Epoch 418/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0813 - val_loss: 0.0642\n",
            "Epoch 419/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0830 - val_loss: 0.0659\n",
            "Epoch 420/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0849 - val_loss: 0.0644\n",
            "Epoch 421/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0828 - val_loss: 0.0665\n",
            "Epoch 422/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0841 - val_loss: 0.0648\n",
            "Epoch 423/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0844 - val_loss: 0.0665\n",
            "Epoch 424/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0846 - val_loss: 0.0648\n",
            "Epoch 425/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0858 - val_loss: 0.0665\n",
            "Epoch 426/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0834 - val_loss: 0.0650\n",
            "Epoch 427/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0835 - val_loss: 0.0666\n",
            "Epoch 428/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0830 - val_loss: 0.0641\n",
            "Epoch 429/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0821 - val_loss: 0.0651\n",
            "Epoch 430/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0827 - val_loss: 0.0633\n",
            "Epoch 431/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0807 - val_loss: 0.0652\n",
            "Epoch 432/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0843 - val_loss: 0.0631\n",
            "Epoch 433/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0830 - val_loss: 0.0648\n",
            "Epoch 434/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0814 - val_loss: 0.0632\n",
            "Epoch 435/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0809 - val_loss: 0.0642\n",
            "Epoch 436/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0812 - val_loss: 0.0625\n",
            "Epoch 437/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0808 - val_loss: 0.0637\n",
            "Epoch 438/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0790 - val_loss: 0.0626\n",
            "Epoch 439/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0822 - val_loss: 0.0641\n",
            "Epoch 440/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0828 - val_loss: 0.0625\n",
            "Epoch 441/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0830 - val_loss: 0.0637\n",
            "Epoch 442/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0796 - val_loss: 0.0624\n",
            "Epoch 443/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0814 - val_loss: 0.0638\n",
            "Epoch 444/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0815 - val_loss: 0.0628\n",
            "Epoch 445/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0844 - val_loss: 0.0643\n",
            "Epoch 446/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0820 - val_loss: 0.0634\n",
            "Epoch 447/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0801 - val_loss: 0.0652\n",
            "Epoch 448/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0807 - val_loss: 0.0636\n",
            "Epoch 449/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0827 - val_loss: 0.0662\n",
            "Epoch 450/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0832 - val_loss: 0.0635\n",
            "Epoch 451/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0823 - val_loss: 0.0654\n",
            "Epoch 452/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0822 - val_loss: 0.0637\n",
            "Epoch 453/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0820 - val_loss: 0.0664\n",
            "Epoch 454/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0825 - val_loss: 0.0638\n",
            "Epoch 455/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0841 - val_loss: 0.0658\n",
            "Epoch 456/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0836 - val_loss: 0.0637\n",
            "Epoch 457/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0813 - val_loss: 0.0655\n",
            "Epoch 458/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0803 - val_loss: 0.0635\n",
            "Epoch 459/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0840 - val_loss: 0.0666\n",
            "Epoch 460/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0821 - val_loss: 0.0644\n",
            "Epoch 461/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0832 - val_loss: 0.0662\n",
            "Epoch 462/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0825 - val_loss: 0.0641\n",
            "Epoch 463/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0830 - val_loss: 0.0660\n",
            "Epoch 464/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0833 - val_loss: 0.0639\n",
            "Epoch 465/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0839 - val_loss: 0.0667\n",
            "Epoch 466/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0840 - val_loss: 0.0640\n",
            "Epoch 467/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0833 - val_loss: 0.0662\n",
            "Epoch 468/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0842 - val_loss: 0.0642\n",
            "Epoch 469/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0834 - val_loss: 0.0670\n",
            "Epoch 470/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0843 - val_loss: 0.0644\n",
            "Epoch 471/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0837 - val_loss: 0.0665\n",
            "Epoch 472/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0816 - val_loss: 0.0641\n",
            "Epoch 473/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0832 - val_loss: 0.0671\n",
            "Epoch 474/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0835 - val_loss: 0.0641\n",
            "Epoch 475/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0822 - val_loss: 0.0656\n",
            "Epoch 476/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0835 - val_loss: 0.0640\n",
            "Epoch 477/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0820 - val_loss: 0.0657\n",
            "Epoch 478/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0802 - val_loss: 0.0633\n",
            "Epoch 479/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0814 - val_loss: 0.0649\n",
            "Epoch 480/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0810 - val_loss: 0.0628\n",
            "Epoch 481/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0803 - val_loss: 0.0638\n",
            "Epoch 482/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0791 - val_loss: 0.0625\n",
            "Epoch 483/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0801 - val_loss: 0.0642\n",
            "Epoch 484/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0808 - val_loss: 0.0624\n",
            "Epoch 485/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0816 - val_loss: 0.0647\n",
            "Epoch 486/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0822 - val_loss: 0.0624\n",
            "Epoch 487/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0810 - val_loss: 0.0645\n",
            "Epoch 488/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0817 - val_loss: 0.0625\n",
            "Epoch 489/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0820 - val_loss: 0.0644\n",
            "Epoch 490/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0810 - val_loss: 0.0627\n",
            "Epoch 491/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0818 - val_loss: 0.0638\n",
            "Epoch 492/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0834 - val_loss: 0.0623\n",
            "Epoch 493/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0812 - val_loss: 0.0636\n",
            "Epoch 494/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0799 - val_loss: 0.0622\n",
            "Epoch 495/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0811 - val_loss: 0.0640\n",
            "Epoch 496/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0808 - val_loss: 0.0623\n",
            "Epoch 497/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0800 - val_loss: 0.0634\n",
            "Epoch 498/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0807 - val_loss: 0.0620\n",
            "Epoch 499/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0807 - val_loss: 0.0632\n",
            "Epoch 500/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0789 - val_loss: 0.0621\n",
            "Epoch 501/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0818 - val_loss: 0.0637\n",
            "Epoch 502/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0802 - val_loss: 0.0620\n",
            "Epoch 503/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0791 - val_loss: 0.0631\n",
            "Epoch 504/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0793 - val_loss: 0.0617\n",
            "Epoch 505/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0787 - val_loss: 0.0636\n",
            "Epoch 506/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0791 - val_loss: 0.0622\n",
            "Epoch 507/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0801 - val_loss: 0.0639\n",
            "Epoch 508/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0814 - val_loss: 0.0625\n",
            "Epoch 509/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0820 - val_loss: 0.0650\n",
            "Epoch 510/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0822 - val_loss: 0.0628\n",
            "Epoch 511/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0824 - val_loss: 0.0648\n",
            "Epoch 512/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0818 - val_loss: 0.0629\n",
            "Epoch 513/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0832 - val_loss: 0.0663\n",
            "Epoch 514/1000\n",
            "3298/3298 [==============================] - 0s 64us/step - loss: 0.0832 - val_loss: 0.0638\n",
            "Epoch 515/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0828 - val_loss: 0.0668\n",
            "Epoch 516/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0832 - val_loss: 0.0633\n",
            "Epoch 517/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0821 - val_loss: 0.0661\n",
            "Epoch 518/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0821 - val_loss: 0.0628\n",
            "Epoch 519/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0829 - val_loss: 0.0652\n",
            "Epoch 520/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0836 - val_loss: 0.0627\n",
            "Epoch 521/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0819 - val_loss: 0.0654\n",
            "Epoch 522/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0802 - val_loss: 0.0625\n",
            "Epoch 523/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0800 - val_loss: 0.0655\n",
            "Epoch 524/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0802 - val_loss: 0.0629\n",
            "Epoch 525/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0815 - val_loss: 0.0652\n",
            "Epoch 526/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0794 - val_loss: 0.0628\n",
            "Epoch 527/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0801 - val_loss: 0.0655\n",
            "Epoch 528/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0817 - val_loss: 0.0627\n",
            "Epoch 529/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0804 - val_loss: 0.0656\n",
            "Epoch 530/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0815 - val_loss: 0.0636\n",
            "Epoch 531/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0805 - val_loss: 0.0655\n",
            "Epoch 532/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0813 - val_loss: 0.0630\n",
            "Epoch 533/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0816 - val_loss: 0.0654\n",
            "Epoch 534/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0821 - val_loss: 0.0629\n",
            "Epoch 535/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0797 - val_loss: 0.0648\n",
            "Epoch 536/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0801 - val_loss: 0.0624\n",
            "Epoch 537/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0804 - val_loss: 0.0638\n",
            "Epoch 538/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0799 - val_loss: 0.0620\n",
            "Epoch 539/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0800 - val_loss: 0.0633\n",
            "Epoch 540/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0800 - val_loss: 0.0617\n",
            "Epoch 541/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0800 - val_loss: 0.0632\n",
            "Epoch 542/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0791 - val_loss: 0.0615\n",
            "Epoch 543/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0790 - val_loss: 0.0633\n",
            "Epoch 544/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0795 - val_loss: 0.0618\n",
            "Epoch 545/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0802 - val_loss: 0.0637\n",
            "Epoch 546/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0808 - val_loss: 0.0619\n",
            "Epoch 547/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0808 - val_loss: 0.0646\n",
            "Epoch 548/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0817 - val_loss: 0.0622\n",
            "Epoch 549/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0804 - val_loss: 0.0643\n",
            "Epoch 550/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0807 - val_loss: 0.0623\n",
            "Epoch 551/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0807 - val_loss: 0.0642\n",
            "Epoch 552/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0819 - val_loss: 0.0624\n",
            "Epoch 553/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0801 - val_loss: 0.0643\n",
            "Epoch 554/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0787 - val_loss: 0.0620\n",
            "Epoch 555/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0806 - val_loss: 0.0642\n",
            "Epoch 556/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0786 - val_loss: 0.0623\n",
            "Epoch 557/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0780 - val_loss: 0.0639\n",
            "Epoch 558/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0814 - val_loss: 0.0621\n",
            "Epoch 559/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0816 - val_loss: 0.0640\n",
            "Epoch 560/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0798 - val_loss: 0.0617\n",
            "Epoch 561/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0806 - val_loss: 0.0640\n",
            "Epoch 562/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0805 - val_loss: 0.0619\n",
            "Epoch 563/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0793 - val_loss: 0.0633\n",
            "Epoch 564/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0799 - val_loss: 0.0616\n",
            "Epoch 565/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0795 - val_loss: 0.0632\n",
            "Epoch 566/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0784 - val_loss: 0.0611\n",
            "Epoch 567/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0808 - val_loss: 0.0632\n",
            "Epoch 568/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0780 - val_loss: 0.0613\n",
            "Epoch 569/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0791 - val_loss: 0.0637\n",
            "Epoch 570/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0791 - val_loss: 0.0616\n",
            "Epoch 571/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0802 - val_loss: 0.0639\n",
            "Epoch 572/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0806 - val_loss: 0.0615\n",
            "Epoch 573/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0784 - val_loss: 0.0633\n",
            "Epoch 574/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0790 - val_loss: 0.0618\n",
            "Epoch 575/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0806 - val_loss: 0.0642\n",
            "Epoch 576/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0794 - val_loss: 0.0618\n",
            "Epoch 577/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0810 - val_loss: 0.0639\n",
            "Epoch 578/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0786 - val_loss: 0.0617\n",
            "Epoch 579/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0813 - val_loss: 0.0645\n",
            "Epoch 580/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0815 - val_loss: 0.0626\n",
            "Epoch 581/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0797 - val_loss: 0.0651\n",
            "Epoch 582/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0806 - val_loss: 0.0622\n",
            "Epoch 583/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0804 - val_loss: 0.0654\n",
            "Epoch 584/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0796 - val_loss: 0.0631\n",
            "Epoch 585/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0805 - val_loss: 0.0655\n",
            "Epoch 586/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0796 - val_loss: 0.0622\n",
            "Epoch 587/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0799 - val_loss: 0.0654\n",
            "Epoch 588/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0813 - val_loss: 0.0620\n",
            "Epoch 589/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0816 - val_loss: 0.0646\n",
            "Epoch 590/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0814 - val_loss: 0.0622\n",
            "Epoch 591/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0769 - val_loss: 0.0647\n",
            "Epoch 592/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0810 - val_loss: 0.0624\n",
            "Epoch 593/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0809 - val_loss: 0.0644\n",
            "Epoch 594/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0805 - val_loss: 0.0617\n",
            "Epoch 595/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0806 - val_loss: 0.0640\n",
            "Epoch 596/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0788 - val_loss: 0.0617\n",
            "Epoch 597/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0798 - val_loss: 0.0645\n",
            "Epoch 598/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0796 - val_loss: 0.0615\n",
            "Epoch 599/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0790 - val_loss: 0.0630\n",
            "Epoch 600/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0781 - val_loss: 0.0616\n",
            "Epoch 601/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0778 - val_loss: 0.0635\n",
            "Epoch 602/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0802 - val_loss: 0.0616\n",
            "Epoch 603/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0796 - val_loss: 0.0637\n",
            "Epoch 604/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0802 - val_loss: 0.0616\n",
            "Epoch 605/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0774 - val_loss: 0.0635\n",
            "Epoch 606/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0798 - val_loss: 0.0612\n",
            "Epoch 607/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0787 - val_loss: 0.0635\n",
            "Epoch 608/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0807 - val_loss: 0.0618\n",
            "Epoch 609/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0793 - val_loss: 0.0626\n",
            "Epoch 610/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0776 - val_loss: 0.0608\n",
            "Epoch 611/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0790 - val_loss: 0.0624\n",
            "Epoch 612/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0787 - val_loss: 0.0606\n",
            "Epoch 613/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0810 - val_loss: 0.0623\n",
            "Epoch 614/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0789 - val_loss: 0.0605\n",
            "Epoch 615/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0797 - val_loss: 0.0630\n",
            "Epoch 616/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0765 - val_loss: 0.0604\n",
            "Epoch 617/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0776 - val_loss: 0.0628\n",
            "Epoch 618/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0780 - val_loss: 0.0604\n",
            "Epoch 619/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0766 - val_loss: 0.0627\n",
            "Epoch 620/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0776 - val_loss: 0.0611\n",
            "Epoch 621/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0783 - val_loss: 0.0634\n",
            "Epoch 622/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0776 - val_loss: 0.0609\n",
            "Epoch 623/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0787 - val_loss: 0.0639\n",
            "Epoch 624/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0785 - val_loss: 0.0609\n",
            "Epoch 625/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0779 - val_loss: 0.0632\n",
            "Epoch 626/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0778 - val_loss: 0.0605\n",
            "Epoch 627/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0787 - val_loss: 0.0629\n",
            "Epoch 628/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0762 - val_loss: 0.0608\n",
            "Epoch 629/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0802 - val_loss: 0.0634\n",
            "Epoch 630/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0773 - val_loss: 0.0608\n",
            "Epoch 631/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0790 - val_loss: 0.0628\n",
            "Epoch 632/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0762 - val_loss: 0.0603\n",
            "Epoch 633/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0781 - val_loss: 0.0631\n",
            "Epoch 634/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0787 - val_loss: 0.0603\n",
            "Epoch 635/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0776 - val_loss: 0.0625\n",
            "Epoch 636/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0787 - val_loss: 0.0607\n",
            "Epoch 637/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0781 - val_loss: 0.0622\n",
            "Epoch 638/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0782 - val_loss: 0.0602\n",
            "Epoch 639/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0784 - val_loss: 0.0622\n",
            "Epoch 640/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0768 - val_loss: 0.0604\n",
            "Epoch 641/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0761 - val_loss: 0.0623\n",
            "Epoch 642/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0797 - val_loss: 0.0608\n",
            "Epoch 643/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0787 - val_loss: 0.0628\n",
            "Epoch 644/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0798 - val_loss: 0.0610\n",
            "Epoch 645/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0774 - val_loss: 0.0630\n",
            "Epoch 646/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0783 - val_loss: 0.0604\n",
            "Epoch 647/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0789 - val_loss: 0.0632\n",
            "Epoch 648/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0790 - val_loss: 0.0611\n",
            "Epoch 649/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0795 - val_loss: 0.0635\n",
            "Epoch 650/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0797 - val_loss: 0.0614\n",
            "Epoch 651/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0788 - val_loss: 0.0649\n",
            "Epoch 652/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0810 - val_loss: 0.0615\n",
            "Epoch 653/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0791 - val_loss: 0.0652\n",
            "Epoch 654/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0794 - val_loss: 0.0620\n",
            "Epoch 655/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0801 - val_loss: 0.0650\n",
            "Epoch 656/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0798 - val_loss: 0.0619\n",
            "Epoch 657/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0807 - val_loss: 0.0653\n",
            "Epoch 658/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0794 - val_loss: 0.0617\n",
            "Epoch 659/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0794 - val_loss: 0.0643\n",
            "Epoch 660/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0791 - val_loss: 0.0617\n",
            "Epoch 661/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0800 - val_loss: 0.0648\n",
            "Epoch 662/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0770 - val_loss: 0.0616\n",
            "Epoch 663/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0773 - val_loss: 0.0632\n",
            "Epoch 664/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0787 - val_loss: 0.0608\n",
            "Epoch 665/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0804 - val_loss: 0.0635\n",
            "Epoch 666/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0802 - val_loss: 0.0617\n",
            "Epoch 667/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0799 - val_loss: 0.0639\n",
            "Epoch 668/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0795 - val_loss: 0.0614\n",
            "Epoch 669/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0787 - val_loss: 0.0633\n",
            "Epoch 670/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0788 - val_loss: 0.0607\n",
            "Epoch 671/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0777 - val_loss: 0.0623\n",
            "Epoch 672/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0761 - val_loss: 0.0603\n",
            "Epoch 673/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0777 - val_loss: 0.0629\n",
            "Epoch 674/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0777 - val_loss: 0.0601\n",
            "Epoch 675/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0763 - val_loss: 0.0625\n",
            "Epoch 676/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0767 - val_loss: 0.0600\n",
            "Epoch 677/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0772 - val_loss: 0.0626\n",
            "Epoch 678/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0776 - val_loss: 0.0600\n",
            "Epoch 679/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0772 - val_loss: 0.0625\n",
            "Epoch 680/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0768 - val_loss: 0.0599\n",
            "Epoch 681/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0771 - val_loss: 0.0628\n",
            "Epoch 682/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0813 - val_loss: 0.0608\n",
            "Epoch 683/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0772 - val_loss: 0.0627\n",
            "Epoch 684/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0786 - val_loss: 0.0604\n",
            "Epoch 685/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0765 - val_loss: 0.0623\n",
            "Epoch 686/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0766 - val_loss: 0.0602\n",
            "Epoch 687/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0778 - val_loss: 0.0622\n",
            "Epoch 688/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0752 - val_loss: 0.0601\n",
            "Epoch 689/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0751 - val_loss: 0.0622\n",
            "Epoch 690/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0772 - val_loss: 0.0601\n",
            "Epoch 691/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0788 - val_loss: 0.0623\n",
            "Epoch 692/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0778 - val_loss: 0.0601\n",
            "Epoch 693/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0761 - val_loss: 0.0622\n",
            "Epoch 694/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0760 - val_loss: 0.0600\n",
            "Epoch 695/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0787 - val_loss: 0.0634\n",
            "Epoch 696/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0784 - val_loss: 0.0613\n",
            "Epoch 697/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0770 - val_loss: 0.0633\n",
            "Epoch 698/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0759 - val_loss: 0.0604\n",
            "Epoch 699/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0792 - val_loss: 0.0623\n",
            "Epoch 700/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0775 - val_loss: 0.0601\n",
            "Epoch 701/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0784 - val_loss: 0.0623\n",
            "Epoch 702/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0770 - val_loss: 0.0599\n",
            "Epoch 703/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0762 - val_loss: 0.0624\n",
            "Epoch 704/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0761 - val_loss: 0.0597\n",
            "Epoch 705/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0775 - val_loss: 0.0624\n",
            "Epoch 706/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0770 - val_loss: 0.0603\n",
            "Epoch 707/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0781 - val_loss: 0.0634\n",
            "Epoch 708/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0773 - val_loss: 0.0602\n",
            "Epoch 709/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0772 - val_loss: 0.0631\n",
            "Epoch 710/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0807 - val_loss: 0.0603\n",
            "Epoch 711/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0780 - val_loss: 0.0623\n",
            "Epoch 712/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0762 - val_loss: 0.0598\n",
            "Epoch 713/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0754 - val_loss: 0.0621\n",
            "Epoch 714/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0758 - val_loss: 0.0600\n",
            "Epoch 715/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0784 - val_loss: 0.0623\n",
            "Epoch 716/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0757 - val_loss: 0.0596\n",
            "Epoch 717/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0757 - val_loss: 0.0626\n",
            "Epoch 718/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0766 - val_loss: 0.0597\n",
            "Epoch 719/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0746 - val_loss: 0.0620\n",
            "Epoch 720/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0753 - val_loss: 0.0592\n",
            "Epoch 721/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0753 - val_loss: 0.0623\n",
            "Epoch 722/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0753 - val_loss: 0.0596\n",
            "Epoch 723/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0779 - val_loss: 0.0627\n",
            "Epoch 724/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0758 - val_loss: 0.0595\n",
            "Epoch 725/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0770 - val_loss: 0.0626\n",
            "Epoch 726/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0773 - val_loss: 0.0597\n",
            "Epoch 727/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0765 - val_loss: 0.0622\n",
            "Epoch 728/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0773 - val_loss: 0.0598\n",
            "Epoch 729/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0763 - val_loss: 0.0616\n",
            "Epoch 730/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0782 - val_loss: 0.0592\n",
            "Epoch 731/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0771 - val_loss: 0.0621\n",
            "Epoch 732/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0775 - val_loss: 0.0599\n",
            "Epoch 733/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0766 - val_loss: 0.0622\n",
            "Epoch 734/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0762 - val_loss: 0.0600\n",
            "Epoch 735/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0766 - val_loss: 0.0624\n",
            "Epoch 736/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0780 - val_loss: 0.0603\n",
            "Epoch 737/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0776 - val_loss: 0.0615\n",
            "Epoch 738/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0767 - val_loss: 0.0590\n",
            "Epoch 739/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0751 - val_loss: 0.0614\n",
            "Epoch 740/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0764 - val_loss: 0.0588\n",
            "Epoch 741/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0774 - val_loss: 0.0614\n",
            "Epoch 742/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0754 - val_loss: 0.0586\n",
            "Epoch 743/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0773 - val_loss: 0.0610\n",
            "Epoch 744/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0755 - val_loss: 0.0587\n",
            "Epoch 745/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0765 - val_loss: 0.0606\n",
            "Epoch 746/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0768 - val_loss: 0.0588\n",
            "Epoch 747/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0767 - val_loss: 0.0606\n",
            "Epoch 748/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0771 - val_loss: 0.0585\n",
            "Epoch 749/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0737 - val_loss: 0.0596\n",
            "Epoch 750/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0767 - val_loss: 0.0583\n",
            "Epoch 751/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0756 - val_loss: 0.0593\n",
            "Epoch 752/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0762 - val_loss: 0.0583\n",
            "Epoch 753/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0768 - val_loss: 0.0597\n",
            "Epoch 754/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0734 - val_loss: 0.0580\n",
            "Epoch 755/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0742 - val_loss: 0.0597\n",
            "Epoch 756/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0764 - val_loss: 0.0581\n",
            "Epoch 757/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0759 - val_loss: 0.0598\n",
            "Epoch 758/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0746 - val_loss: 0.0580\n",
            "Epoch 759/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0739 - val_loss: 0.0596\n",
            "Epoch 760/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0737 - val_loss: 0.0580\n",
            "Epoch 761/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0759 - val_loss: 0.0595\n",
            "Epoch 762/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0756 - val_loss: 0.0580\n",
            "Epoch 763/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0743 - val_loss: 0.0600\n",
            "Epoch 764/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0767 - val_loss: 0.0585\n",
            "Epoch 765/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0746 - val_loss: 0.0607\n",
            "Epoch 766/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0744 - val_loss: 0.0587\n",
            "Epoch 767/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0734 - val_loss: 0.0616\n",
            "Epoch 768/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0755 - val_loss: 0.0592\n",
            "Epoch 769/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0764 - val_loss: 0.0617\n",
            "Epoch 770/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0755 - val_loss: 0.0592\n",
            "Epoch 771/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0773 - val_loss: 0.0618\n",
            "Epoch 772/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0756 - val_loss: 0.0594\n",
            "Epoch 773/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0755 - val_loss: 0.0624\n",
            "Epoch 774/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0773 - val_loss: 0.0594\n",
            "Epoch 775/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0757 - val_loss: 0.0628\n",
            "Epoch 776/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0775 - val_loss: 0.0598\n",
            "Epoch 777/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0784 - val_loss: 0.0625\n",
            "Epoch 778/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0789 - val_loss: 0.0604\n",
            "Epoch 779/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0782 - val_loss: 0.0642\n",
            "Epoch 780/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0783 - val_loss: 0.0598\n",
            "Epoch 781/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0764 - val_loss: 0.0643\n",
            "Epoch 782/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0782 - val_loss: 0.0598\n",
            "Epoch 783/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0770 - val_loss: 0.0624\n",
            "Epoch 784/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0755 - val_loss: 0.0588\n",
            "Epoch 785/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0754 - val_loss: 0.0619\n",
            "Epoch 786/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0754 - val_loss: 0.0591\n",
            "Epoch 787/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0774 - val_loss: 0.0618\n",
            "Epoch 788/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0768 - val_loss: 0.0599\n",
            "Epoch 789/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0763 - val_loss: 0.0630\n",
            "Epoch 790/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0770 - val_loss: 0.0594\n",
            "Epoch 791/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0766 - val_loss: 0.0628\n",
            "Epoch 792/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0766 - val_loss: 0.0592\n",
            "Epoch 793/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0769 - val_loss: 0.0623\n",
            "Epoch 794/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0773 - val_loss: 0.0588\n",
            "Epoch 795/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0745 - val_loss: 0.0620\n",
            "Epoch 796/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0770 - val_loss: 0.0586\n",
            "Epoch 797/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0746 - val_loss: 0.0617\n",
            "Epoch 798/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0741 - val_loss: 0.0582\n",
            "Epoch 799/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0727 - val_loss: 0.0609\n",
            "Epoch 800/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0745 - val_loss: 0.0577\n",
            "Epoch 801/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0735 - val_loss: 0.0609\n",
            "Epoch 802/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0749 - val_loss: 0.0583\n",
            "Epoch 803/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0770 - val_loss: 0.0608\n",
            "Epoch 804/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0748 - val_loss: 0.0579\n",
            "Epoch 805/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0776 - val_loss: 0.0605\n",
            "Epoch 806/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0768 - val_loss: 0.0578\n",
            "Epoch 807/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0748 - val_loss: 0.0593\n",
            "Epoch 808/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0729 - val_loss: 0.0574\n",
            "Epoch 809/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0733 - val_loss: 0.0592\n",
            "Epoch 810/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0720 - val_loss: 0.0573\n",
            "Epoch 811/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0749 - val_loss: 0.0596\n",
            "Epoch 812/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0741 - val_loss: 0.0573\n",
            "Epoch 813/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0743 - val_loss: 0.0597\n",
            "Epoch 814/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0749 - val_loss: 0.0571\n",
            "Epoch 815/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0726 - val_loss: 0.0589\n",
            "Epoch 816/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0736 - val_loss: 0.0571\n",
            "Epoch 817/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0720 - val_loss: 0.0586\n",
            "Epoch 818/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0736 - val_loss: 0.0569\n",
            "Epoch 819/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0751 - val_loss: 0.0592\n",
            "Epoch 820/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0749 - val_loss: 0.0571\n",
            "Epoch 821/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0740 - val_loss: 0.0590\n",
            "Epoch 822/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0729 - val_loss: 0.0570\n",
            "Epoch 823/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0744 - val_loss: 0.0591\n",
            "Epoch 824/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0763 - val_loss: 0.0571\n",
            "Epoch 825/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0731 - val_loss: 0.0587\n",
            "Epoch 826/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0729 - val_loss: 0.0572\n",
            "Epoch 827/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0736 - val_loss: 0.0583\n",
            "Epoch 828/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0747 - val_loss: 0.0574\n",
            "Epoch 829/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0735 - val_loss: 0.0581\n",
            "Epoch 830/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0731 - val_loss: 0.0574\n",
            "Epoch 831/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0728 - val_loss: 0.0587\n",
            "Epoch 832/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0750 - val_loss: 0.0576\n",
            "Epoch 833/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0743 - val_loss: 0.0598\n",
            "Epoch 834/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0766 - val_loss: 0.0577\n",
            "Epoch 835/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0742 - val_loss: 0.0613\n",
            "Epoch 836/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0743 - val_loss: 0.0581\n",
            "Epoch 837/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0750 - val_loss: 0.0610\n",
            "Epoch 838/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0733 - val_loss: 0.0582\n",
            "Epoch 839/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0756 - val_loss: 0.0611\n",
            "Epoch 840/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0746 - val_loss: 0.0584\n",
            "Epoch 841/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0761 - val_loss: 0.0627\n",
            "Epoch 842/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0762 - val_loss: 0.0583\n",
            "Epoch 843/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0759 - val_loss: 0.0624\n",
            "Epoch 844/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0760 - val_loss: 0.0582\n",
            "Epoch 845/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0742 - val_loss: 0.0613\n",
            "Epoch 846/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0766 - val_loss: 0.0580\n",
            "Epoch 847/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0743 - val_loss: 0.0614\n",
            "Epoch 848/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0736 - val_loss: 0.0582\n",
            "Epoch 849/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0740 - val_loss: 0.0615\n",
            "Epoch 850/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0756 - val_loss: 0.0582\n",
            "Epoch 851/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0765 - val_loss: 0.0616\n",
            "Epoch 852/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0744 - val_loss: 0.0579\n",
            "Epoch 853/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0737 - val_loss: 0.0615\n",
            "Epoch 854/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0747 - val_loss: 0.0578\n",
            "Epoch 855/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0741 - val_loss: 0.0611\n",
            "Epoch 856/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0748 - val_loss: 0.0571\n",
            "Epoch 857/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0748 - val_loss: 0.0600\n",
            "Epoch 858/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0739 - val_loss: 0.0573\n",
            "Epoch 859/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0745 - val_loss: 0.0595\n",
            "Epoch 860/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0740 - val_loss: 0.0569\n",
            "Epoch 861/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0710 - val_loss: 0.0587\n",
            "Epoch 862/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0721 - val_loss: 0.0568\n",
            "Epoch 863/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0738 - val_loss: 0.0592\n",
            "Epoch 864/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0739 - val_loss: 0.0571\n",
            "Epoch 865/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0750 - val_loss: 0.0591\n",
            "Epoch 866/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0713 - val_loss: 0.0570\n",
            "Epoch 867/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0736 - val_loss: 0.0587\n",
            "Epoch 868/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0726 - val_loss: 0.0569\n",
            "Epoch 869/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0731 - val_loss: 0.0588\n",
            "Epoch 870/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0735 - val_loss: 0.0567\n",
            "Epoch 871/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0736 - val_loss: 0.0593\n",
            "Epoch 872/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0734 - val_loss: 0.0567\n",
            "Epoch 873/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0715 - val_loss: 0.0589\n",
            "Epoch 874/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0713 - val_loss: 0.0565\n",
            "Epoch 875/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0721 - val_loss: 0.0582\n",
            "Epoch 876/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0745 - val_loss: 0.0563\n",
            "Epoch 877/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0720 - val_loss: 0.0578\n",
            "Epoch 878/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0722 - val_loss: 0.0562\n",
            "Epoch 879/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0718 - val_loss: 0.0588\n",
            "Epoch 880/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0727 - val_loss: 0.0564\n",
            "Epoch 881/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0711 - val_loss: 0.0591\n",
            "Epoch 882/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0738 - val_loss: 0.0563\n",
            "Epoch 883/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0732 - val_loss: 0.0596\n",
            "Epoch 884/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0730 - val_loss: 0.0571\n",
            "Epoch 885/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0746 - val_loss: 0.0597\n",
            "Epoch 886/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0744 - val_loss: 0.0570\n",
            "Epoch 887/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0745 - val_loss: 0.0603\n",
            "Epoch 888/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0750 - val_loss: 0.0569\n",
            "Epoch 889/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0755 - val_loss: 0.0601\n",
            "Epoch 890/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0740 - val_loss: 0.0571\n",
            "Epoch 891/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0737 - val_loss: 0.0602\n",
            "Epoch 892/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0728 - val_loss: 0.0570\n",
            "Epoch 893/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0728 - val_loss: 0.0596\n",
            "Epoch 894/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0739 - val_loss: 0.0577\n",
            "Epoch 895/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0722 - val_loss: 0.0598\n",
            "Epoch 896/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0736 - val_loss: 0.0577\n",
            "Epoch 897/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0745 - val_loss: 0.0594\n",
            "Epoch 898/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0744 - val_loss: 0.0575\n",
            "Epoch 899/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0725 - val_loss: 0.0594\n",
            "Epoch 900/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0739 - val_loss: 0.0577\n",
            "Epoch 901/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0741 - val_loss: 0.0607\n",
            "Epoch 902/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0738 - val_loss: 0.0574\n",
            "Epoch 903/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0739 - val_loss: 0.0604\n",
            "Epoch 904/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0725 - val_loss: 0.0573\n",
            "Epoch 905/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0765 - val_loss: 0.0609\n",
            "Epoch 906/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0753 - val_loss: 0.0572\n",
            "Epoch 907/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0736 - val_loss: 0.0605\n",
            "Epoch 908/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0732 - val_loss: 0.0570\n",
            "Epoch 909/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0747 - val_loss: 0.0605\n",
            "Epoch 910/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0728 - val_loss: 0.0573\n",
            "Epoch 911/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0738 - val_loss: 0.0597\n",
            "Epoch 912/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0742 - val_loss: 0.0572\n",
            "Epoch 913/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0738 - val_loss: 0.0591\n",
            "Epoch 914/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0746 - val_loss: 0.0569\n",
            "Epoch 915/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0722 - val_loss: 0.0585\n",
            "Epoch 916/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0724 - val_loss: 0.0567\n",
            "Epoch 917/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0732 - val_loss: 0.0589\n",
            "Epoch 918/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0736 - val_loss: 0.0567\n",
            "Epoch 919/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0731 - val_loss: 0.0586\n",
            "Epoch 920/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0732 - val_loss: 0.0566\n",
            "Epoch 921/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0715 - val_loss: 0.0586\n",
            "Epoch 922/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0742 - val_loss: 0.0564\n",
            "Epoch 923/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0732 - val_loss: 0.0594\n",
            "Epoch 924/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0734 - val_loss: 0.0570\n",
            "Epoch 925/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0725 - val_loss: 0.0589\n",
            "Epoch 926/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0749 - val_loss: 0.0569\n",
            "Epoch 927/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0735 - val_loss: 0.0584\n",
            "Epoch 928/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0716 - val_loss: 0.0568\n",
            "Epoch 929/1000\n",
            "3298/3298 [==============================] - 0s 63us/step - loss: 0.0740 - val_loss: 0.0603\n",
            "Epoch 930/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0731 - val_loss: 0.0566\n",
            "Epoch 931/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0722 - val_loss: 0.0598\n",
            "Epoch 932/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0732 - val_loss: 0.0566\n",
            "Epoch 933/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0750 - val_loss: 0.0600\n",
            "Epoch 934/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0741 - val_loss: 0.0569\n",
            "Epoch 935/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0714 - val_loss: 0.0586\n",
            "Epoch 936/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0711 - val_loss: 0.0564\n",
            "Epoch 937/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0720 - val_loss: 0.0581\n",
            "Epoch 938/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0724 - val_loss: 0.0559\n",
            "Epoch 939/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0726 - val_loss: 0.0579\n",
            "Epoch 940/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0714 - val_loss: 0.0559\n",
            "Epoch 941/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0727 - val_loss: 0.0579\n",
            "Epoch 942/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0719 - val_loss: 0.0558\n",
            "Epoch 943/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0724 - val_loss: 0.0586\n",
            "Epoch 944/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0716 - val_loss: 0.0559\n",
            "Epoch 945/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0709 - val_loss: 0.0578\n",
            "Epoch 946/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0721 - val_loss: 0.0555\n",
            "Epoch 947/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0724 - val_loss: 0.0579\n",
            "Epoch 948/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0724 - val_loss: 0.0557\n",
            "Epoch 949/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0716 - val_loss: 0.0578\n",
            "Epoch 950/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0728 - val_loss: 0.0562\n",
            "Epoch 951/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0730 - val_loss: 0.0586\n",
            "Epoch 952/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0724 - val_loss: 0.0561\n",
            "Epoch 953/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0725 - val_loss: 0.0581\n",
            "Epoch 954/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0718 - val_loss: 0.0565\n",
            "Epoch 955/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0730 - val_loss: 0.0580\n",
            "Epoch 956/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0711 - val_loss: 0.0561\n",
            "Epoch 957/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0735 - val_loss: 0.0582\n",
            "Epoch 958/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0751 - val_loss: 0.0563\n",
            "Epoch 959/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0734 - val_loss: 0.0580\n",
            "Epoch 960/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0715 - val_loss: 0.0559\n",
            "Epoch 961/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0733 - val_loss: 0.0579\n",
            "Epoch 962/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0739 - val_loss: 0.0558\n",
            "Epoch 963/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0716 - val_loss: 0.0576\n",
            "Epoch 964/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0725 - val_loss: 0.0555\n",
            "Epoch 965/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0711 - val_loss: 0.0575\n",
            "Epoch 966/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0719 - val_loss: 0.0560\n",
            "Epoch 967/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0742 - val_loss: 0.0576\n",
            "Epoch 968/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0717 - val_loss: 0.0565\n",
            "Epoch 969/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0712 - val_loss: 0.0578\n",
            "Epoch 970/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0724 - val_loss: 0.0561\n",
            "Epoch 971/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0722 - val_loss: 0.0582\n",
            "Epoch 972/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0720 - val_loss: 0.0562\n",
            "Epoch 973/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0717 - val_loss: 0.0587\n",
            "Epoch 974/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0730 - val_loss: 0.0566\n",
            "Epoch 975/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0753 - val_loss: 0.0591\n",
            "Epoch 976/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0716 - val_loss: 0.0559\n",
            "Epoch 977/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0713 - val_loss: 0.0586\n",
            "Epoch 978/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0719 - val_loss: 0.0555\n",
            "Epoch 979/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0713 - val_loss: 0.0579\n",
            "Epoch 980/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0700 - val_loss: 0.0553\n",
            "Epoch 981/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0711 - val_loss: 0.0584\n",
            "Epoch 982/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0704 - val_loss: 0.0556\n",
            "Epoch 983/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0706 - val_loss: 0.0578\n",
            "Epoch 984/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0714 - val_loss: 0.0554\n",
            "Epoch 985/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0723 - val_loss: 0.0578\n",
            "Epoch 986/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0732 - val_loss: 0.0558\n",
            "Epoch 987/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0698 - val_loss: 0.0584\n",
            "Epoch 988/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0722 - val_loss: 0.0563\n",
            "Epoch 989/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0729 - val_loss: 0.0587\n",
            "Epoch 990/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0733 - val_loss: 0.0565\n",
            "Epoch 991/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0705 - val_loss: 0.0582\n",
            "Epoch 992/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0732 - val_loss: 0.0560\n",
            "Epoch 993/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0707 - val_loss: 0.0584\n",
            "Epoch 994/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0710 - val_loss: 0.0557\n",
            "Epoch 995/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0730 - val_loss: 0.0577\n",
            "Epoch 996/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0695 - val_loss: 0.0548\n",
            "Epoch 997/1000\n",
            "3298/3298 [==============================] - 0s 60us/step - loss: 0.0716 - val_loss: 0.0571\n",
            "Epoch 998/1000\n",
            "3298/3298 [==============================] - 0s 62us/step - loss: 0.0694 - val_loss: 0.0546\n",
            "Epoch 999/1000\n",
            "3298/3298 [==============================] - 0s 59us/step - loss: 0.0708 - val_loss: 0.0570\n",
            "Epoch 1000/1000\n",
            "3298/3298 [==============================] - 0s 61us/step - loss: 0.0699 - val_loss: 0.0548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X94jTMXFTA6K",
        "colab_type": "text"
      },
      "source": [
        "## Model Selection<a id=\"Selection\"></a>\n",
        "The following models were compared\n",
        "- Bidirectional LSTM with Time Distributed(Best performance)\n",
        "- LSTM with Time Distributed\n",
        "- MLP\n",
        "- Bidirectional GRU with Time Distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "68by3caOTA6L",
        "colab_type": "code",
        "outputId": "5765dbbd-ae9f-447a-e91c-68fa04c1203a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "def ai_errors(predictions, observations, history = None) :\n",
        "    '''\n",
        "    PURPOSE: Provide descriptive statistics on the predicted output versus the observed measurments\n",
        "    METHOD:  Take the errors of the predictions and answers and then calculate standard descriptive statistics\n",
        "    INPUT:   predictions - 2D array of predictions of observed output\n",
        "             observations - 2D array measurements of observed output\n",
        "             history - Keras history model for displaying model loss, default is None if not available\n",
        "    OUTPUT:\n",
        "    '''\n",
        "    errors = []\n",
        "    for i in range(len(predictions)) :\n",
        "        for j in range(len(predictions[i])) :\n",
        "            # Calculate errors\n",
        "            error = predictions[i][j] - observations[i][j]\n",
        "            errors.append(error)\n",
        "    \n",
        "    # Display history and erros\n",
        "    plt.figure(1)\n",
        "    plt.hist(errors, bins = 50)\n",
        "    plt.title('error histogram')\n",
        "    plt.xlabel('error')\n",
        "    plt.ylabel('frequency')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.figure(2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    return pd.DataFrame(errors)\n",
        "\n",
        "# Predict values\n",
        "wind_predictions = model_wind.predict(X_test)\n",
        "lat_predictions = model_lat.predict(X_test)\n",
        "long_predictions = model_long.predict(X_test)\n",
        "\n",
        "# Scale back our predictions\n",
        "# Wind\n",
        "wind_predictions_scaled = [scaler.inverse_transform([[0,0,winds[0],0,0,0,0,0,0,0,0] for winds in prediction])\n",
        "                           for prediction in wind_predictions]\n",
        "y_wind_test_scaled = [scaler.inverse_transform([[0,0,winds[0],0,0,0,0,0,0,0,0] for winds in observation])\n",
        "                      for observation in y_test_wind]\n",
        "# Latitude\n",
        "lat_predictions_scaled = [scaler.inverse_transform([[lat[0],0,0,0,0,0,0,0,0,0,0] for lat in prediction])\n",
        "                          for prediction in lat_predictions]\n",
        "y_lat_test_scaled = [scaler.inverse_transform([[lat[0],0,0,0,0,0,0,0,0,0,0] for lat in observation])\n",
        "                     for observation in y_test_lat]\n",
        "# Longitude\n",
        "long_predictions_scaled = [scaler.inverse_transform([[0,long[0],0,0,0,0,0,0,0,0,0] for long in prediction])\n",
        "                           for prediction in long_predictions]\n",
        "y_long_test_scaled = [scaler.inverse_transform([[0,long[0],0,0,0,0,0,0,0,0,0] for long in observation])\n",
        "                      for observation in y_test_long]\n",
        "\n",
        "# Record wind predictions and observations\n",
        "print(\"Wind\")\n",
        "wind_predictions = [[pred[2] for pred in hurricanes_pred] for hurricanes_pred in wind_predictions_scaled]\n",
        "wind_observations = [[obsrv[2] for obsrv in hurricanes_obsrv] for hurricanes_obsrv in y_wind_test_scaled]\n",
        "\n",
        "# Present Errors\n",
        "ai_errors(wind_predictions, wind_observations, model_wind_history).describe()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wind\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-1.516831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>19.431081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-93.354545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-12.478253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.156914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>11.313929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>64.106746</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count  4125.000000\n",
              "mean     -1.516831\n",
              "std      19.431081\n",
              "min     -93.354545\n",
              "25%     -12.478253\n",
              "50%      -0.156914\n",
              "75%      11.313929\n",
              "max      64.106746"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "oB2lPcNjTA6N",
        "colab_type": "code",
        "outputId": "1b3f5961-7e50-4687-f601-092f26bd0263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "print(\"Lat\")\n",
        "lat_predictions = [[pred[0] for pred in hurricanes_pred] for hurricanes_pred in lat_predictions_scaled]\n",
        "lat_observations = [[obsrv[0] for obsrv in hurricanes_obsrv] for hurricanes_obsrv in y_lat_test_scaled]\n",
        "ai_errors(lat_predictions, lat_observations, model_lat_history).describe()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh74AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.623246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.330510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-23.107975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.103693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.870390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.838730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.327249</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count  4125.000000\n",
              "mean      0.623246\n",
              "std       4.330510\n",
              "min     -23.107975\n",
              "25%      -1.103693\n",
              "50%       0.870390\n",
              "75%       2.838730\n",
              "max      17.327249"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSvYUSOdTA6R",
        "colab_type": "code",
        "outputId": "411cb43f-0ca4-4ac2-8623-c9982208158a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "print(\"Long\")\n",
        "long_predictions = [[pred[1] for pred in hurricanes_pred] for hurricanes_pred in long_predictions_scaled]\n",
        "long_observations = [[obsrv[1] for obsrv in hurricanes_obsrv] for hurricanes_obsrv in y_long_test_scaled]\n",
        "ai_errors(long_predictions, long_observations, model_long_history).describe()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Long\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npGAhIiItUrAQEZEW/f8Otj0e8FrqAQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.562773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>6.636024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-32.663445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-3.537567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.673196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.282909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>39.855995</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count  4125.000000\n",
              "mean     -0.562773\n",
              "std       6.636024\n",
              "min     -32.663445\n",
              "25%      -3.537567\n",
              "50%      -0.673196\n",
              "75%       2.282909\n",
              "max      39.855995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynICeA8zTA6V",
        "colab_type": "text"
      },
      "source": [
        "## Model Evaluation & Benchmarks<a id=\"Benchmarks\"></a>\n",
        "\n",
        "The machine learning neural network will use 2 main methods of applied evaluation. The first will be evaluated compared to the other models that predict Atlantic hurricanes. The forecast errors have been loaded into each hurricane object corresponding to their forecast model; both the OFCL (official track) and the BCD5 (model using multivariate regression). The BCD5 model is \"the CLP5 (track) and DSF5 (intensity) models merged\" that uses the best track as input. BCD5 is similar to OCD5 except for the inputs to the models. The *O* is for operational input while the *B* is for best track input. The best track is only available post-season and is better than the operational input.\n",
        "\n",
        "### References\n",
        "http://www.hurricanecity.com/models/models.cgi?page=models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKQPNFILTA6X",
        "colab_type": "code",
        "outputId": "9ea7d0a8-95d1-44a7-8324-322e21a9ccfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_data = hurdat2('data/hurdat2-1851-2017-050118.txt')\n",
        "\n",
        "# Parse in hurricanes\n",
        "hurricanes_2017 = dict()\n",
        "print(\"Transforming 2017 HURDAT2 into objects . . .\")\n",
        "for index, entry in test_data.hurricanes.iterrows() :\n",
        "    print(\"Transforming {}/{} entries from HURDAT2\".format(index + 1, len(dataset.hurricanes)), end = \"\\r\")\n",
        "    # Filter to capture 2017 data\n",
        "    if entry['storm_id'][-4:] != '2017' :\n",
        "        continue\n",
        "    if entry['storm_id'] not in hurricanes_2017 :\n",
        "        hurricanes_2017[entry['storm_id']] = hurricane(entry['storm_name'], entry['storm_id'])\n",
        "        storm_ids[entry['storm_id']] = entry['storm_name']\n",
        "    # Add entry to hurricane\n",
        "    hurricanes_2017[entry['storm_id']].add_entry(entry[2:])\n",
        "print(\"\\nDone!\")\n",
        "\n",
        "# Filter storms that have more than 6 entries. We need at least 6 to calculate 5 speed vectors\n",
        "storms_filter = [storm for storm in hurricanes_2017.values() if len(storm.entries) > 6]\n",
        "\n",
        "# Begin creating hurricane forecast and track predictions\n",
        "tracks = {\n",
        "    'storms' : [], # Reference storm\n",
        "    'inputs' : [], # The inputs for the ai\n",
        "    'valid_times' : [], # The valid time to compare to the error database\n",
        "}\n",
        "for index, storm in enumerate(storms_filter) :\n",
        "    # Create inputs to ai. ai requires scaled data as input\n",
        "    entries = [entry[1] for entry in sorted(storm.entries.items())] # Extracts data from data structure\n",
        "    \n",
        "    # Scale the entries\n",
        "    for start_index in range(1, len(entries) - 5) : # Go through each entry\n",
        "        # Build feature extraction\n",
        "        extracted_features = []\n",
        "        valid_time = None # Going to be set to the last element in the series\n",
        "        for pivot in range(start_index, start_index + 5) :\n",
        "            extracted_features.append(np.array(list(feature_extraction(entries[pivot], entries[pivot - 1]).values())))\n",
        "            if pivot is start_index + 4 : # We're on the last element\n",
        "                valid_time = entries[pivot]['entry_time']\n",
        "        \n",
        "        # If there's an incomplete value we can't process, skip it\n",
        "        if any(None in entry for entry in extracted_features) :\n",
        "            continue\n",
        "            \n",
        "        # Scale extracted features        \n",
        "        scaled_entries = scaler.transform(extracted_features)\n",
        "        \n",
        "        # Add to our results\n",
        "        tracks['storms'].append(storm)\n",
        "        tracks['inputs'].append(scaled_entries.tolist())\n",
        "        tracks['valid_times'].append(valid_time)\n",
        "        \n",
        "    print(\"\\rDone with track processing {}/{} storms\".format(index + 1, len(storms_filter)), end = '')\n",
        "tracks['inputs'] = np.array(tracks['inputs'])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transforming 2017 HURDAT2 into objects . . .\n",
            "Transforming 50303/20291 entries from HURDAT2\n",
            "Done!\n",
            "Done with track processing 18/18 storms"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b63CD4pbTA6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tracks['wind_predictions_raw'] = model_wind.predict(tracks['inputs'])\n",
        "tracks['lat_predictions_raw'] = model_lat.predict(tracks['inputs'])\n",
        "tracks['long_predictions_raw'] = model_long.predict(tracks['inputs'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGdhct5GTA6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to return the distance between two coordinates in nautical miles\n",
        "import math\n",
        "\n",
        "def distance(origin, destination):\n",
        "    lat1, lon1 = origin\n",
        "    lat2, lon2 = destination\n",
        "    radius = 6371 # km\n",
        "\n",
        "    dlat = math.radians(lat2-lat1)\n",
        "    dlon = math.radians(lon2-lon1)\n",
        "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
        "        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "    d = radius * c\n",
        "\n",
        "    return d * 0.539957 # km to nautical miles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ir6U5L-TA6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale back and store our wind predictions and our lat, long predictions\n",
        "tracks['wind_predictions'] = []\n",
        "tracks['lat_predictions'] = []\n",
        "tracks['long_predictions'] = []\n",
        "intensity_errors = {\n",
        "    '24' : [],\n",
        "    '48' : [],\n",
        "    '72' : [],\n",
        "    '96' : [],\n",
        "    '120' : []\n",
        "}\n",
        "track_errors = {\n",
        "    '24' : [],\n",
        "    '48' : [],\n",
        "    '72' : [],\n",
        "    '96' : [],\n",
        "    '120' : []\n",
        "}\n",
        "for index, prediction in enumerate(tracks['wind_predictions_raw']) :\n",
        "    # Use our standard scaler to scale the raw predictions back\n",
        "    winds_scaled = [scaler.inverse_transform([[0,0,winds[0],0,0,0,0,0,0,0,0] for winds in prediction])] # Index 2 is winds\n",
        "    lat_scaled = [scaler.inverse_transform([[lats[0],0,0,0,0,0,0,0,0,0,0] for lats in tracks['lat_predictions_raw'][index]])] # Index 0 is lat\n",
        "    long_scaled = [scaler.inverse_transform([[0,longs[0],0,0,0,0,0,0,0,0,0] for longs in tracks['long_predictions_raw'][index]])] # Index 1 is long\n",
        "    \n",
        "    # Extract the wind prediction from data structure and store into new data structure\n",
        "    for i in range(len(winds_scaled)) :        \n",
        "        # The new data structure is a tuple of (wind, storm_id, valid_time, forecast_time)\n",
        "        wind_predictions = []\n",
        "        lat_predictions = []\n",
        "        long_predictions = []\n",
        "        for step, pred in enumerate(winds_scaled[i]) :\n",
        "            wind = pred[2]\n",
        "            lat = lat_scaled[i][step][0]\n",
        "            long = long_scaled[i][step][1]\n",
        "            \n",
        "            storm_id = tracks['storms'][index].id\n",
        "            valid_time = tracks['valid_times'][index]\n",
        "            forecast_time = valid_time + datetime.timedelta(days = step + 1)\n",
        "            \n",
        "            # See if we can find the error\n",
        "            if forecast_time in hurricanes_2017[storm_id].entries :\n",
        "                wind_truth = hurricanes_2017[storm_id].entries[forecast_time]['max_wind']\n",
        "                lat_truth = hurricanes_2017[storm_id].entries[forecast_time]['lat']\n",
        "                long_truth = hurricanes_2017[storm_id].entries[forecast_time]['long']\n",
        "                intensity_error = abs(wind_truth - wind)\n",
        "                track_error = distance((lat_truth,long_truth), (lat, long))\n",
        "                \n",
        "                wind_predictions.append({\n",
        "                    'ai-wind' : wind,\n",
        "                    'truth' : wind_truth,\n",
        "                    'storm_id' : storm_id,\n",
        "                    'valid_time' : valid_time,\n",
        "                    'forecast_time' : forecast_time\n",
        "                })\n",
        "                lat_predictions.append({\n",
        "                    'ai-lat' : lat,\n",
        "                    'truth' : lat_truth,\n",
        "                    'storm_id' : storm_id,\n",
        "                    'valid_time' : valid_time,\n",
        "                    'forecast_time' : forecast_time\n",
        "                })\n",
        "                long_predictions.append({\n",
        "                    'ai-long' : long,\n",
        "                    'truth' : long_truth,\n",
        "                    'storm_id' : storm_id,\n",
        "                    'valid_time' : valid_time,\n",
        "                    'forecast_time' : forecast_time\n",
        "                })\n",
        "                if step is 0 :\n",
        "                    intensity_errors['24'].append(intensity_error)\n",
        "                    track_errors['24'].append(track_error)\n",
        "                if step is 1 :\n",
        "                    intensity_errors['48'].append(intensity_error)\n",
        "                    track_errors['48'].append(track_error)\n",
        "                if step is 2 :\n",
        "                    intensity_errors['72'].append(intensity_error)\n",
        "                    track_errors['72'].append(track_error)\n",
        "                if step is 3 :\n",
        "                    intensity_errors['96'].append(intensity_error)\n",
        "                    track_errors['96'].append(track_error)\n",
        "                if step is 4 :\n",
        "                    intensity_errors['120'].append(intensity_error)\n",
        "                    track_errors['120'].append(track_error)\n",
        "                    \n",
        "        tracks['wind_predictions'].append(wind_predictions)\n",
        "        tracks['lat_predictions'].append(lat_predictions)\n",
        "        tracks['long_predictions'].append(long_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlOkSiHUTA6h",
        "colab_type": "code",
        "outputId": "9c1f4e87-d38d-4260-b06b-d8a29b064f3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['24']).describe()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>432.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.067501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>11.846847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.095776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.368947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>11.564249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>19.436491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>73.006323</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  432.000000\n",
              "mean    14.067501\n",
              "std     11.846847\n",
              "min      0.095776\n",
              "25%      5.368947\n",
              "50%     11.564249\n",
              "75%     19.436491\n",
              "max     73.006323"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epSK3WCiTA6j",
        "colab_type": "code",
        "outputId": "624cdd5a-bfa3-4e2e-b5a7-8bc305bdd338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['48']).describe()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>378.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>18.963914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>14.748939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.021094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.144317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15.494117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>27.574900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>75.301771</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  378.000000\n",
              "mean    18.963914\n",
              "std     14.748939\n",
              "min      0.021094\n",
              "25%      7.144317\n",
              "50%     15.494117\n",
              "75%     27.574900\n",
              "max     75.301771"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c_0j5MzTA6l",
        "colab_type": "code",
        "outputId": "05c627f2-d53e-4c0b-dcda-16d6da8eb591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['72']).describe()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>327.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>21.265267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>16.891219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.228403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.643285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>18.060204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>31.838729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>79.765987</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  327.000000\n",
              "mean    21.265267\n",
              "std     16.891219\n",
              "min      0.228403\n",
              "25%      6.643285\n",
              "50%     18.060204\n",
              "75%     31.838729\n",
              "max     79.765987"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-IRvW50TA6o",
        "colab_type": "code",
        "outputId": "8be6bee3-9dc3-4734-ecd2-6782c88ec9f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['96']).describe()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>285.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>23.012680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>18.749728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.072621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.926264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>17.588649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>36.292549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>81.005248</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  285.000000\n",
              "mean    23.012680\n",
              "std     18.749728\n",
              "min      0.072621\n",
              "25%      7.926264\n",
              "50%     17.588649\n",
              "75%     36.292549\n",
              "max     81.005248"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-ZM0xu8TA6s",
        "colab_type": "code",
        "outputId": "9d23def5-449f-4dad-d08d-d04b9bdb9c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(intensity_errors['120']).describe()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>249.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>24.137672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>20.492001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.045814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.560330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>18.494293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>37.847882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>93.422785</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  249.000000\n",
              "mean    24.137672\n",
              "std     20.492001\n",
              "min      0.045814\n",
              "25%      7.560330\n",
              "50%     18.494293\n",
              "75%     37.847882\n",
              "max     93.422785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCERfJiJTA6w",
        "colab_type": "code",
        "outputId": "4881f5aa-566b-4ece-a147-b18262ca60fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['24']).describe()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>432.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>260.587143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>183.911685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.366501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>144.768180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>210.803645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>327.016801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1110.035792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   432.000000\n",
              "mean    260.587143\n",
              "std     183.911685\n",
              "min       4.366501\n",
              "25%     144.768180\n",
              "50%     210.803645\n",
              "75%     327.016801\n",
              "max    1110.035792"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c4Jc9hBTA60",
        "colab_type": "code",
        "outputId": "649ee947-f227-48a4-dd2a-6f8c0f6da294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['48']).describe()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>378.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>396.291383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>291.240091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.025399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>181.084609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>317.126789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>562.710229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1648.171924</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   378.000000\n",
              "mean    396.291383\n",
              "std     291.240091\n",
              "min       6.025399\n",
              "25%     181.084609\n",
              "50%     317.126789\n",
              "75%     562.710229\n",
              "max    1648.171924"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40GaDhbcTA63",
        "colab_type": "code",
        "outputId": "9fdb0cda-8a44-4d44-ea8f-424d7976482f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['72']).describe()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>327.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>581.718347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>402.029337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>21.166599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>284.140988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>436.842153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>814.487646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1997.220919</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   327.000000\n",
              "mean    581.718347\n",
              "std     402.029337\n",
              "min      21.166599\n",
              "25%     284.140988\n",
              "50%     436.842153\n",
              "75%     814.487646\n",
              "max    1997.220919"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaulJVPBTA66",
        "colab_type": "code",
        "outputId": "f5e0da25-16bd-4b5b-932b-7eadbc7a62fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['96']).describe()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>285.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>761.414455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>528.443388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>44.203010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>360.283111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>601.840060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1020.012982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2107.289613</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   285.000000\n",
              "mean    761.414455\n",
              "std     528.443388\n",
              "min      44.203010\n",
              "25%     360.283111\n",
              "50%     601.840060\n",
              "75%    1020.012982\n",
              "max    2107.289613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_lHpN7tTA69",
        "colab_type": "code",
        "outputId": "32e0857e-94e7-463d-f873-e5e150a9eb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(track_errors['120']).describe()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>249.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>924.283498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>635.770523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>69.554098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>389.089144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>741.679576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1342.346244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2482.168650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   249.000000\n",
              "mean    924.283498\n",
              "std     635.770523\n",
              "min      69.554098\n",
              "25%     389.089144\n",
              "50%     741.679576\n",
              "75%    1342.346244\n",
              "max    2482.168650"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRbJiBIjTA6_",
        "colab_type": "code",
        "outputId": "66eadd65-6103-40d8-9fc7-86098fad8edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Compare predictions when we can find them\n",
        "import errors\n",
        "errordb = errors.models.models(\"errors/1970-present_OFCL_v_BCD5_ind_ATL_TI_errors_noTDs.txt\")\n",
        "ai_wind_errors = []\n",
        "ai_track_errors = []\n",
        "bcd5_wind_errors = []\n",
        "bcd5_track_errors = []\n",
        "for index, prediction in enumerate(tracks['wind_predictions']) :\n",
        "    # Find the time stamp for the storm ID in the error database\n",
        "    if prediction == [] :\n",
        "      continue\n",
        "    valid_time = prediction[0]['valid_time']\n",
        "    storm_id = prediction[0]['storm_id']\n",
        "    # Check to see if we have error for this storm and at the valid time\n",
        "    if storm_id in errordb.models['BCD5'].storm and valid_time in errordb.models['BCD5'].storm[storm_id] :\n",
        "        print(\"Found {} at {}\".format(storm_id, valid_time))\n",
        "        # If we find it, compare\n",
        "        for i, forecast in enumerate(prediction) :\n",
        "            # See if we can find another prediction like that in the error database\n",
        "            if errordb.models['BCD5'].storm[storm_id][valid_time]['intensity_forecast'][forecast['forecast_time'].to_pydatetime()] :\n",
        "                print(\"\\tIntensity Truth: {}, AI forecast: {}, BCD5 forecast: {}\".format(forecast['truth'],\n",
        "                                                                                         forecast['ai-wind'],\n",
        "                                                                                         errordb.models['BCD5'].storm[storm_id][valid_time]['track_forecast'][forecast['forecast_time'].to_pydatetime()]))\n",
        "                print(\"\\tTrajectory Truth: {}, {}; AI forecast: {}, {} ; AI error: {} BCD5 error: {}\".format(tracks['lat_predictions'][index][i]['truth'],\n",
        "                                                                                                       tracks['long_predictions'][index][i]['truth'],\n",
        "                                                                                                       tracks['lat_predictions'][index][i]['ai-lat'],\n",
        "                                                                                                       tracks['long_predictions'][index][i]['ai-long'],\n",
        "                                                                                                       distance((tracks['lat_predictions'][index][i]['truth'], tracks['long_predictions'][index][i]['truth']), (tracks['lat_predictions'][index][i]['ai-lat'], tracks['long_predictions'][index][i]['ai-long'])),\n",
        "                                                                                                       errordb.models['BCD5'].storm[storm_id][valid_time]['intensity_forecast'][forecast['forecast_time'].to_pydatetime()]\n",
        "                                                                                                      ))\n",
        "                ai_wind_errors.append(abs(forecast['truth'] - forecast['ai-wind']))\n",
        "                ai_track_errors.append(abs(distance((tracks['lat_predictions'][index][i]['truth'], tracks['long_predictions'][index][i]['truth']), (tracks['lat_predictions'][index][i]['ai-lat'], tracks['long_predictions'][index][i]['ai-long']))))\n",
        "                bcd5_wind_errors.append(abs(errordb.models['BCD5'].storm[storm_id][valid_time]['track_forecast'][forecast['forecast_time'].to_pydatetime()]))                \n",
        "                bcd5_track_errors.append(abs(errordb.models['BCD5'].storm[storm_id][valid_time]['intensity_forecast'][forecast['forecast_time'].to_pydatetime()]))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found AL012017 at 2017-04-20 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 45.13278434518725, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 40.0, 46.3; AI forecast: 31.878912580013274, 40.403824391961095 ; AI error: 565.2598025507049 BCD5 error: 391.5\n",
            "Found AL012017 at 2017-04-20 12:00:00\n",
            "Found AL012017 at 2017-04-20 18:00:00\n",
            "Found AL012017 at 2017-04-21 00:00:00\n",
            "Found AL012017 at 2017-04-21 06:00:00\n",
            "Found AL032017 at 2017-06-21 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 49.39181759953499, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 28.5, 93.4; AI forecast: 28.581614565849303, 88.7944468677044 ; AI error: 242.95058297581528 BCD5 error: 42.6\n",
            "Found AL032017 at 2017-06-21 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 50.97920894622803, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 29.410256111621855, 88.7694480895996 ; AI error: 252.6469235734241 BCD5 error: 36.6\n",
            "Found AL032017 at 2017-06-21 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 54.14319887757301, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 30.5, 93.8; AI forecast: 30.737903654575348, 88.34924409985543 ; AI error: 281.9705156586292 BCD5 error: 99.9\n",
            "Found AL032017 at 2017-06-21 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 51.60675950348377, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 31.6, 93.8; AI forecast: 31.089749825000762, 88.44115725755691 ; AI error: 276.46259899432545 BCD5 error: 106.4\n",
            "Found AL032017 at 2017-06-22 00:00:00\n",
            "Found AL032017 at 2017-06-22 06:00:00\n",
            "Found AL032017 at 2017-06-22 12:00:00\n",
            "Found AL032017 at 2017-06-22 18:00:00\n",
            "Found AL062017 at 2017-07-31 18:00:00\n",
            "Found AL072017 at 2017-08-08 00:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 61.410854160785675, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 20.2, 90.9; AI forecast: 21.932567065954206, 86.18999541401863 ; AI error: 283.63103991572206 BCD5 error: 24.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 64.0288171172142, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 20.3, 95.5; AI forecast: 25.363840401172638, 89.22939033508301 ; AI error: 461.1965502808704 BCD5 error: 133.4\n",
            "Found AL072017 at 2017-08-08 06:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 60.861755311489105, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 20.4, 92.2; AI forecast: 23.649843019247054, 86.87909821271896 ; AI error: 354.59330837842634 BCD5 error: 28.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 62.74645924568176, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 20.0, 96.8; AI forecast: 27.951207363605498, 88.8810828447342 ; AI error: 645.0613372593106 BCD5 error: 160.4\n",
            "Found AL072017 at 2017-08-08 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 53.97850155830383, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 20.2, 93.3; AI forecast: 23.709495294094086, 88.2093411564827 ; AI error: 353.1509398371634 BCD5 error: 78.2\n",
            "Found AL072017 at 2017-08-08 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 44.64078663382679, BCD5 forecast: -31.0\n",
            "\tTrajectory Truth: 20.2, 94.4; AI forecast: 23.378821462392807, 89.7615275323391 ; AI error: 321.35600551218 BCD5 error: 86.3\n",
            "Found AL072017 at 2017-08-09 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 45.272874594666064, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 20.3, 95.5; AI forecast: 22.551086583733557, 91.28657848238944 ; AI error: 271.4970195038175 BCD5 error: 58.6\n",
            "Found AL072017 at 2017-08-09 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 51.90769471228123, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 20.0, 96.8; AI forecast: 22.143620920181274, 90.00458986163139 ; AI error: 401.8254551689425 BCD5 error: 41.1\n",
            "Found AL072017 at 2017-08-09 12:00:00\n",
            "Found AL082017 at 2017-08-13 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 42.489684857428074, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 29.2, 72.1; AI forecast: 25.448577070236205, 73.11617032289504 ; AI error: 231.66346841767162 BCD5 error: 16.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 48.37576098740101, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 31.5, 72.3; AI forecast: 27.42591872215271, 75.53436544835567 ; AI error: 297.31484109369137 BCD5 error: 73.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 56.15538716316223, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 35.4, 69.5; AI forecast: 29.82040535211563, 75.93693022429943 ; AI error: 466.8942732047616 BCD5 error: 7.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 58.23998302221298, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 40.7, 56.2; AI forecast: 32.857998156547545, 75.517524933815 ; AI error: 1038.3217430708582 BCD5 error: 529.1\n",
            "Found AL082017 at 2017-08-13 12:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 44.22144478186965, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.7, 72.2; AI forecast: 27.28557288646698, 72.73195188939572 ; AI error: 147.65530154532289 BCD5 error: 37.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 53.41474689543247, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 32.3, 72.1; AI forecast: 30.27936028242111, 73.95645281672478 ; AI error: 154.23691249139986 BCD5 error: 85.2\n",
            "\tIntensity Truth: 85.0, AI forecast: 58.26091766357422, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 36.8, 67.1; AI forecast: 33.17563543319702, 72.6117094963789 ; AI error: 347.5337418728168 BCD5 error: 80.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 56.15674316883087, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 42.2, 52.0; AI forecast: 36.04850809574127, 70.33659091442823 ; AI error: 928.0555703712782 BCD5 error: 673.2\n",
            "Found AL082017 at 2017-08-13 18:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 48.745052963495255, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 30.2, 72.3; AI forecast: 29.418328058719634, 71.99393512010575 ; AI error: 49.56650749321925 BCD5 error: 52.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 55.8119797706604, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 33.2, 71.8; AI forecast: 33.03067684173584, 71.8933091878891 ; AI error: 11.196904723071208 BCD5 error: 71.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 56.68092802166939, BCD5 forecast: -39.0\n",
            "\tTrajectory Truth: 38.2, 64.1; AI forecast: 35.7847008228302, 69.06197203248739 ; AI error: 278.59576008076044 BCD5 error: 213.5\n",
            "Found AL082017 at 2017-08-14 00:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 49.45220731198788, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 30.8, 72.3; AI forecast: 30.72607361078262, 71.68563042283058 ; AI error: 32.00589038829125 BCD5 error: 49.2\n",
            "\tIntensity Truth: 75.0, AI forecast: 52.04500176012516, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 34.2, 71.0; AI forecast: 33.96406219005585, 71.04162784814835 ; AI error: 14.316268804361043 BCD5 error: 39.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 52.913361713290215, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 39.4, 60.4; AI forecast: 36.481192398071286, 67.66755419671536 ; AI error: 386.0062994188706 BCD5 error: 375.7\n",
            "Found AL082017 at 2017-08-14 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 47.29533974081278, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.5, 72.3; AI forecast: 31.30994620323181, 71.70793973803521 ; AI error: 32.41487761438586 BCD5 error: 31.2\n",
            "\tIntensity Truth: 80.0, AI forecast: 50.31708545982838, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 35.4, 69.5; AI forecast: 34.413030076026914, 70.93615206480027 ; AI error: 92.25825024523802 BCD5 error: 74.5\n",
            "\tIntensity Truth: 80.0, AI forecast: 50.67758582532406, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 40.7, 56.2; AI forecast: 36.59126467704773, 67.19343546330929 ; AI error: 570.9301363697457 BCD5 error: 577.1\n",
            "Found AL082017 at 2017-08-14 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 50.832132920622826, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 32.3, 72.1; AI forecast: 32.3295972943306, 71.38857873678208 ; AI error: 36.142325285264604 BCD5 error: 23.6\n",
            "\tIntensity Truth: 85.0, AI forecast: 52.721104100346565, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 36.8, 67.1; AI forecast: 35.385100388526915, 70.0850930660963 ; AI error: 167.89020968609688 BCD5 error: 208.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 53.39044101536274, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 42.2, 52.0; AI forecast: 38.03278615474701, 65.35992126315833 ; AI error: 661.5586798743052 BCD5 error: 783.0\n",
            "Found AL082017 at 2017-08-14 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 55.06752327084541, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 33.2, 71.8; AI forecast: 32.88916842937469, 70.93804621994495 ; AI error: 47.224791564528765 BCD5 error: 31.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 58.03870588541031, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 38.2, 64.1; AI forecast: 36.296129417419436, 69.0732021510601 ; AI error: 263.69404470322587 BCD5 error: 320.0\n",
            "Found AL082017 at 2017-08-15 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 62.25095495581627, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 34.2, 71.0; AI forecast: 33.81651108264923, 70.40518069714308 ; AI error: 37.50432406926411 BCD5 error: 37.4\n",
            "\tIntensity Truth: 90.0, AI forecast: 61.155643463134766, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 39.4, 60.4; AI forecast: 37.224081897735594, 67.88116470277309 ; AI error: 375.72856537300237 BCD5 error: 436.5\n",
            "Found AL082017 at 2017-08-15 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 65.23487359285355, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 35.4, 69.5; AI forecast: 34.26502385139465, 69.96489476263523 ; AI error: 71.8926875948494 BCD5 error: 72.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 62.776378989219666, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 40.7, 56.2; AI forecast: 37.44277398586273, 67.07038744837045 ; AI error: 542.6608778251846 BCD5 error: 559.8\n",
            "Found AL082017 at 2017-08-15 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 67.97315388917923, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 36.8, 67.1; AI forecast: 34.685471057891846, 69.40658702254295 ; AI error: 169.5540519898153 BCD5 error: 141.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 63.93671929836273, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 42.2, 52.0; AI forecast: 38.04095873832702, 66.2940119177103 ; AI error: 701.0898109741381 BCD5 error: 655.2\n",
            "Found AL082017 at 2017-08-15 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 68.94366800785065, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 38.2, 64.1; AI forecast: 35.83632583618164, 68.62385835647584 ; AI error: 259.124584639953 BCD5 error: 208.2\n",
            "Found AL082017 at 2017-08-16 00:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 68.68017107248306, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 39.4, 60.4; AI forecast: 37.44899842739105, 66.98778820931912 ; AI error: 331.1717363989807 BCD5 error: 221.8\n",
            "Found AL082017 at 2017-08-16 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 67.69309282302856, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 40.7, 56.2; AI forecast: 38.95739359855652, 64.77993841916323 ; AI error: 409.020322752721 BCD5 error: 196.0\n",
            "Found AL082017 at 2017-08-16 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 64.62718486785889, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 42.2, 52.0; AI forecast: 40.57780017852783, 61.119693875312805 ; AI error: 421.9533267100136 BCD5 error: 155.3\n",
            "Found AL082017 at 2017-08-16 18:00:00\n",
            "Found AL082017 at 2017-08-17 00:00:00\n",
            "Found AL082017 at 2017-08-17 06:00:00\n",
            "Found AL082017 at 2017-08-17 12:00:00\n",
            "Found AL092017 at 2017-08-17 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 41.863670237362385, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 13.2, 62.2; AI forecast: 15.03926956653595, 59.5840853843838 ; AI error: 188.12865667106257 BCD5 error: 66.7\n",
            "Found AL092017 at 2017-08-18 00:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 47.23512317985296, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 13.4, 64.0; AI forecast: 15.142641258239745, 61.044196446239944 ; AI error: 201.3083128956086 BCD5 error: 59.6\n",
            "Found AL092017 at 2017-08-18 06:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 50.263790264725685, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 13.5, 65.7; AI forecast: 14.895578694343566, 61.98216359019279 ; AI error: 232.04999581728782 BCD5 error: 34.3\n",
            "Found AL092017 at 2017-08-18 12:00:00\n",
            "Found AL092017 at 2017-08-18 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 45.037857168354094, BCD5 forecast: 52.0\n",
            "\tTrajectory Truth: 21.6, 92.4; AI forecast: 22.72967390716076, 87.25262492895126 ; AI error: 294.1195060383446 BCD5 error: 314.4\n",
            "Found AL092017 at 2017-08-19 00:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 44.878864982165396, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 22.0, 92.5; AI forecast: 22.962319543957708, 87.98760799765587 ; AI error: 256.90478971509526 BCD5 error: 282.0\n",
            "Found AL092017 at 2017-08-19 06:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 43.52192526683211, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 22.8, 92.6; AI forecast: 22.522319757938384, 89.40210269093514 ; AI error: 177.9595315289506 BCD5 error: 211.9\n",
            "Found AL092017 at 2017-08-23 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 41.9737995415926, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 24.4, 93.6; AI forecast: 23.281060630083083, 91.01079831123351 ; AI error: 157.25924208454975 BCD5 error: 87.0\n",
            "\tIntensity Truth: 105.0, AI forecast: 46.41252985224128, BCD5 forecast: -39.0\n",
            "\tTrajectory Truth: 27.1, 96.3; AI forecast: 25.66179047822952, 94.28846675753593 ; AI error: 138.42522490019888 BCD5 error: 221.0\n",
            "\tIntensity Truth: 50.0, AI forecast: 48.98413918912411, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 29.0, 97.5; AI forecast: 28.33006842136383, 93.0709065437317 ; AI error: 236.7587600430414 BCD5 error: 263.5\n",
            "\tIntensity Truth: 35.0, AI forecast: 51.21854655444622, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 29.0, 97.2; AI forecast: 31.57732682228088, 92.01580955982209 ; AI error: 310.07617818684935 BCD5 error: 231.1\n",
            "\tIntensity Truth: 40.0, AI forecast: 49.14681684225798, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 34.666719627380374, 90.66621808409691 ; AI error: 461.67065531120795 BCD5 error: 251.5\n",
            "Found AL092017 at 2017-08-24 00:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 47.00458461418748, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 25.0, 94.4; AI forecast: 23.50606690645218, 91.3654682636261 ; AI error: 188.77448569077086 BCD5 error: 89.3\n",
            "\tIntensity Truth: 115.0, AI forecast: 50.92915512621403, BCD5 forecast: -47.0\n",
            "\tTrajectory Truth: 27.8, 96.8; AI forecast: 25.642356371879576, 94.57969183921814 ; AI error: 175.94579782696698 BCD5 error: 225.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 54.43717986345291, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 29.2, 97.4; AI forecast: 28.49104473590851, 93.20464034080506 ; AI error: 224.6921913277772 BCD5 error: 243.4\n",
            "\tIntensity Truth: 35.0, AI forecast: 52.5886494666338, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 28.8, 96.8; AI forecast: 31.49010878801346, 91.49540696740151 ; AI error: 319.21756368081776 BCD5 error: 272.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 48.88977527618408, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 35.08499038219452, 90.39684446454048 ; AI error: 485.9032134685557 BCD5 error: 347.6\n",
            "Found AL092017 at 2017-08-24 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 55.38352057337761, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 25.6, 95.1; AI forecast: 23.94771386384964, 90.93669733405113 ; AI error: 247.67586729078207 BCD5 error: 92.7\n",
            "\tIntensity Truth: 105.0, AI forecast: 60.02442702651024, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 28.2, 97.1; AI forecast: 26.33353749513626, 93.91154853701592 ; AI error: 203.7349331661724 BCD5 error: 190.4\n",
            "\tIntensity Truth: 40.0, AI forecast: 57.11534947156906, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 29.3, 97.6; AI forecast: 28.894883918762204, 91.82393859624862 ; AI error: 303.9726796063913 BCD5 error: 223.3\n",
            "\tIntensity Truth: 40.0, AI forecast: 52.17672049999237, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 28.6, 96.5; AI forecast: 32.498718404769896, 90.28737688064575 ; AI error: 397.33656338838136 BCD5 error: 300.0\n",
            "\tIntensity Truth: 40.0, AI forecast: 47.84357100725174, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 36.584346294403076, 88.60975232720375 ; AI error: 603.4181727968434 BCD5 error: 399.4\n",
            "Found AL092017 at 2017-08-24 12:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 66.78755015134811, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 26.3, 95.8; AI forecast: 25.317425954341886, 89.97865841984749 ; AI error: 320.1037612207348 BCD5 error: 65.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 65.62922716140747, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 28.7, 97.3; AI forecast: 27.671318531036377, 91.84285301566123 ; AI error: 295.2971405036366 BCD5 error: 100.0\n",
            "\tIntensity Truth: 35.0, AI forecast: 58.455211371183395, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 29.1, 97.5; AI forecast: 30.94074399471283, 89.67788964509964 ; AI error: 421.28054291000063 BCD5 error: 129.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 51.71348862349987, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.5, 96.2; AI forecast: 35.1445867061615, 87.35394611954689 ; AI error: 601.8400601329888 BCD5 error: 246.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 44.67397461179644, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 39.19843149185181, 84.72184373140335 ; AI error: 823.1640333757546 BCD5 error: 354.9\n",
            "Found AL092017 at 2017-08-24 18:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 72.67301738262177, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 27.1, 96.3; AI forecast: 26.200380408763884, 88.71242333650589 ; AI error: 410.66396258071853 BCD5 error: 40.1\n",
            "\tIntensity Truth: 50.0, AI forecast: 69.03944373130798, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 29.0, 97.5; AI forecast: 29.350312066078185, 90.71843368411064 ; AI error: 356.08374734584663 BCD5 error: 33.9\n",
            "\tIntensity Truth: 35.0, AI forecast: 59.95209872722626, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 29.0, 97.2; AI forecast: 33.28005254268646, 87.8995577633381 ; AI error: 542.3162628474232 BCD5 error: 88.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 50.42524725198746, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 37.47707300186157, 84.78491892814637 ; AI error: 780.3394051526558 BCD5 error: 239.6\n",
            "\tIntensity Truth: 45.0, AI forecast: 41.844438426196575, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 41.414328169822696, 80.36667402386665 ; AI error: 1029.0716929407263 BCD5 error: 342.7\n",
            "Found AL092017 at 2017-08-25 00:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 78.48545849323273, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 27.8, 96.8; AI forecast: 28.45764994621277, 88.4179661512375 ; AI error: 445.4817583941239 BCD5 error: 42.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 72.06738770008087, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.2, 97.4; AI forecast: 32.811966729164126, 89.1765066087246 ; AI error: 475.30765061125993 BCD5 error: 79.4\n",
            "\tIntensity Truth: 35.0, AI forecast: 58.77151012420654, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 28.8, 96.8; AI forecast: 37.01556761264801, 84.96719193458557 ; AI error: 772.86272538756 BCD5 error: 188.1\n",
            "\tIntensity Truth: 40.0, AI forecast: 46.666738409549, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 41.15066659450531, 79.72177270054817 ; AI error: 1093.8202686352859 BCD5 error: 337.7\n",
            "\tIntensity Truth: 45.0, AI forecast: 36.85804180800915, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 45.16882147789001, 74.41253213882446 ; AI error: 1341.1775606728932 BCD5 error: 409.9\n",
            "Found AL092017 at 2017-08-25 06:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 85.41849434375763, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 28.2, 97.1; AI forecast: 30.824655306339263, 88.12273570299149 ; AI error: 494.6405646390594 BCD5 error: 32.0\n",
            "\tIntensity Truth: 40.0, AI forecast: 74.55563515424728, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 29.3, 97.6; AI forecast: 35.66293969154358, 88.24898784160614 ; AI error: 607.9488147146064 BCD5 error: 90.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 57.7480074763298, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.6, 96.5; AI forecast: 39.820282340049744, 82.62047615349293 ; AI error: 961.4533364854501 BCD5 error: 263.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 43.75127511098981, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 44.03337159156799, 77.19363687634468 ; AI error: 1283.4977710345502 BCD5 error: 415.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 32.96540901064873, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 48.08827681541443, 71.36171123981475 ; AI error: 1519.7800634944201 BCD5 error: 450.7\n",
            "Found AL092017 at 2017-08-25 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 89.70996856689453, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 28.7, 97.3; AI forecast: 31.773544681072234, 89.43211411237716 ; AI error: 447.7406220350477 BCD5 error: 28.9\n",
            "\tIntensity Truth: 35.0, AI forecast: 76.54524594545364, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 29.1, 97.5; AI forecast: 36.23139462471008, 89.72402766942977 ; AI error: 580.76580124647 BCD5 error: 139.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 58.033722043037415, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.5, 96.2; AI forecast: 40.25905139446259, 85.09633024334907 ; AI error: 893.4943720833113 BCD5 error: 329.1\n",
            "\tIntensity Truth: 40.0, AI forecast: 42.869984079152346, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 44.50813546180725, 80.07658509612084 ; AI error: 1200.9185488844735 BCD5 error: 473.7\n",
            "\tIntensity Truth: 40.0, AI forecast: 31.629112362861633, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 49.033950328826904, 74.3420855075121 ; AI error: 1430.2874757782101 BCD5 error: 456.3\n",
            "Found AL092017 at 2017-08-25 18:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 93.46284985542297, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 29.0, 97.5; AI forecast: 31.804499411582945, 90.33543449640274 ; AI error: 407.3110831796418 BCD5 error: 49.5\n",
            "\tIntensity Truth: 35.0, AI forecast: 78.27170610427856, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.0, 97.2; AI forecast: 35.88657948970795, 92.08568472266197 ; AI error: 487.7725237409039 BCD5 error: 202.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 58.11464220285416, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 39.74445950984955, 88.13565055131912 ; AI error: 782.1852591603684 BCD5 error: 396.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 42.33339633792639, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 44.408074259757996, 83.27675427794456 ; AI error: 1088.8085580142256 BCD5 error: 526.4\n",
            "\tIntensity Truth: 35.0, AI forecast: 30.971696823835373, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 48.951800918579096, 77.15020866394043 ; AI error: 1319.0794585874287 BCD5 error: 485.9\n",
            "Found AL092017 at 2017-08-26 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 95.80646991729736, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 29.2, 97.4; AI forecast: 32.24888232946396, 91.36195805668831 ; AI error: 361.3292272024477 BCD5 error: 71.1\n",
            "\tIntensity Truth: 35.0, AI forecast: 78.30284416675568, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 28.8, 96.8; AI forecast: 36.36179096698761, 93.30699594020844 ; AI error: 487.08709907815523 BCD5 error: 236.8\n",
            "\tIntensity Truth: 40.0, AI forecast: 57.00174227356911, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 40.69032528400421, 88.98926844000816 ; AI error: 813.7217421109979 BCD5 error: 462.1\n",
            "\tIntensity Truth: 45.0, AI forecast: 41.14257588982582, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 45.43650250434875, 83.02027176618576 ; AI error: 1116.4324612036248 BCD5 error: 587.5\n",
            "Found AL092017 at 2017-08-26 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 92.23166346549988, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 29.3, 97.6; AI forecast: 33.581933450698855, 91.10503125786781 ; AI error: 420.29360633703516 BCD5 error: 55.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 76.93238765001297, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.6, 96.5; AI forecast: 38.819071006774905, 90.43952315449715 ; AI error: 683.7262994876494 BCD5 error: 243.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 56.819249987602234, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 43.391467046737674, 83.28081730008125 ; AI error: 1078.7451768232804 BCD5 error: 452.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 41.593266278505325, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 48.26911005973815, 74.24687750339508 ; AI error: 1441.42687649406 BCD5 error: 539.4\n",
            "Found AL092017 at 2017-08-26 12:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 84.62723791599274, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 29.1, 97.5; AI forecast: 32.69754455089569, 91.13232947587967 ; AI error: 392.65074671159283 BCD5 error: 82.2\n",
            "\tIntensity Truth: 40.0, AI forecast: 77.217276096344, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.5, 96.2; AI forecast: 37.6185033082962, 89.67860864400863 ; AI error: 637.8636903995815 BCD5 error: 245.0\n",
            "\tIntensity Truth: 40.0, AI forecast: 62.11304500699043, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 42.338642716407776, 80.57818688452244 ; AI error: 1089.1529782049704 BCD5 error: 389.4\n",
            "\tIntensity Truth: 40.0, AI forecast: 48.883364237844944, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 48.05551438331604, 68.84914072453975 ; AI error: 1559.3110624547612 BCD5 error: 381.2\n",
            "Found AL092017 at 2017-08-26 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 72.16876268386841, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 29.0, 97.2; AI forecast: 31.149917674064636, 91.38954794406891 ; AI error: 328.27091485037056 BCD5 error: 80.7\n",
            "\tIntensity Truth: 40.0, AI forecast: 68.79059374332428, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 35.43634839057923, 88.67540506720543 ; AI error: 560.015662556464 BCD5 error: 219.0\n",
            "\tIntensity Truth: 45.0, AI forecast: 55.91535955667496, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 40.28756456375122, 77.41349792480469 ; AI error: 1088.614072030822 BCD5 error: 335.8\n",
            "\tIntensity Truth: 35.0, AI forecast: 38.119676783680916, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 44.433667469024655, 65.92613290399314 ; AI error: 1526.4748771739494 BCD5 error: 291.8\n",
            "Found AL092017 at 2017-08-27 00:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 53.24173100292683, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 28.8, 96.8; AI forecast: 29.663076734542848, 90.3121077299118 ; AI error: 343.8080836440034 BCD5 error: 91.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 45.424377978779376, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 33.51062123775482, 85.94209679961205 ; AI error: 582.0773721430131 BCD5 error: 216.6\n",
            "\tIntensity Truth: 45.0, AI forecast: 29.73341017961502, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 36.27938170433045, 76.60043734014035 ; AI error: 974.3418346577483 BCD5 error: 288.9\n",
            "Found AL092017 at 2017-08-27 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 28.816986083984375, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.6, 96.5; AI forecast: 28.68617511987686, 89.44800670146942 ; AI error: 371.571909349199 BCD5 error: 86.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 16.21151566505432, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 30.46085442304611, 87.8506183564663 ; AI error: 400.24056795311833 BCD5 error: 182.2\n",
            "\tIntensity Truth: 40.0, AI forecast: 17.59832352399826, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 31.76555459499359, 82.4193311214447 ; AI error: 594.7965215911713 BCD5 error: 202.7\n",
            "Found AL092017 at 2017-08-27 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 14.089986383914948, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.5, 96.2; AI forecast: 27.53845509290695, 91.41043621897697 ; AI error: 260.32233088999317 BCD5 error: 49.5\n",
            "\tIntensity Truth: 40.0, AI forecast: 13.20126861333847, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 28.89165904521942, 92.36281132102013 ; AI error: 125.0846764415766 BCD5 error: 134.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 20.16552597284317, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 30.040473306179045, 87.54456900954247 ; AI error: 304.23437478160633 BCD5 error: 109.9\n",
            "Found AL092017 at 2017-08-27 18:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 28.19134384393692, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 28.4, 95.9; AI forecast: 28.845460891723633, 92.258230894804 ; AI error: 193.7788512294668 BCD5 error: 48.0\n",
            "\tIntensity Truth: 45.0, AI forecast: 31.60689577460289, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 30.690827560424804, 92.02038809061051 ; AI error: 173.91771462061803 BCD5 error: 120.5\n",
            "\tIntensity Truth: 35.0, AI forecast: 33.360616862773895, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 31.993846201896666, 87.62876988649369 ; AI error: 292.86308493095174 BCD5 error: 99.4\n",
            "Found AL092017 at 2017-08-28 00:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 37.31291078031063, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 28.2, 95.4; AI forecast: 29.28263703584671, 91.73325994610786 ; AI error: 203.66827170411096 BCD5 error: 42.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 37.98987314105034, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 30.72431848049164, 92.15966021418572 ; AI error: 138.918475270255 BCD5 error: 72.8\n",
            "Found AL092017 at 2017-08-28 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 39.89716447889805, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 28.1, 95.0; AI forecast: 28.645119190216064, 92.18741289377212 ; AI error: 152.14158236710475 BCD5 error: 47.0\n",
            "\tIntensity Truth: 40.0, AI forecast: 40.6830558180809, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 29.811184346675873, 92.60823074579238 ; AI error: 57.35694003393045 BCD5 error: 44.8\n",
            "Found AL092017 at 2017-08-28 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 42.02590771019459, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 28.2, 94.6; AI forecast: 28.18597275018692, 91.26237331628799 ; AI error: 176.61480500364488 BCD5 error: 52.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 46.48112470284104, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 29.76772665977478, 91.54271166920662 ; AI error: 98.67339163557581 BCD5 error: 12.0\n",
            "Found AL092017 at 2017-08-28 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 47.879283502697945, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 28.5, 94.2; AI forecast: 28.715530931949615, 90.27133506536484 ; AI error: 207.47679295931195 BCD5 error: 19.8\n",
            "\tIntensity Truth: 35.0, AI forecast: 50.65642900764942, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 30.421561884880063, 90.17977123260498 ; AI error: 151.42948189956192 BCD5 error: 39.2\n",
            "Found AL092017 at 2017-08-29 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 48.99671796709299, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 28.9, 93.8; AI forecast: 28.611188173294067, 89.8243822813034 ; AI error: 209.96882367130848 BCD5 error: 24.6\n",
            "Found AL092017 at 2017-08-29 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 52.45646446943283, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 29.4, 93.6; AI forecast: 28.64342789649963, 89.59382781982421 ; AI error: 215.16682265676297 BCD5 error: 30.5\n",
            "Found AL092017 at 2017-08-29 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 48.05225010961294, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 30.1, 93.4; AI forecast: 28.502818453311917, 89.60227944850922 ; AI error: 220.74052676466252 BCD5 error: 31.8\n",
            "Found AL092017 at 2017-08-29 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 50.9385522454977, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 30.6, 93.1; AI forecast: 28.582209372520445, 89.14296123981475 ; AI error: 239.46979585686907 BCD5 error: 19.6\n",
            "Found AL092017 at 2017-08-30 00:00:00\n",
            "Found AL092017 at 2017-08-30 06:00:00\n",
            "Found AL092017 at 2017-08-30 12:00:00\n",
            "Found AL092017 at 2017-08-30 18:00:00\n",
            "Found AL112017 at 2017-08-31 06:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 67.06126302480698, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 17.9, 36.1; AI forecast: 18.628102833032607, 38.288444814085956 ; AI error: 132.21049282753083 BCD5 error: 12.0\n",
            "\tIntensity Truth: 100.0, AI forecast: 75.18582105636597, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 19.1, 41.1; AI forecast: 20.640272499620913, 42.59197547733783 ; AI error: 125.09610870532093 BCD5 error: 84.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 74.87542510032654, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 18.2, 46.7; AI forecast: 23.055912974476815, 48.61097649037838 ; AI error: 310.6809776871106 BCD5 error: 310.6\n",
            "\tIntensity Truth: 105.0, AI forecast: 74.67111259698868, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 17.0, 51.5; AI forecast: 25.274093687534332, 53.18286501765251 ; AI error: 505.6184638757423 BCD5 error: 581.4\n",
            "\tIntensity Truth: 135.0, AI forecast: 75.52054584026337, BCD5 forecast: -59.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 27.06314297914505, 56.13942789144348 ; AI error: 628.3792696653788 BCD5 error: 848.5\n",
            "Found AL112017 at 2017-08-31 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 78.45114946365356, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 18.4, 37.3; AI forecast: 18.889508724212646, 38.51538054049014 ; AI error: 75.12924728247314 BCD5 error: 18.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 77.64017045497894, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 18.9, 42.6; AI forecast: 20.76285806149244, 42.302253679931155 ; AI error: 113.10389758075381 BCD5 error: 168.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 72.58406549692154, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 17.9, 47.9; AI forecast: 23.245776090025903, 48.248154209554194 ; AI error: 321.5584225951566 BCD5 error: 423.4\n",
            "\tIntensity Truth: 110.0, AI forecast: 70.15891194343567, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 16.8, 52.6; AI forecast: 25.398198747634886, 52.731997589766976 ; AI error: 516.2927745798132 BCD5 error: 712.6\n",
            "\tIntensity Truth: 150.0, AI forecast: 69.60563868284225, BCD5 forecast: -68.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 27.1480531334877, 56.084621606091964 ; AI error: 634.5107027111743 BCD5 error: 987.4\n",
            "Found AL112017 at 2017-08-31 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 79.8870238661766, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 18.8, 38.5; AI forecast: 19.019644650816918, 37.9019133746624 ; AI error: 36.44122637296916 BCD5 error: 48.8\n",
            "\tIntensity Truth: 95.0, AI forecast: 75.25815039873123, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 18.7, 44.1; AI forecast: 20.829904958605766, 41.79990660846233 ; AI error: 182.3207383233366 BCD5 error: 242.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 67.59781986474991, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 17.6, 49.2; AI forecast: 23.098075905442236, 47.75420111119747 ; AI error: 339.9818378972297 BCD5 error: 515.9\n",
            "\tIntensity Truth: 115.0, AI forecast: 63.78370553255081, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 16.7, 53.9; AI forecast: 25.16457415819168, 52.627167887985706 ; AI error: 513.1920096104006 BCD5 error: 810.4\n",
            "\tIntensity Truth: 155.0, AI forecast: 61.57721519470215, BCD5 forecast: -66.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 27.009430134296416, 55.59357779575512 ; AI error: 639.2128131256701 BCD5 error: 1087.7\n",
            "Found AL112017 at 2017-09-01 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 84.21219170093536, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 19.1, 39.7; AI forecast: 19.747523449361324, 38.01305975317955 ; AI error: 103.12781165874422 BCD5 error: 51.3\n",
            "\tIntensity Truth: 95.0, AI forecast: 79.4709637761116, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 18.5, 45.5; AI forecast: 21.783914214372633, 41.72012486159801 ; AI error: 290.2634658405082 BCD5 error: 277.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 71.90845549106598, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 17.3, 50.4; AI forecast: 24.187393915653228, 47.88981041014194 ; AI error: 436.84215269123337 BCD5 error: 555.4\n",
            "\tIntensity Truth: 125.0, AI forecast: 67.49263346195221, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 16.6, 55.1; AI forecast: 26.345574820041655, 52.25869705677032 ; AI error: 606.2083328936485 BCD5 error: 848.4\n",
            "\tIntensity Truth: 155.0, AI forecast: 64.42349404096603, BCD5 forecast: -65.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 28.194946670532225, 54.66330380234867 ; AI error: 731.7368094425301 BCD5 error: 1113.5\n",
            "Found AL112017 at 2017-09-01 06:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 91.23945534229279, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 19.1, 41.1; AI forecast: 20.50491521060467, 38.58808082640171 ; AI error: 165.07157744730767 BCD5 error: 75.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 89.2136162519455, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 18.2, 46.7; AI forecast: 22.926580074429513, 42.67076690495014 ; AI error: 363.0348505934313 BCD5 error: 309.7\n",
            "\tIntensity Truth: 105.0, AI forecast: 82.17345178127289, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 17.0, 51.5; AI forecast: 25.481451404094695, 48.33101629018783 ; AI error: 539.1485199194778 BCD5 error: 600.4\n",
            "\tIntensity Truth: 135.0, AI forecast: 76.61612540483475, BCD5 forecast: -46.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 27.487484967708586, 52.07254628911614 ; AI error: 696.4511582843572 BCD5 error: 891.6\n",
            "\tIntensity Truth: 155.0, AI forecast: 71.54155999422073, BCD5 forecast: -64.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 29.909951543807985, 53.00326854400336 ; AI error: 880.202573858699 BCD5 error: 1148.3\n",
            "Found AL112017 at 2017-09-01 12:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 97.57132589817047, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 18.9, 42.6; AI forecast: 20.944991694390772, 40.06840631365776 ; AI error: 188.39578284647268 BCD5 error: 106.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 96.28566145896912, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 17.9, 47.9; AI forecast: 23.42332344055176, 44.038348615169525 ; AI error: 396.21159910559174 BCD5 error: 345.4\n",
            "\tIntensity Truth: 110.0, AI forecast: 88.0007255077362, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 16.8, 52.6; AI forecast: 25.627012312412262, 49.165645994246006 ; AI error: 563.6670985618707 BCD5 error: 629.5\n",
            "\tIntensity Truth: 150.0, AI forecast: 80.97439646720886, BCD5 forecast: -59.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 28.09436501264572, 51.30771416723728 ; AI error: 772.8141339446894 BCD5 error: 914.6\n",
            "\tIntensity Truth: 155.0, AI forecast: 75.2407518029213, BCD5 forecast: -62.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 30.764826917648314, 51.27852984890342 ; AI error: 1003.4524904194702 BCD5 error: 1164.7\n",
            "Found AL112017 at 2017-09-01 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 99.70301270484924, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 18.7, 44.1; AI forecast: 20.92336968332529, 41.4062275737524 ; AI error: 202.408185370939 BCD5 error: 135.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 97.80876457691193, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 17.6, 49.2; AI forecast: 22.941293278336524, 45.42741465717554 ; AI error: 384.6348504719055 BCD5 error: 356.6\n",
            "\tIntensity Truth: 115.0, AI forecast: 89.19627606868744, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 16.7, 53.9; AI forecast: 25.50952297449112, 49.12843695282936 ; AI error: 592.4433311562382 BCD5 error: 627.4\n",
            "\tIntensity Truth: 155.0, AI forecast: 82.481849193573, BCD5 forecast: -63.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 28.183475613594055, 50.478956852853294 ; AI error: 831.6393887009847 BCD5 error: 893.9\n",
            "\tIntensity Truth: 150.0, AI forecast: 76.5063202381134, BCD5 forecast: -56.0\n",
            "\tTrajectory Truth: 18.6, 64.7; AI forecast: 30.811119699478148, 49.46618033125996 ; AI error: 1105.8762108252283 BCD5 error: 1131.6\n",
            "Found AL112017 at 2017-09-02 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 96.46014392375946, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 18.5, 45.5; AI forecast: 20.33404718413949, 43.23241442888975 ; AI error: 169.147974955419 BCD5 error: 119.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 94.54413294792175, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 17.3, 50.4; AI forecast: 22.44435727596283, 46.32049354612827 ; AI error: 385.2396182885428 BCD5 error: 313.3\n",
            "\tIntensity Truth: 125.0, AI forecast: 87.04673171043396, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 16.6, 55.1; AI forecast: 25.01523036956787, 49.47928001657128 ; AI error: 595.4247049155509 BCD5 error: 558.1\n",
            "\tIntensity Truth: 155.0, AI forecast: 80.57400345802307, BCD5 forecast: -64.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 27.46536897420883, 50.11770229488611 ; AI error: 842.4303362848025 BCD5 error: 780.3\n",
            "\tIntensity Truth: 150.0, AI forecast: 74.52092349529266, BCD5 forecast: -56.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 30.063911843299863, 48.76132187545299 ; AI error: 1151.3592158222077 BCD5 error: 1005.7\n",
            "Found AL112017 at 2017-09-02 06:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 95.11332988739014, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 18.2, 46.7; AI forecast: 20.631886016577482, 44.46874153167009 ; AI error: 193.0789834044137 BCD5 error: 78.0\n",
            "\tIntensity Truth: 105.0, AI forecast: 95.71862161159515, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 17.0, 51.5; AI forecast: 23.215252444148064, 47.59167005121708 ; AI error: 433.2906935385822 BCD5 error: 240.0\n",
            "\tIntensity Truth: 135.0, AI forecast: 89.22642111778259, BCD5 forecast: -40.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 26.029359221458435, 50.39498575031757 ; AI error: 657.9892684745588 BCD5 error: 439.3\n",
            "\tIntensity Truth: 155.0, AI forecast: 83.02611231803894, BCD5 forecast: -62.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 28.810295200347902, 51.05208292305469 ; AI error: 895.0670310845013 BCD5 error: 597.1\n",
            "\tIntensity Truth: 145.0, AI forecast: 78.29122006893158, BCD5 forecast: -48.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 31.37947174310684, 49.758938796818256 ; AI error: 1191.2216405867293 BCD5 error: 793.3\n",
            "Found AL112017 at 2017-09-02 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 96.65726959705353, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 17.9, 47.9; AI forecast: 21.042500184476374, 46.21547985523939 ; AI error: 211.39650802535516 BCD5 error: 71.9\n",
            "\tIntensity Truth: 110.0, AI forecast: 100.54794609546661, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 16.8, 52.6; AI forecast: 24.03666750192642, 49.126735265552995 ; AI error: 476.3458087436532 BCD5 error: 210.3\n",
            "\tIntensity Truth: 150.0, AI forecast: 95.81236124038696, BCD5 forecast: -56.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 27.424742627143857, 51.90760683715343 ; AI error: 722.2545608366743 BCD5 error: 372.2\n",
            "\tIntensity Truth: 155.0, AI forecast: 91.66739404201508, BCD5 forecast: -61.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 30.34777431488037, 52.677766588330265 ; AI error: 936.3687932393992 BCD5 error: 491.2\n",
            "\tIntensity Truth: 145.0, AI forecast: 84.55571174621582, BCD5 forecast: -44.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 33.407548451423644, 51.679182240366934 ; AI error: 1217.4983558606189 BCD5 error: 673.6\n",
            "Found AL112017 at 2017-09-02 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 98.4700071811676, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 17.6, 49.2; AI forecast: 20.81295306235552, 47.76644105017185 ; AI error: 209.32597847000196 BCD5 error: 77.4\n",
            "\tIntensity Truth: 115.0, AI forecast: 103.56829226016998, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 16.7, 53.9; AI forecast: 23.942385387420654, 51.147498656809326 ; AI error: 461.57810519479 BCD5 error: 203.2\n",
            "\tIntensity Truth: 155.0, AI forecast: 100.71400880813599, BCD5 forecast: -60.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 27.138660144805907, 54.21428018696606 ; AI error: 674.2381010469992 BCD5 error: 348.0\n",
            "\tIntensity Truth: 150.0, AI forecast: 94.23781335353851, BCD5 forecast: -55.0\n",
            "\tTrajectory Truth: 18.6, 64.7; AI forecast: 30.297173690795898, 55.399165694601834 ; AI error: 866.0830141860152 BCD5 error: 450.6\n",
            "\tIntensity Truth: 145.0, AI forecast: 90.90564429759979, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 33.42131011486053, 55.73559507708996 ; AI error: 1092.2133651353472 BCD5 error: 640.5\n",
            "Found AL112017 at 2017-09-03 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 99.0279471874237, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 17.3, 50.4; AI forecast: 20.490564091503618, 49.60346715301275 ; AI error: 196.83231399363805 BCD5 error: 74.0\n",
            "\tIntensity Truth: 125.0, AI forecast: 105.6655865907669, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 16.6, 55.1; AI forecast: 23.089029136300088, 53.23509439565241 ; AI error: 403.5684002502481 BCD5 error: 180.8\n",
            "\tIntensity Truth: 155.0, AI forecast: 100.11136651039124, BCD5 forecast: -59.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 26.26865550279617, 56.453079189243724 ; AI error: 585.8780272935563 BCD5 error: 298.4\n",
            "\tIntensity Truth: 150.0, AI forecast: 97.62789845466614, BCD5 forecast: -55.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 29.3846351146698, 58.61291583739221 ; AI error: 738.6034937185117 BCD5 error: 413.0\n",
            "\tIntensity Truth: 140.0, AI forecast: 92.70946979522705, BCD5 forecast: -39.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 32.40908359289169, 60.11537110209465 ; AI error: 922.4696906922966 BCD5 error: 625.9\n",
            "Found AL112017 at 2017-09-03 06:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 100.18576323986053, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 17.0, 51.5; AI forecast: 19.628555637598037, 51.323569025844336 ; AI error: 158.13974361253693 BCD5 error: 60.3\n",
            "\tIntensity Truth: 135.0, AI forecast: 104.15231347084045, BCD5 forecast: -40.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 22.034031170606614, 55.218077594973145 ; AI error: 333.05690015580257 BCD5 error: 144.5\n",
            "\tIntensity Truth: 155.0, AI forecast: 103.54385077953339, BCD5 forecast: -61.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 25.187309741973877, 59.1748047940433 ; AI error: 474.5869663331889 BCD5 error: 247.0\n",
            "\tIntensity Truth: 145.0, AI forecast: 99.50040876865387, BCD5 forecast: -53.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 28.270370709896085, 62.21581703871488 ; AI error: 593.0815781252417 BCD5 error: 377.0\n",
            "\tIntensity Truth: 135.0, AI forecast: 94.5152896642685, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 31.107419788837433, 63.632106685638426 ; AI error: 772.4906235548751 BCD5 error: 598.0\n",
            "Found AL112017 at 2017-09-03 12:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 97.97753095626831, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 16.8, 52.6; AI forecast: 19.11604050695896, 52.093796183168884 ; AI error: 142.02950056467955 BCD5 error: 58.7\n",
            "\tIntensity Truth: 150.0, AI forecast: 106.82789206504822, BCD5 forecast: -50.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 21.425673027336597, 56.44056888378691 ; AI error: 294.02445749519734 BCD5 error: 114.1\n",
            "\tIntensity Truth: 155.0, AI forecast: 103.7207156419754, BCD5 forecast: -58.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 24.542994427680966, 60.90473863780498 ; AI error: 409.3475595599096 BCD5 error: 201.8\n",
            "\tIntensity Truth: 145.0, AI forecast: 99.43944692611694, BCD5 forecast: -51.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 27.45679084062576, 63.47354114204645 ; AI error: 530.8082724807183 BCD5 error: 322.4\n",
            "\tIntensity Truth: 135.0, AI forecast: 97.3866754770279, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 30.196210515499114, 65.11244997829199 ; AI error: 721.7206023817316 BCD5 error: 559.5\n",
            "Found AL112017 at 2017-09-03 18:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 105.1340115070343, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 16.7, 53.9; AI forecast: 19.10697571337223, 52.949044537544246 ; AI error: 154.38959676778856 BCD5 error: 47.9\n",
            "\tIntensity Truth: 155.0, AI forecast: 111.6044533252716, BCD5 forecast: -54.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 21.63753056526184, 57.40340779665857 ; AI error: 302.1066796305011 BCD5 error: 84.2\n",
            "\tIntensity Truth: 150.0, AI forecast: 108.1061589717865, BCD5 forecast: -52.0\n",
            "\tTrajectory Truth: 18.6, 64.7; AI forecast: 24.79344333410263, 61.04374198466539 ; AI error: 424.05227938023256 BCD5 error: 153.7\n",
            "\tIntensity Truth: 145.0, AI forecast: 106.12769663333893, BCD5 forecast: -50.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 27.76180725097656, 63.672399452328676 ; AI error: 561.3770469888069 BCD5 error: 282.3\n",
            "\tIntensity Truth: 140.0, AI forecast: 100.69285929203033, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 30.49258420467377, 65.47411066889762 ; AI error: 761.6359227610546 BCD5 error: 531.2\n",
            "Found AL112017 at 2017-09-04 00:00:00\n",
            "\tIntensity Truth: 125.0, AI forecast: 106.12761735916138, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 16.6, 55.1; AI forecast: 19.090851533412934, 53.94489384926855 ; AI error: 163.47176335120307 BCD5 error: 30.5\n",
            "\tIntensity Truth: 155.0, AI forecast: 112.33430564403534, BCD5 forecast: -55.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 21.430665892362594, 57.74372999016195 ; AI error: 296.0865139684621 BCD5 error: 66.4\n",
            "\tIntensity Truth: 150.0, AI forecast: 111.13228797912598, BCD5 forecast: -51.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 24.482084572315216, 61.40965986102819 ; AI error: 414.4503069475728 BCD5 error: 145.2\n",
            "\tIntensity Truth: 140.0, AI forecast: 105.77652871608734, BCD5 forecast: -44.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 27.31838413476944, 64.07499983906746 ; AI error: 563.9269930491107 BCD5 error: 302.5\n",
            "\tIntensity Truth: 145.0, AI forecast: 100.68501114845276, BCD5 forecast: -47.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 30.26972471475601, 65.69371396303177 ; AI error: 789.657174661338 BCD5 error: 541.0\n",
            "Found AL112017 at 2017-09-04 06:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 109.76006865501404, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 16.6, 56.4; AI forecast: 18.79693085849285, 54.23055516816675 ; AI error: 181.09236625477234 BCD5 error: 29.5\n",
            "\tIntensity Truth: 155.0, AI forecast: 117.35987544059753, BCD5 forecast: -50.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 20.98861779719591, 58.28061836697161 ; AI error: 284.6245442997881 BCD5 error: 82.0\n",
            "\tIntensity Truth: 145.0, AI forecast: 111.38006567955017, BCD5 forecast: -44.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 23.882482647895813, 61.946751198172564 ; AI error: 402.8842816504902 BCD5 error: 172.0\n",
            "\tIntensity Truth: 135.0, AI forecast: 105.7865422964096, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 26.953796184062956, 64.29593769013881 ; AI error: 586.9660540846202 BCD5 error: 334.1\n",
            "\tIntensity Truth: 130.0, AI forecast: 101.72757089138031, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 29.87729275226593, 65.56592547297478 ; AI error: 819.2180174362832 BCD5 error: 559.6\n",
            "Found AL112017 at 2017-09-04 12:00:00\n",
            "\tIntensity Truth: 150.0, AI forecast: 116.65554523468018, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 16.7, 57.8; AI forecast: 18.474578312039373, 55.208554674871266 ; AI error: 182.61443519997587 BCD5 error: 29.5\n",
            "\tIntensity Truth: 155.0, AI forecast: 119.0883195400238, BCD5 forecast: -46.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 20.44320143684745, 59.365924327448006 ; AI error: 263.6208552755942 BCD5 error: 75.1\n",
            "\tIntensity Truth: 145.0, AI forecast: 112.45720148086548, BCD5 forecast: -41.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 23.525177323818205, 62.66750266700983 ; AI error: 405.3394699243547 BCD5 error: 157.4\n",
            "\tIntensity Truth: 135.0, AI forecast: 107.76560962200165, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 26.53968607187271, 64.59292748123407 ; AI error: 622.1577348723108 BCD5 error: 337.3\n",
            "\tIntensity Truth: 110.0, AI forecast: 101.8854683637619, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 29.39267026185989, 66.19111113399268 ; AI error: 812.4654601689097 BCD5 error: 543.8\n",
            "Found AL112017 at 2017-09-04 18:00:00\n",
            "\tIntensity Truth: 155.0, AI forecast: 116.43049836158752, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 16.9, 59.2; AI forecast: 17.968510487675665, 56.38640687826555 ; AI error: 173.4639785424733 BCD5 error: 16.6\n",
            "\tIntensity Truth: 150.0, AI forecast: 119.50360774993896, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 18.6, 64.7; AI forecast: 19.924801184237, 60.47361781969666 ; AI error: 252.39636530362426 BCD5 error: 63.7\n",
            "\tIntensity Truth: 145.0, AI forecast: 114.74225878715515, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 22.818882516026495, 63.47772032320499 ; AI error: 406.36605837964294 BCD5 error: 147.7\n",
            "\tIntensity Truth: 140.0, AI forecast: 108.88056337833405, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 25.712778627872467, 65.81697394698858 ; AI error: 601.6979753968544 BCD5 error: 326.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 102.47247695922852, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 28.67416783571243, 67.69885608553886 ; AI error: 753.0087336606771 BCD5 error: 510.9\n",
            "Found AL112017 at 2017-09-05 00:00:00\n",
            "\tIntensity Truth: 155.0, AI forecast: 119.81480479240417, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 17.3, 60.6; AI forecast: 17.966383790969847, 57.07081277472898 ; AI error: 205.85942502228747 BCD5 error: 23.7\n",
            "\tIntensity Truth: 150.0, AI forecast: 122.74067282676697, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 19.716869032382963, 60.87553736194968 ; AI error: 303.00397779581795 BCD5 error: 92.4\n",
            "\tIntensity Truth: 140.0, AI forecast: 115.03188490867615, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 22.468506464362143, 64.16463221013545 ; AI error: 433.49338009161005 BCD5 error: 163.2\n",
            "\tIntensity Truth: 145.0, AI forecast: 107.75174081325531, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 25.414709138870236, 66.65491715073586 ; AI error: 612.456010671868 BCD5 error: 343.2\n",
            "\tIntensity Truth: 100.0, AI forecast: 100.39226412773132, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 28.438352525234222, 68.39150300621986 ; AI error: 739.5418206918091 BCD5 error: 501.2\n",
            "Found AL112017 at 2017-09-05 06:00:00\n",
            "\tIntensity Truth: 155.0, AI forecast: 125.81244707107544, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 17.7, 61.9; AI forecast: 17.823579308390617, 57.70814675111323 ; AI error: 239.7937984355097 BCD5 error: 13.3\n",
            "\tIntensity Truth: 145.0, AI forecast: 125.5345368385315, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 19.468933033943177, 61.95944729447365 ; AI error: 319.3560611453049 BCD5 error: 74.2\n",
            "\tIntensity Truth: 135.0, AI forecast: 116.10060930252075, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 22.262925657629967, 65.39354294091463 ; AI error: 437.2907338399895 BCD5 error: 149.3\n",
            "\tIntensity Truth: 130.0, AI forecast: 107.73016154766083, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 25.254932951927184, 67.73208010941744 ; AI error: 604.9830884388025 BCD5 error: 336.0\n",
            "\tIntensity Truth: 115.0, AI forecast: 99.42374646663666, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 27.87261627912521, 69.99777370095254 ; AI error: 659.9974637412116 BCD5 error: 478.1\n",
            "Found AL112017 at 2017-09-05 12:00:00\n",
            "\tIntensity Truth: 155.0, AI forecast: 128.24986100196838, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 18.1, 63.3; AI forecast: 17.707570978999136, 58.82100052312016 ; AI error: 256.97414703911613 BCD5 error: 13.3\n",
            "\tIntensity Truth: 145.0, AI forecast: 126.65713429450989, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 19.325100219249723, 63.52467909157276 ; AI error: 313.78910749732523 BCD5 error: 59.2\n",
            "\tIntensity Truth: 135.0, AI forecast: 116.58855438232422, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 22.09621663093567, 66.92933393716812 ; AI error: 433.05815623782416 BCD5 error: 166.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 107.97354578971863, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 24.631923282146452, 69.79702971428632 ; AI error: 535.1651032569988 BCD5 error: 356.7\n",
            "\tIntensity Truth: 115.0, AI forecast: 98.9029860496521, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 27.024994242191312, 72.22091154456139 ; AI error: 523.9826206282178 BCD5 error: 466.9\n",
            "Found AL112017 at 2017-09-05 18:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 129.5686161518097, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 19.35233308970928, 65.564725728333 ; AI error: 284.4900241993412 BCD5 error: 80.5\n",
            "\tIntensity Truth: 140.0, AI forecast: 119.38436269760132, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 21.700785663723945, 69.67042401134968 ; AI error: 353.16068488174335 BCD5 error: 237.3\n",
            "\tIntensity Truth: 95.0, AI forecast: 109.7970563173294, BCD5 forecast: 35.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 24.033700227737427, 72.89067937731743 ; AI error: 406.08947963048644 BCD5 error: 430.3\n",
            "\tIntensity Truth: 100.0, AI forecast: 98.16352903842926, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 26.580736744403836, 75.53202700614929 ; AI error: 337.727038076947 BCD5 error: 503.7\n",
            "Found AL112017 at 2017-09-06 00:00:00\n",
            "\tIntensity Truth: 150.0, AI forecast: 132.59262561798096, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 19.2, 66.2; AI forecast: 17.923088136315346, 62.374018843472 ; AI error: 230.85650357093584 BCD5 error: 25.6\n",
            "\tIntensity Truth: 140.0, AI forecast: 131.03991150856018, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 19.147368869185446, 68.21842267215251 ; AI error: 233.46477662483218 BCD5 error: 124.0\n",
            "\tIntensity Truth: 145.0, AI forecast: 119.82975840568542, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 21.229581433534623, 72.66901235282421 ; AI error: 258.1579124231524 BCD5 error: 320.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 107.30222284793854, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 23.66149882674217, 76.10087384581566 ; AI error: 264.6351518567783 BCD5 error: 504.9\n",
            "\tIntensity Truth: 80.0, AI forecast: 95.04913032054901, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 26.773738741874695, 78.66824598312378 ; AI error: 162.49817935521628 BCD5 error: 519.5\n",
            "Found AL112017 at 2017-09-06 06:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 133.42804193496704, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 19.7, 67.6; AI forecast: 18.20989535152912, 66.888182272017 ; AI error: 98.17307471868236 BCD5 error: 28.9\n",
            "\tIntensity Truth: 135.0, AI forecast: 127.65971899032593, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 19.771447801589964, 73.63902158141136 ; AI error: 106.67412900870123 BCD5 error: 136.3\n",
            "\tIntensity Truth: 130.0, AI forecast: 112.78404474258423, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 22.912518942356108, 77.62932817041875 ; AI error: 48.24693385848021 BCD5 error: 327.9\n",
            "\tIntensity Truth: 115.0, AI forecast: 100.8087831735611, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 26.6154435634613, 78.7518126487732 ; AI error: 223.18220627410295 BCD5 error: 488.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 90.13202428817749, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 30.476893174648282, 77.07116626501083 ; AI error: 301.1966349314822 BCD5 error: 469.8\n",
            "Found AL112017 at 2017-09-06 12:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 124.9399721622467, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 20.2, 69.0; AI forecast: 19.628983718156814, 69.40848965644837 ; AI error: 41.31736285321418 BCD5 error: 23.3\n",
            "\tIntensity Truth: 135.0, AI forecast: 120.31087756156921, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 22.58761762678623, 73.97128556072713 ; AI error: 62.26819306359764 BCD5 error: 152.7\n",
            "\tIntensity Truth: 110.0, AI forecast: 108.32232296466827, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 26.367201149463654, 74.0167910695076 ; AI error: 362.897721681289 BCD5 error: 335.2\n",
            "\tIntensity Truth: 115.0, AI forecast: 98.7920480966568, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 31.096701252460477, 73.60687690377236 ; AI error: 576.3639339245722 BCD5 error: 457.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 90.38097441196442, BCD5 forecast: 40.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 35.5882824420929, 71.30266175866127 ; AI error: 678.7203459313591 BCD5 error: 440.6\n",
            "Found AL112017 at 2017-09-06 18:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 123.9818811416626, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 20.7, 70.4; AI forecast: 19.073116794228554, 69.93415247201919 ; AI error: 101.15768678067406 BCD5 error: 32.8\n",
            "\tIntensity Truth: 140.0, AI forecast: 121.37356042861938, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 21.94692879319191, 75.27290251255036 ; AI error: 40.60920648204749 BCD5 error: 187.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 110.99470555782318, BCD5 forecast: 40.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 25.519399619102476, 76.04723381400109 ; AI error: 269.66035998679854 BCD5 error: 372.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 99.92737948894501, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 30.539244985580446, 75.53734149336815 ; AI error: 440.90064264568025 BCD5 error: 451.9\n",
            "\tIntensity Truth: 45.0, AI forecast: 87.24615216255188, BCD5 forecast: 42.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 36.126668095588684, 71.57833679318428 ; AI error: 673.549876326041 BCD5 error: 463.4\n",
            "Found AL112017 at 2017-09-07 00:00:00\n",
            "\tIntensity Truth: 140.0, AI forecast: 123.2952332496643, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 21.1, 71.8; AI forecast: 20.454955533891916, 71.95756462812423 ; AI error: 39.72595261416573 BCD5 error: 39.7\n",
            "\tIntensity Truth: 145.0, AI forecast: 120.45256972312927, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 22.993116185069084, 75.9492159485817 ; AI error: 87.66853371804106 BCD5 error: 205.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 107.26617395877838, BCD5 forecast: 34.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 27.804267585277557, 76.63751496076584 ; AI error: 350.916866156347 BCD5 error: 372.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 93.41600716114044, BCD5 forecast: 39.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 33.33764514923096, 73.2384747505188 ; AI error: 588.9570924319455 BCD5 error: 397.1\n",
            "\tIntensity Truth: 35.0, AI forecast: 85.58814525604248, BCD5 forecast: 37.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 39.49272260665893, 69.44813634157181 ; AI error: 858.092147952618 BCD5 error: 461.1\n",
            "Found AL112017 at 2017-09-07 06:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 122.72497653961182, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 21.5, 73.2; AI forecast: 19.808167819678783, 72.63307258188725 ; AI error: 106.45442018964097 BCD5 error: 49.1\n",
            "\tIntensity Truth: 130.0, AI forecast: 117.7787435054779, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 23.370932763814924, 77.58654264807701 ; AI error: 70.39686474691142 BCD5 error: 205.4\n",
            "\tIntensity Truth: 115.0, AI forecast: 101.67082726955414, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 28.233368027210233, 76.06326545476914 ; AI error: 392.3114961214037 BCD5 error: 347.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 93.21460485458374, BCD5 forecast: 50.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 34.15247020721436, 73.37296146154404 ; AI error: 576.9201167468018 BCD5 error: 319.1\n",
            "Found AL112017 at 2017-09-07 12:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 118.87511372566223, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 21.8, 74.7; AI forecast: 22.069515106081962, 73.32325387895108 ; AI error: 78.3652350795962 BCD5 error: 61.8\n",
            "\tIntensity Truth: 110.0, AI forecast: 109.19368803501129, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 26.4474317073822, 74.66379851698875 ; AI error: 338.6215082576782 BCD5 error: 208.6\n",
            "\tIntensity Truth: 115.0, AI forecast: 99.56481277942657, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 32.47673834562302, 72.54151722788811 ; AI error: 672.3633374844148 BCD5 error: 306.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 88.62678706645966, BCD5 forecast: 62.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 39.02122266292572, 67.69965308904648 ; AI error: 932.3991152502256 BCD5 error: 269.4\n",
            "Found AL112017 at 2017-09-07 18:00:00\n",
            "\tIntensity Truth: 140.0, AI forecast: 111.22286081314087, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 22.0, 76.0; AI forecast: 22.263705402612686, 72.54712677597999 ; AI error: 192.6844894564813 BCD5 error: 63.4\n",
            "\tIntensity Truth: 95.0, AI forecast: 110.03589272499084, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 27.07672139406204, 75.06683301627636 ; AI error: 367.22689610543523 BCD5 error: 186.7\n",
            "\tIntensity Truth: 100.0, AI forecast: 97.42805659770966, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 33.011890864372255, 72.1750075340271 ; AI error: 667.7724892050754 BCD5 error: 230.3\n",
            "\tIntensity Truth: 45.0, AI forecast: 88.31733822822571, BCD5 forecast: 65.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 39.45395503044128, 67.67380473911763 ; AI error: 928.9388806931083 BCD5 error: 201.2\n",
            "Found AL112017 at 2017-09-08 00:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 118.54871273040771, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 22.1, 77.2; AI forecast: 23.428668439388275, 74.98867240846157 ; AI error: 146.11960947681519 BCD5 error: 69.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 110.58615148067474, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 28.310137891769408, 77.02229299247264 ; AI error: 361.6052767513259 BCD5 error: 159.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 97.73357093334198, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 34.117375111579896, 74.52271193861961 ; AI error: 574.9524186365175 BCD5 error: 144.1\n",
            "\tIntensity Truth: 35.0, AI forecast: 86.06134116649628, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 40.296630859375, 70.43957902938128 ; AI error: 842.5656675195912 BCD5 error: 150.3\n",
            "Found AL112017 at 2017-09-08 06:00:00\n",
            "\tIntensity Truth: 130.0, AI forecast: 117.08914995193481, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 22.4, 78.3; AI forecast: 23.780396699905396, 76.77325483262538 ; AI error: 118.23201657845159 BCD5 error: 60.0\n",
            "\tIntensity Truth: 115.0, AI forecast: 109.12043869495392, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 28.618192172050477, 79.47942428290844 ; AI error: 311.1465782509027 BCD5 error: 139.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 93.39008867740631, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 34.495263600349425, 76.92598105370999 ; AI error: 464.57014721963213 BCD5 error: 79.2\n",
            "Found AL112017 at 2017-09-08 12:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 115.91175317764282, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 22.7, 79.3; AI forecast: 23.815165102481842, 77.75062192976475 ; AI error: 108.56794214461814 BCD5 error: 47.5\n",
            "\tIntensity Truth: 115.0, AI forecast: 106.51939034461975, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 28.697389328479765, 80.15011799633503 ; AI error: 262.2182291068669 BCD5 error: 112.8\n",
            "\tIntensity Truth: 50.0, AI forecast: 91.66229963302612, BCD5 forecast: 68.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 34.911048483848575, 76.89310041964055 ; AI error: 434.11787648993464 BCD5 error: 127.3\n",
            "Found AL112017 at 2017-09-08 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 114.05817687511444, BCD5 forecast: 46.0\n",
            "\tTrajectory Truth: 23.1, 80.2; AI forecast: 23.688379657268523, 78.3645723849535 ; AI error: 107.13161426832454 BCD5 error: 25.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 105.24923026561737, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 28.690226113796236, 80.44257080256939 ; AI error: 197.3217748435292 BCD5 error: 108.4\n",
            "\tIntensity Truth: 45.0, AI forecast: 92.99989700317383, BCD5 forecast: 72.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 33.626560473442076, 78.03837750256062 ; AI error: 321.93139028860963 BCD5 error: 188.8\n",
            "Found AL112017 at 2017-09-09 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 114.59215939044952, BCD5 forecast: 44.0\n",
            "\tTrajectory Truth: 23.4, 80.9; AI forecast: 23.84111955165863, 78.75628773868084 ; AI error: 120.86212583881209 BCD5 error: 30.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 107.63356804847717, BCD5 forecast: 46.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 27.51442700624466, 81.35814299583436 ; AI error: 46.62030211788495 BCD5 error: 137.6\n",
            "\tIntensity Truth: 35.0, AI forecast: 93.12159955501556, BCD5 forecast: 79.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 32.62532585859299, 80.61140158176423 ; AI error: 197.20598547444405 BCD5 error: 194.0\n",
            "Found AL112017 at 2017-09-09 06:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 114.90422427654266, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 23.7, 81.3; AI forecast: 23.584955394268036, 81.12626923918724 ; AI error: 11.790516345454735 BCD5 error: 48.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 108.77858757972717, BCD5 forecast: 56.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 27.470638120174407, 84.80121849775314 ; AI error: 144.87985243919957 BCD5 error: 135.4\n",
            "Found AL112017 at 2017-09-09 12:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 110.73034703731537, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 24.5, 81.5; AI forecast: 23.240956991910934, 82.20395364165306 ; AI error: 84.90076602519855 BCD5 error: 54.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 105.0799423456192, BCD5 forecast: 52.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 26.9332212805748, 85.98217081427575 ; AI error: 236.1126249316019 BCD5 error: 169.6\n",
            "Found AL112017 at 2017-09-09 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 101.15038812160492, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 25.6, 81.7; AI forecast: 23.379617166519164, 83.01896434128284 ; AI error: 151.54156834611376 BCD5 error: 69.7\n",
            "\tIntensity Truth: 45.0, AI forecast: 96.17845356464386, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 27.903379499912262, 85.78858714103698 ; AI error: 216.08849446747593 BCD5 error: 211.6\n",
            "Found AL112017 at 2017-09-10 00:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 92.17139005661011, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 26.8, 81.7; AI forecast: 24.702967381477354, 82.49950797855854 ; AI error: 133.1220662070856 BCD5 error: 107.6\n",
            "\tIntensity Truth: 35.0, AI forecast: 75.69828271865845, BCD5 forecast: 58.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 30.22505713701248, 81.2111636787653 ; AI error: 192.36671232886857 BCD5 error: 263.2\n",
            "Found AL112017 at 2017-09-10 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.3025461435318, BCD5 forecast: 33.0\n",
            "\tTrajectory Truth: 28.2, 82.2; AI forecast: 24.82953578233719, 80.29060427844524 ; AI error: 226.86798598773404 BCD5 error: 109.2\n",
            "Found AL112017 at 2017-09-10 12:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 86.65218412876129, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 29.6, 82.7; AI forecast: 26.435767638683316, 80.54257269799709 ; AI error: 221.72736470014206 BCD5 error: 98.7\n",
            "Found AL112017 at 2017-09-10 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 100.8739423751831, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 30.9, 83.5; AI forecast: 29.322377181053163, 80.09514510333538 ; AI error: 200.5965095755656 BCD5 error: 142.7\n",
            "Found AL112017 at 2017-09-11 00:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 83.55311036109924, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 31.9, 84.4; AI forecast: 35.36909918785095, 74.87296106517314 ; AI error: 519.4921603736794 BCD5 error: 174.4\n",
            "Found AL112017 at 2017-09-11 06:00:00\n",
            "Found AL112017 at 2017-09-11 12:00:00\n",
            "Found AL112017 at 2017-09-11 18:00:00\n",
            "Found AL112017 at 2017-09-12 00:00:00\n",
            "Found AL122017 at 2017-09-05 12:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 46.06081848964095, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 13.1, 43.9; AI forecast: 15.441844785213469, 43.75659770071506 ; AI error: 140.85281912288906 BCD5 error: 44.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 48.48007172346115, BCD5 forecast: -31.0\n",
            "\tTrajectory Truth: 14.7, 49.9; AI forecast: 17.028404462337495, 47.481147709488866 ; AI error: 197.62462974334105 BCD5 error: 135.5\n",
            "\tIntensity Truth: 130.0, AI forecast: 52.81604327261448, BCD5 forecast: -68.0\n",
            "\tTrajectory Truth: 16.1, 56.4; AI forecast: 19.569480144977568, 52.16035445481538 ; AI error: 319.5067296260945 BCD5 error: 286.0\n",
            "\tIntensity Truth: 125.0, AI forecast: 58.80354732275009, BCD5 forecast: -67.0\n",
            "\tTrajectory Truth: 17.9, 60.8; AI forecast: 21.38064862191677, 55.006999376788734 ; AI error: 388.4902187425728 BCD5 error: 314.4\n",
            "\tIntensity Truth: 115.0, AI forecast: 61.71622455120087, BCD5 forecast: -52.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 22.220760849118232, 56.324145324598064 ; AI error: 504.34461543373936 BCD5 error: 363.9\n",
            "Found AL122017 at 2017-09-05 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 44.2079227976501, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 13.7, 45.2; AI forecast: 15.294969594478607, 43.96106471121311 ; AI error: 119.81918664887777 BCD5 error: 37.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 47.959741316735744, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 15.1, 51.5; AI forecast: 16.904030537605284, 47.79079798609018 ; AI error: 239.90293764770956 BCD5 error: 101.9\n",
            "\tIntensity Truth: 135.0, AI forecast: 55.234012603759766, BCD5 forecast: -69.0\n",
            "\tTrajectory Truth: 16.4, 57.8; AI forecast: 19.419913397729395, 52.208497282117605 ; AI error: 367.2658432532312 BCD5 error: 202.4\n",
            "\tIntensity Truth: 120.0, AI forecast: 60.16023725271225, BCD5 forecast: -56.0\n",
            "\tTrajectory Truth: 18.6, 61.8; AI forecast: 21.09845491349697, 55.0143886167556 ; AI error: 411.45812508938764 BCD5 error: 189.4\n",
            "\tIntensity Truth: 105.0, AI forecast: 63.9669269323349, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 21.960857558250428, 56.29349163703154 ; AI error: 567.9333785225572 BCD5 error: 266.9\n",
            "Found AL122017 at 2017-09-06 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 50.0246075540781, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 14.1, 46.7; AI forecast: 15.575834000110625, 44.54772712290287 ; AI error: 153.1474513291745 BCD5 error: 56.8\n",
            "\tIntensity Truth: 105.0, AI forecast: 57.505158334970474, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 15.5, 53.2; AI forecast: 17.346194007992743, 48.10206770598888 ; AI error: 313.8000932739115 BCD5 error: 114.8\n",
            "\tIntensity Truth: 135.0, AI forecast: 62.75094449520111, BCD5 forecast: -63.0\n",
            "\tTrajectory Truth: 16.7, 58.9; AI forecast: 19.81262661665678, 52.439239972084756 ; AI error: 413.0037765122331 BCD5 error: 179.2\n",
            "\tIntensity Truth: 115.0, AI forecast: 67.42271572351456, BCD5 forecast: -45.0\n",
            "\tTrajectory Truth: 19.4, 62.9; AI forecast: 21.50541532933712, 55.3153335377574 ; AI error: 444.94230029872165 BCD5 error: 176.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 70.60566574335098, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 22.66715837419033, 56.947960619162764 ; AI error: 589.8744848665643 BCD5 error: 272.9\n",
            "Found AL122017 at 2017-09-06 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 60.348028391599655, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 14.4, 48.3; AI forecast: 15.853861808776855, 45.21409836411476 ; AI error: 199.0165896732966 BCD5 error: 40.7\n",
            "\tIntensity Truth: 115.0, AI forecast: 67.39459425210953, BCD5 forecast: -43.0\n",
            "\tTrajectory Truth: 15.9, 54.9; AI forecast: 17.546349456906317, 48.78857176452875 ; AI error: 365.02356106709874 BCD5 error: 152.8\n",
            "\tIntensity Truth: 130.0, AI forecast: 72.5037732720375, BCD5 forecast: -54.0\n",
            "\tTrajectory Truth: 17.2, 59.9; AI forecast: 19.9187951952219, 53.14305871985852 ; AI error: 417.7353802127493 BCD5 error: 228.7\n",
            "\tIntensity Truth: 115.0, AI forecast: 76.11196875572205, BCD5 forecast: -41.0\n",
            "\tTrajectory Truth: 20.3, 64.0; AI forecast: 21.842246198654173, 56.20847632212099 ; AI error: 446.178522008029 BCD5 error: 255.3\n",
            "\tIntensity Truth: 90.0, AI forecast: 77.173712849617, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 23.367967742681504, 58.77618221789598 ; AI error: 542.3037131303674 BCD5 error: 343.5\n",
            "Found AL122017 at 2017-09-06 12:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 64.33296710252762, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 14.7, 49.9; AI forecast: 15.60454694032669, 46.45101813226938 ; AI error: 207.12177792707746 BCD5 error: 71.6\n",
            "\tIntensity Truth: 130.0, AI forecast: 72.48532325029373, BCD5 forecast: -55.0\n",
            "\tTrajectory Truth: 16.1, 56.4; AI forecast: 16.879919087886808, 50.46198449358344 ; AI error: 345.03406065829876 BCD5 error: 210.1\n",
            "\tIntensity Truth: 125.0, AI forecast: 76.96203410625458, BCD5 forecast: -46.0\n",
            "\tTrajectory Truth: 17.9, 60.8; AI forecast: 19.17847342789173, 55.037150591798124 ; AI error: 336.88714249142 BCD5 error: 272.4\n",
            "\tIntensity Truth: 115.0, AI forecast: 78.87946784496307, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 21.27593992948532, 58.862162574753164 ; AI error: 360.28311126328657 BCD5 error: 340.5\n",
            "\tIntensity Truth: 85.0, AI forecast: 78.7359419465065, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 22.946889680624007, 62.10344632714987 ; AI error: 425.8590259842829 BCD5 error: 406.4\n",
            "Found AL122017 at 2017-09-06 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 71.70266389846802, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 15.1, 51.5; AI forecast: 15.440757310390472, 47.92708643376827 ; AI error: 207.951739397566 BCD5 error: 68.1\n",
            "\tIntensity Truth: 135.0, AI forecast: 79.05528962612152, BCD5 forecast: -49.0\n",
            "\tTrajectory Truth: 16.4, 57.8; AI forecast: 16.665296018123627, 52.314713685214514 ; AI error: 316.11502309545216 BCD5 error: 187.0\n",
            "\tIntensity Truth: 120.0, AI forecast: 80.56298434734344, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 18.6, 61.8; AI forecast: 19.10186784863472, 57.60748287644237 ; AI error: 240.11067813063772 BCD5 error: 210.8\n",
            "\tIntensity Truth: 105.0, AI forecast: 80.2794498205185, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 21.32785586863756, 62.0561174608767 ; AI error: 253.25480643295137 BCD5 error: 293.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 81.51407420635223, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 22.914092513918877, 65.20909682810307 ; AI error: 317.95707230787195 BCD5 error: 318.9\n",
            "Found AL122017 at 2017-09-07 00:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 76.48656845092773, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 15.5, 53.2; AI forecast: 15.898981499671937, 49.17013888955116 ; AI error: 234.15294789814425 BCD5 error: 42.2\n",
            "\tIntensity Truth: 135.0, AI forecast: 82.52731919288635, BCD5 forecast: -46.0\n",
            "\tTrajectory Truth: 16.7, 58.9; AI forecast: 17.52021851837635, 53.82721797712147 ; AI error: 295.2172681902623 BCD5 error: 104.8\n",
            "\tIntensity Truth: 115.0, AI forecast: 82.38724172115326, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 19.4, 62.9; AI forecast: 20.245663334429263, 59.272233173623675 ; AI error: 211.09715274542484 BCD5 error: 94.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 83.72642040252686, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 22.43250319957733, 63.34659204930067 ; AI error: 240.991921284106 BCD5 error: 184.6\n",
            "\tIntensity Truth: 75.0, AI forecast: 81.5814197063446, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 24.072359657287596, 65.73404742777348 ; AI error: 273.1700954985777 BCD5 error: 187.6\n",
            "Found AL122017 at 2017-09-07 06:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 82.95144855976105, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 15.9, 54.9; AI forecast: 17.032016110420226, 50.01152107864618 ; AI error: 289.54681462881246 BCD5 error: 34.6\n",
            "\tIntensity Truth: 130.0, AI forecast: 86.9526207447052, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 17.2, 59.9; AI forecast: 19.328003469109536, 54.504110724478956 ; AI error: 333.0945928438128 BCD5 error: 49.7\n",
            "\tIntensity Truth: 115.0, AI forecast: 87.16524660587311, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 20.3, 64.0; AI forecast: 22.169950127601624, 59.203024229407305 ; AI error: 290.9635036023538 BCD5 error: 33.8\n",
            "\tIntensity Truth: 90.0, AI forecast: 84.51752245426178, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 24.458320093154907, 62.28451280593872 ; AI error: 345.4752229677785 BCD5 error: 148.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 85.00267624855042, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 26.290985536575317, 64.19862695336342 ; AI error: 273.8278689563023 BCD5 error: 108.7\n",
            "Found AL122017 at 2017-09-07 12:00:00\n",
            "\tIntensity Truth: 130.0, AI forecast: 90.09301722049713, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 16.1, 56.4; AI forecast: 17.968963727355003, 51.0938603207469 ; AI error: 324.59318331465835 BCD5 error: 34.0\n",
            "\tIntensity Truth: 125.0, AI forecast: 95.69074630737305, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 17.9, 60.8; AI forecast: 20.367307682335376, 55.5819445121102 ; AI error: 330.9507151230597 BCD5 error: 36.0\n",
            "\tIntensity Truth: 115.0, AI forecast: 91.74727320671082, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 23.222807690501213, 59.859816166758534 ; AI error: 325.8343954975851 BCD5 error: 34.1\n",
            "\tIntensity Truth: 85.0, AI forecast: 91.05685353279114, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 25.58421627283096, 62.964563679695125 ; AI error: 348.91094492302904 BCD5 error: 141.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 88.93223881721497, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 27.522099411487577, 65.37824047356844 ; AI error: 155.80837742641222 BCD5 error: 63.8\n",
            "Found AL122017 at 2017-09-07 18:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 101.08787834644318, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 16.4, 57.8; AI forecast: 18.1413472622633, 52.81454447247088 ; AI error: 304.3336419636543 BCD5 error: 32.1\n",
            "\tIntensity Truth: 120.0, AI forecast: 103.10437560081482, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 18.6, 61.8; AI forecast: 20.330228865146637, 57.792862076871096 ; AI error: 249.47997155757764 BCD5 error: 53.6\n",
            "\tIntensity Truth: 105.0, AI forecast: 100.67334949970245, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 23.0318484634161, 62.64827622920274 ; AI error: 219.23235553945034 BCD5 error: 16.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 96.98372900485992, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 25.335723018646238, 66.59736332297325 ; AI error: 171.6211164044387 BCD5 error: 99.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 91.96780145168304, BCD5 forecast: 30.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 27.395265901088713, 69.73247751742602 ; AI error: 124.33355884387205 BCD5 error: 156.2\n",
            "Found AL122017 at 2017-09-08 00:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 104.17942523956299, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 16.7, 58.9; AI forecast: 17.972591146826744, 54.979794955067334 ; AI error: 237.30586317030676 BCD5 error: 49.8\n",
            "\tIntensity Truth: 115.0, AI forecast: 110.06584167480469, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 19.4, 62.9; AI forecast: 19.90499072372913, 60.5912059366703 ; AI error: 134.01955057718342 BCD5 error: 85.7\n",
            "\tIntensity Truth: 100.0, AI forecast: 105.74163138866425, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 22.458665117621422, 66.08004954308271 ; AI error: 98.08503956596157 BCD5 error: 58.3\n",
            "\tIntensity Truth: 75.0, AI forecast: 99.85375046730042, BCD5 forecast: 37.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 24.84019273519516, 70.56072271466255 ; AI error: 154.9046552912754 BCD5 error: 112.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 92.35933244228363, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 27.217833268642423, 73.75930128097534 ; AI error: 378.3993775535514 BCD5 error: 291.2\n",
            "Found AL122017 at 2017-09-08 06:00:00\n",
            "\tIntensity Truth: 130.0, AI forecast: 114.43298935890198, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 17.2, 59.9; AI forecast: 18.129705348610877, 56.872582581639286 ; AI error: 181.96569827198368 BCD5 error: 60.0\n",
            "\tIntensity Truth: 115.0, AI forecast: 117.01886296272278, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 20.3, 64.0; AI forecast: 20.036207178235053, 62.892542542517184 ; AI error: 64.39329794825052 BCD5 error: 109.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 108.96641731262207, BCD5 forecast: 30.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 22.661028185486792, 68.53537740111351 ; AI error: 98.46897298790061 BCD5 error: 121.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 99.85993802547455, BCD5 forecast: 48.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 25.31349662542343, 72.77238201200961 ; AI error: 240.41949386157054 BCD5 error: 192.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 92.59042918682098, BCD5 forecast: 33.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 27.68057708740234, 75.67931664586067 ; AI error: 513.7435624748524 BCD5 error: 391.6\n",
            "Found AL122017 at 2017-09-08 12:00:00\n",
            "\tIntensity Truth: 125.0, AI forecast: 119.82370018959045, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 17.9, 60.8; AI forecast: 18.375166177749634, 58.262110928073525 ; AI error: 147.58674318911315 BCD5 error: 70.9\n",
            "\tIntensity Truth: 115.0, AI forecast: 119.50555205345154, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 20.41761315613985, 64.56328827738761 ; AI error: 62.57952035629879 BCD5 error: 115.9\n",
            "\tIntensity Truth: 85.0, AI forecast: 108.80306661128998, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 23.333483412861824, 69.79129891991616 ; AI error: 125.9064778025496 BCD5 error: 175.1\n",
            "\tIntensity Truth: 70.0, AI forecast: 100.83627045154572, BCD5 forecast: 54.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 25.97671432495117, 73.35570718348026 ; AI error: 289.90637704754414 BCD5 error: 252.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 93.2300215959549, BCD5 forecast: 35.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 27.931933975219724, 75.66774483323097 ; AI error: 543.6143995430766 BCD5 error: 467.3\n",
            "Found AL122017 at 2017-09-08 18:00:00\n",
            "\tIntensity Truth: 120.0, AI forecast: 123.01401019096375, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 18.6, 61.8; AI forecast: 18.704741269350052, 60.026813837885854 ; AI error: 101.06654915999384 BCD5 error: 59.4\n",
            "\tIntensity Truth: 105.0, AI forecast: 120.98708629608154, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 21.072695729136466, 66.2022530734539 ; AI error: 69.69383514201235 BCD5 error: 109.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 111.99171602725983, BCD5 forecast: 43.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 23.999594974517823, 70.86255303025246 ; AI error: 167.36488468901638 BCD5 error: 181.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 103.83370637893677, BCD5 forecast: 52.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 26.25438765287399, 74.00469866096974 ; AI error: 360.44277393489034 BCD5 error: 233.6\n",
            "\tIntensity Truth: 70.0, AI forecast: 92.93326914310455, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 28.906543481349942, 75.80770882368088 ; AI error: 584.2766431329871 BCD5 error: 470.8\n",
            "Found AL122017 at 2017-09-09 00:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 122.77891635894775, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 19.4, 62.9; AI forecast: 19.048117265105247, 61.803013735264535 ; AI error: 65.68134159942684 BCD5 error: 43.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 123.36881637573242, BCD5 forecast: 29.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 21.313178150355814, 67.90513924658299 ; AI error: 120.48771631446108 BCD5 error: 134.6\n",
            "\tIntensity Truth: 75.0, AI forecast: 114.24778401851654, BCD5 forecast: 48.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 23.736194002628327, 72.38513835668564 ; AI error: 263.47621966667714 BCD5 error: 158.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 102.26323068141937, BCD5 forecast: 46.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 26.60633746385574, 75.1074886739254 ; AI error: 451.0197799254946 BCD5 error: 197.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 90.81059038639069, BCD5 forecast: 23.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 30.056561625003816, 75.94158845543862 ; AI error: 620.5863681247037 BCD5 error: 457.9\n",
            "Found AL122017 at 2017-09-09 06:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 126.084223985672, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 20.3, 64.0; AI forecast: 19.101568004488943, 63.60409286022186 ; AI error: 75.35408114939136 BCD5 error: 32.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 126.34301662445068, BCD5 forecast: 35.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 20.795026813447475, 69.85536274015904 ; AI error: 221.64952121370686 BCD5 error: 143.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 112.3064136505127, BCD5 forecast: 47.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 23.70686823129654, 73.90472559332848 ; AI error: 349.5820308369707 BCD5 error: 96.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 99.15594160556793, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 27.297684562206268, 75.64735171794891 ; AI error: 509.7048089852647 BCD5 error: 242.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 87.4767941236496, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 31.60873967409134, 75.36643680930138 ; AI error: 642.0534928719849 BCD5 error: 507.7\n",
            "Found AL122017 at 2017-09-09 12:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 126.29063725471497, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 21.2, 65.3; AI forecast: 18.76224281489849, 65.25557739436627 ; AI error: 146.3855668433285 BCD5 error: 58.8\n",
            "\tIntensity Truth: 85.0, AI forecast: 121.6295576095581, BCD5 forecast: 34.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 20.88135301321745, 71.27594687938691 ; AI error: 290.38379127013303 BCD5 error: 158.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 107.20864176750183, BCD5 forecast: 41.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 24.446788203716277, 74.10894671976567 ; AI error: 369.0977851659751 BCD5 error: 27.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 94.3953937292099, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 28.984470665454865, 74.20856367945672 ; AI error: 487.1482753884292 BCD5 error: 336.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 83.49429309368134, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 34.21710286140442, 71.67316155731677 ; AI error: 630.9902991986402 BCD5 error: 574.7\n",
            "Found AL122017 at 2017-09-09 18:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 116.97618842124939, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 22.2, 66.5; AI forecast: 19.86061876565218, 65.97147816419601 ; AI error: 143.54594404950635 BCD5 error: 63.2\n",
            "\tIntensity Truth: 80.0, AI forecast: 112.95990407466888, BCD5 forecast: 34.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 22.98782939016819, 70.1167849779129 ; AI error: 213.53619213044053 BCD5 error: 120.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 99.9506276845932, BCD5 forecast: 36.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 27.90484173297882, 70.16988152116537 ; AI error: 150.3917821729894 BCD5 error: 104.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 88.9207273721695, BCD5 forecast: 27.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 33.829798102378845, 66.89686367511749 ; AI error: 510.619848754359 BCD5 error: 424.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 79.48485344648361, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 40.07225956916809, 61.10942652001977 ; AI error: 950.6368276672297 BCD5 error: 620.2\n",
            "Found AL122017 at 2017-09-10 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 108.60085487365723, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 23.3, 67.6; AI forecast: 21.234352466464042, 65.97010036557913 ; AI error: 153.56326701164113 BCD5 error: 62.8\n",
            "\tIntensity Truth: 75.0, AI forecast: 103.99227142333984, BCD5 forecast: 34.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 25.519431912899016, 67.66249068081379 ; AI error: 137.54058470786646 BCD5 error: 60.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 92.4409681558609, BCD5 forecast: 33.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 31.47237423658371, 64.72178853452206 ; AI error: 299.0802129612758 BCD5 error: 232.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 82.63362169265747, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 37.98274879455566, 58.894340107589954 ; AI error: 847.6006616311976 BCD5 error: 516.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 73.53078901767731, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 44.94942193031311, 50.87487597614526 ; AI error: 1425.9932617521054 BCD5 error: 672.3\n",
            "Found AL122017 at 2017-09-10 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 102.12054133415222, BCD5 forecast: 25.0\n",
            "\tTrajectory Truth: 24.3, 68.6; AI forecast: 23.274389708042143, 65.31733923107386 ; AI error: 190.5648960537833 BCD5 error: 48.7\n",
            "\tIntensity Truth: 70.0, AI forecast: 97.39889621734619, BCD5 forecast: 39.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 28.664183795452118, 64.63618781417608 ; AI error: 244.8129125503849 BCD5 error: 42.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 86.25701487064362, BCD5 forecast: 32.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 34.99328501224518, 59.52602961398661 ; AI error: 625.1183681458682 BCD5 error: 354.7\n",
            "\tIntensity Truth: 70.0, AI forecast: 76.44439250230789, BCD5 forecast: 23.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 42.03457183837891, 51.66640474796295 ; AI error: 1247.7943358292434 BCD5 error: 604.6\n",
            "\tIntensity Truth: 60.0, AI forecast: 69.08116072416306, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 48.58241996765136, 42.72297826558351 ; AI error: 1814.6586381240538 BCD5 error: 728.2\n",
            "Found AL122017 at 2017-09-10 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 100.30657708644867, BCD5 forecast: 31.0\n",
            "\tTrajectory Truth: 25.4, 69.4; AI forecast: 25.160944485664366, 65.17399306297303 ; AI error: 229.8699175993851 BCD5 error: 31.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 94.97962772846222, BCD5 forecast: 38.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 30.887703311443328, 63.4057398840785 ; AI error: 319.7899279904405 BCD5 error: 152.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 83.08829665184021, BCD5 forecast: 32.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 37.65344069004058, 56.80072969486937 ; AI error: 846.1488133349084 BCD5 error: 468.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 74.43932950496674, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 44.39129800796509, 48.18552160412073 ; AI error: 1471.0184088867654 BCD5 error: 683.4\n",
            "\tIntensity Truth: 60.0, AI forecast: 66.81746572256088, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 50.43767004013061, 39.03136230707168 ; AI error: 1996.7366825387396 BCD5 error: 782.7\n",
            "Found AL122017 at 2017-09-10 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 98.29077661037445, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 26.5, 69.5; AI forecast: 26.48463190793991, 66.07043462842702 ; AI error: 184.28754660573694 BCD5 error: 28.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 94.11878943443298, BCD5 forecast: 29.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 32.95831921100616, 63.15422846972942 ; AI error: 399.8131845269747 BCD5 error: 245.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 84.40499067306519, BCD5 forecast: 25.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 39.807315254211424, 55.39575914051383 ; AI error: 1005.6352467623108 BCD5 error: 551.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 75.17929553985596, BCD5 forecast: 23.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 46.501924419403075, 45.78733889758587 ; AI error: 1639.7636296262551 BCD5 error: 738.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 64.18125092983246, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 52.915853929519656, 34.41475170850754 ; AI error: 2225.4708290014996 BCD5 error: 849.2\n",
            "Found AL122017 at 2017-09-11 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 95.1323264837265, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 27.2, 69.4; AI forecast: 28.19832249879837, 66.4056822836399 ; AI error: 170.08187181460747 BCD5 error: 59.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 92.36943781375885, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 34.774109268188475, 62.41136439293623 ; AI error: 527.2403732670039 BCD5 error: 348.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 81.47262632846832, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 41.40639441013336, 53.49691661894321 ; AI error: 1152.2487380263447 BCD5 error: 615.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.67815166711807, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 48.34656410217285, 41.336952707171434 ; AI error: 1849.6956649051622 BCD5 error: 769.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 60.00092536211014, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 53.90384283065795, 30.554697537422175 ; AI error: 2380.9386841056958 BCD5 error: 903.5\n",
            "Found AL122017 at 2017-09-11 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 93.30861568450928, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 27.6, 69.1; AI forecast: 29.64212031364441, 67.11054285913706 ; AI error: 161.32179153419594 BCD5 error: 99.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 90.44406414031982, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 36.49233601093292, 62.196611797809595 ; AI error: 645.245503438346 BCD5 error: 425.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 76.41250342130661, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 43.74056448936462, 50.402820125222206 ; AI error: 1362.4041476938437 BCD5 error: 661.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 66.29607945680618, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 50.14144129753113, 38.2743191987276 ; AI error: 2009.5555020601637 BCD5 error: 785.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 56.175577044487, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 55.00944724082946, 28.17555897533893 ; AI error: 2482.1686497454093 BCD5 error: 939.8\n",
            "Found AL122017 at 2017-09-11 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 87.46536612510681, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 27.7, 68.3; AI forecast: 30.64078344106674, 67.54977742731572 ; AI error: 180.89177637614713 BCD5 error: 144.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 80.93989133834839, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 37.87781498432159, 60.208646372705694 ; AI error: 778.794550366851 BCD5 error: 490.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 71.38625979423523, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 44.40258731842041, 48.200575643777846 ; AI error: 1471.0758452578239 BCD5 error: 732.7\n",
            "\tIntensity Truth: 60.0, AI forecast: 60.420149117708206, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 49.77797536849975, 36.61711489260196 ; AI error: 2052.737632573698 BCD5 error: 877.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 53.48172105848789, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 53.91780877113342, 27.60333589315414 ; AI error: 2465.2277356873446 BCD5 error: 1066.5\n",
            "Found AL122017 at 2017-09-11 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 73.44937235116959, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 27.4, 67.4; AI forecast: 31.941521990299222, 65.83858546018601 ; AI error: 284.5696715059425 BCD5 error: 179.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 73.08044493198395, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 38.63515858650207, 57.854707959294316 ; AI error: 886.2023806383595 BCD5 error: 500.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 63.346614837646484, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 43.82217917442321, 46.04758513122797 ; AI error: 1522.1434797078293 BCD5 error: 739.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 56.96417048573494, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 48.353662729263306, 35.27456757128238 ; AI error: 2065.9766313283753 BCD5 error: 906.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 50.48036731779575, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 51.986771917343134, 26.83221295475959 ; AI error: 2442.4691923004334 BCD5 error: 1088.2\n",
            "Found AL122017 at 2017-09-12 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 72.34101921319962, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 26.8, 66.7; AI forecast: 33.07722947597504, 65.6784623682499 ; AI error: 380.60957700501757 BCD5 error: 194.6\n",
            "\tIntensity Truth: 70.0, AI forecast: 68.23940247297287, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 39.12358648777008, 57.52110740933567 ; AI error: 938.0648281522501 BCD5 error: 450.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 62.14501231908798, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 44.06841411590576, 45.74220475852489 ; AI error: 1545.8163065766105 BCD5 error: 646.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 54.86394822597504, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 48.30767035484314, 34.765958935022354 ; AI error: 2094.3776255434645 BCD5 error: 821.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 48.83073400706053, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 51.56788382530212, 26.654443866014475 ; AI error: 2419.5003971953774 BCD5 error: 971.4\n",
            "Found AL122017 at 2017-09-12 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 64.95347529649734, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 26.3, 66.2; AI forecast: 33.23452730178833, 65.55802411586046 ; AI error: 417.6913147923903 BCD5 error: 176.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 65.85218161344528, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 39.28789882659912, 57.64712002296 ; AI error: 959.3503097224668 BCD5 error: 400.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 59.771596640348434, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 44.01667795181274, 45.66745176464319 ; AI error: 1547.9607850616292 BCD5 error: 592.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 53.493954837322235, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 47.85240292549133, 35.15321106910705 ; AI error: 2073.3718518278047 BCD5 error: 796.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 47.89262346923351, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 51.19910368919372, 26.681002736091607 ; AI error: 2382.2443256720435 BCD5 error: 911.8\n",
            "Found AL122017 at 2017-09-12 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 67.32961028814316, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 25.8, 65.8; AI forecast: 34.02019331455231, 64.66465355753898 ; AI error: 497.0567193237492 BCD5 error: 147.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 64.97890561819077, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 39.95569999217987, 56.2041292615002 ; AI error: 1042.3556531395561 BCD5 error: 342.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 58.11011001467705, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 44.18661391735077, 44.79870098382234 ; AI error: 1589.2787621493808 BCD5 error: 557.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 51.79412662982941, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 47.990887737274164, 34.159725847840306 ; AI error: 2107.2896134446096 BCD5 error: 776.2\n",
            "\tIntensity Truth: 80.0, AI forecast: 48.18515457212925, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 51.47852163314819, 25.56366486549377 ; AI error: 2393.3520148750863 BCD5 error: 854.0\n",
            "Found AL122017 at 2017-09-12 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 65.42817741632462, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 25.4, 65.6; AI forecast: 33.79901385307312, 63.139678068459034 ; AI error: 520.3244441196623 BCD5 error: 83.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 63.20298880338669, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 38.830208611488345, 55.93270400706678 ; AI error: 999.185489316972 BCD5 error: 240.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 56.83950453996658, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 42.877590131759646, 45.36747999340295 ; AI error: 1536.9798542592675 BCD5 error: 448.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 52.677502781152725, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 46.7965790271759, 35.274788019061084 ; AI error: 2031.2003467373538 BCD5 error: 639.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 48.290032744407654, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 50.39777293205261, 27.495699721574777 ; AI error: 2274.0976969596363 BCD5 error: 683.3\n",
            "Found AL122017 at 2017-09-13 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 65.49584627151489, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 25.1, 65.7; AI forecast: 31.917825853824617, 63.70849353671073 ; AI error: 422.5861996497824 BCD5 error: 52.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 63.23200523853302, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 36.066036868095395, 58.660906470939516 ; AI error: 788.3836788080848 BCD5 error: 184.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 58.58738124370575, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 39.79630532264709, 50.22896016910672 ; AI error: 1268.5497643533238 BCD5 error: 376.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 53.111210241913795, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 43.41018993854523, 42.194429279863826 ; AI error: 1665.9971592163656 BCD5 error: 512.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 48.75329565256834, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 47.07529702186584, 35.52299526035785 ; AI error: 1880.6522156296812 BCD5 error: 535.3\n",
            "Found AL122017 at 2017-09-13 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 66.5188878774643, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 24.9, 65.9; AI forecast: 30.174430227279665, 63.375995543599124 ; AI error: 343.97450294736024 BCD5 error: 57.4\n",
            "\tIntensity Truth: 60.0, AI forecast: 66.40439718961716, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 33.56654357910156, 59.569154710695145 ; AI error: 647.6220938357446 BCD5 error: 184.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 60.15847444534302, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 36.425927948951724, 53.39906790480018 ; AI error: 1052.571768906074 BCD5 error: 366.1\n",
            "\tIntensity Truth: 75.0, AI forecast: 54.614847898483276, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 39.606908965110776, 47.3233249053359 ; AI error: 1346.8245045326332 BCD5 error: 457.1\n",
            "\tIntensity Truth: 75.0, AI forecast: 50.4817546159029, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 43.15809104442596, 42.03482424020767 ; AI error: 1515.5409061655273 BCD5 error: 466.3\n",
            "Found AL122017 at 2017-09-13 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.5406191945076, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 24.8, 66.3; AI forecast: 28.73077059984207, 62.08022088184953 ; AI error: 326.84928272237494 BCD5 error: 67.7\n",
            "\tIntensity Truth: 60.0, AI forecast: 69.01840478181839, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 31.130169641971587, 59.272466339543456 ; AI error: 588.2439745627024 BCD5 error: 205.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 62.822595834732056, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 33.48099055290222, 54.61674374770373 ; AI error: 939.4061951797428 BCD5 error: 345.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 57.48093068599701, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 36.378744459152216, 50.223241668939586 ; AI error: 1139.0255904202509 BCD5 error: 392.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 53.07436130940914, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 39.64674298763275, 46.31398948878049 ; AI error: 1250.7250264589154 BCD5 error: 400.5\n",
            "Found AL122017 at 2017-09-13 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 71.0646903514862, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 25.0, 66.9; AI forecast: 26.91422652006149, 61.810519558191295 ; AI error: 297.78290272507314 BCD5 error: 65.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 71.31749361753464, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 28.738500082492827, 60.04643072262406 ; AI error: 536.0786090434232 BCD5 error: 197.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 66.41892105340958, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 30.80814491510391, 56.94437028793618 ; AI error: 795.6351543326516 BCD5 error: 284.9\n",
            "\tIntensity Truth: 80.0, AI forecast: 61.4175882935524, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 33.4586177110672, 53.71894928999245 ; AI error: 926.0706703446868 BCD5 error: 308.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 57.07509905099869, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 36.61062142848968, 51.07384620606899 ; AI error: 993.6241436836808 BCD5 error: 328.1\n",
            "Found AL122017 at 2017-09-14 00:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 72.27913945913315, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 25.4, 67.6; AI forecast: 25.868418955802916, 62.391920050978655 ; AI error: 283.2983000252183 BCD5 error: 64.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.53934019804001, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 27.523592436313628, 61.567899820208545 ; AI error: 487.2307286907687 BCD5 error: 184.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 68.87043535709381, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 29.455103933811188, 59.167135966941714 ; AI error: 672.0684467955205 BCD5 error: 225.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 63.91390919685364, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 32.07444025278092, 56.95425482718274 ; AI error: 745.3250793783891 BCD5 error: 234.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 59.460890889167786, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 35.26476018428802, 55.33599935118109 ; AI error: 788.7105808841189 BCD5 error: 285.9\n",
            "Found AL122017 at 2017-09-14 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 73.78991544246674, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 25.8, 68.2; AI forecast: 25.574533390998837, 63.24989602267742 ; AI error: 268.1606388695438 BCD5 error: 69.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 75.4901859164238, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 27.303841412067413, 62.602285735309124 ; AI error: 469.03125700283897 BCD5 error: 183.2\n",
            "\tIntensity Truth: 75.0, AI forecast: 70.99459528923035, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 29.415660440921783, 60.82056192532181 ; AI error: 583.8035338267991 BCD5 error: 183.4\n",
            "\tIntensity Truth: 75.0, AI forecast: 66.00581705570221, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 32.25819571018219, 59.400988454371685 ; AI error: 607.9970215340801 BCD5 error: 188.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 61.30042418837547, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 35.521467328071594, 57.95276747662574 ; AI error: 666.052162704249 BCD5 error: 267.4\n",
            "Found AL122017 at 2017-09-14 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 72.5463518500328, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 26.3, 69.0; AI forecast: 25.175591599941253, 64.19930355846881 ; AI error: 268.2529411257655 BCD5 error: 64.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 76.78046077489853, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 27.102741181850433, 64.10506722033024 ; AI error: 414.43261782476736 BCD5 error: 149.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 74.36600506305695, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 29.544146943092343, 63.06252304166555 ; AI error: 462.86309558139857 BCD5 error: 140.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 70.14301747083664, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 32.574438345432284, 61.88145880550146 ; AI error: 472.1457295941852 BCD5 error: 153.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 63.075232207775116, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 35.94258894920349, 61.00783824846148 ; AI error: 518.7384343358697 BCD5 error: 258.7\n",
            "Found AL122017 at 2017-09-14 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.81232714653015, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 26.8, 69.9; AI forecast: 25.025107765197752, 65.52407854944468 ; AI error: 259.2093487953532 BCD5 error: 53.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 77.51396387815475, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 27.088356924057006, 66.11098006218671 ; AI error: 325.5049609159138 BCD5 error: 122.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 75.07396101951599, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 29.56857382059097, 65.18127970993518 ; AI error: 356.460247839012 BCD5 error: 131.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 68.42591166496277, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 32.68504534959793, 64.44852146804332 ; AI error: 350.18685191535144 BCD5 error: 158.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 65.65255671739578, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 36.518483471870425, 64.00723249614238 ; AI error: 361.4791596364124 BCD5 error: 274.9\n",
            "Found AL122017 at 2017-09-15 00:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.12988299131393, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 27.2, 70.7; AI forecast: 25.18568754196167, 67.32538617700338 ; AI error: 218.3411967228346 BCD5 error: 70.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 75.68305373191833, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 27.305614566802976, 67.7021051466465 ; AI error: 251.61491601825028 BCD5 error: 115.5\n",
            "\tIntensity Truth: 80.0, AI forecast: 71.72102212905884, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 29.897928488254546, 66.70573290735483 ; AI error: 279.29311334988944 BCD5 error: 127.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.09550929069519, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 33.60194959640503, 65.97321631014347 ; AI error: 280.3277114908687 BCD5 error: 191.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 63.012232184410095, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 37.936762428283686, 64.82348109483719 ; AI error: 303.254659731324 BCD5 error: 291.1\n",
            "Found AL122017 at 2017-09-15 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 68.69986444711685, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 27.7, 71.4; AI forecast: 25.60239918231964, 68.16070774793624 ; AI error: 214.63620324885233 BCD5 error: 70.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 70.57090610265732, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 27.863433575630186, 68.38115383088589 ; AI error: 223.02587517279488 BCD5 error: 83.5\n",
            "\tIntensity Truth: 75.0, AI forecast: 71.14076644182205, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 31.00232150554657, 67.11746746301651 ; AI error: 240.99352033150853 BCD5 error: 98.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 64.88064110279083, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 35.07371609210968, 65.38280289471149 ; AI error: 306.0944932371091 BCD5 error: 195.1\n",
            "\tIntensity Truth: 60.0, AI forecast: 62.19519183039665, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 39.355499505996704, 62.64730541110038 ; AI error: 389.089144360963 BCD5 error: 269.8\n",
            "Found AL122017 at 2017-09-15 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 63.52488577365875, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 28.2, 71.8; AI forecast: 26.262134408950804, 68.97717849016189 ; AI error: 190.37562505878304 BCD5 error: 35.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 70.19416391849518, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 29.162979507446288, 68.93698916435241 ; AI error: 173.94714347338402 BCD5 error: 31.7\n",
            "\tIntensity Truth: 70.0, AI forecast: 65.94512224197388, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 32.6012917637825, 66.57356683313847 ; AI error: 238.97369081275883 BCD5 error: 39.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 64.35268968343735, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 36.55462698936462, 63.3669919475913 ; AI error: 402.7252538689243 BCD5 error: 155.0\n",
            "\tIntensity Truth: 60.0, AI forecast: 61.25975236296654, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 40.88890209197998, 59.723125613853334 ; AI error: 500.51279100982094 BCD5 error: 211.4\n",
            "Found AL122017 at 2017-09-15 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 71.03703409433365, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 28.6, 72.0; AI forecast: 27.88556534051895, 69.50629216134548 ; AI error: 138.69485757435652 BCD5 error: 20.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 69.98384326696396, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 31.277023804187774, 68.66326339691878 ; AI error: 161.0870978971119 BCD5 error: 42.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 67.48425960540771, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 34.60416429042816, 65.12959827184677 ; AI error: 301.65268189802424 BCD5 error: 11.6\n",
            "\tIntensity Truth: 60.0, AI forecast: 63.33038657903671, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 38.55043468475341, 61.32185551077127 ; AI error: 491.6394729050369 BCD5 error: 109.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 61.02057695388794, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 42.68317847251892, 57.136861790809775 ; AI error: 593.5034077030941 BCD5 error: 137.3\n",
            "Found AL122017 at 2017-09-16 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 67.34261959791183, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 29.1, 72.0; AI forecast: 29.02416875362396, 69.95035368800163 ; AI error: 107.6624618687105 BCD5 error: 33.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 70.64720541238785, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 32.34423164129257, 68.36358075141906 ; AI error: 166.70996809054213 BCD5 error: 56.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 67.04253554344177, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 35.75615611076355, 64.38927527964115 ; AI error: 347.1623767995709 BCD5 error: 24.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 64.19083893299103, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 39.713869023323056, 60.07499015629291 ; AI error: 536.4266042905687 BCD5 error: 113.2\n",
            "\tIntensity Truth: 55.0, AI forecast: 57.64834940433502, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 44.09490704536438, 54.823789255321024 ; AI error: 673.2550783123339 BCD5 error: 113.0\n",
            "Found AL122017 at 2017-09-16 06:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 73.6642450094223, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 29.8, 72.0; AI forecast: 29.818877780437468, 70.50575660467148 ; AI error: 77.8520141431668 BCD5 error: 28.6\n",
            "\tIntensity Truth: 75.0, AI forecast: 74.53724354505539, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 33.43810439109802, 69.11840581595898 ; AI error: 123.05090282022269 BCD5 error: 32.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.48062533140182, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 36.97894644737244, 65.11693439483642 ; AI error: 324.4110542316617 BCD5 error: 77.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 62.033788561820984, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 41.237583470344546, 60.08682099506259 ; AI error: 529.2945085372631 BCD5 error: 159.2\n",
            "\tIntensity Truth: 55.0, AI forecast: 55.60001000761986, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 45.68987212181091, 54.212088003382085 ; AI error: 709.1243013311667 BCD5 error: 166.2\n",
            "Found AL122017 at 2017-09-16 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 75.0110736489296, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 30.5, 71.9; AI forecast: 30.573631119728088, 71.25104645490646 ; AI error: 33.849239283326035 BCD5 error: 12.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 77.33950436115265, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 34.26207084655762, 69.96956401616335 ; AI error: 76.51743656311419 BCD5 error: 42.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.50204825401306, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 38.003044319152835, 65.21000914275646 ; AI error: 326.88890215993143 BCD5 error: 139.7\n",
            "\tIntensity Truth: 60.0, AI forecast: 60.728241950273514, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 42.383948659896845, 59.35437371172011 ; AI error: 541.497374755396 BCD5 error: 191.5\n",
            "\tIntensity Truth: 55.0, AI forecast: 54.406274408102036, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 46.77808294296264, 52.60437740311026 ; AI error: 794.1502188971615 BCD5 error: 221.8\n",
            "Found AL122017 at 2017-09-16 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 78.70960623025894, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 31.2, 71.8; AI forecast: 31.079599809646606, 71.9530284911394 ; AI error: 10.681724400450205 BCD5 error: 26.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 75.78509628772736, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 34.89348065853119, 70.30422154963017 ; AI error: 60.79449405309751 BCD5 error: 77.0\n",
            "\tIntensity Truth: 60.0, AI forecast: 67.56516724824905, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 38.8084410905838, 64.96102439910173 ; AI error: 328.1629121705556 BCD5 error: 194.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 59.90046724677086, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 43.18888580799103, 58.30694915838539 ; AI error: 554.8499622853035 BCD5 error: 219.1\n",
            "\tIntensity Truth: 50.0, AI forecast: 53.93205627799034, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 47.52878603935241, 50.78554968833923 ; AI error: 888.1267383913984 BCD5 error: 293.9\n",
            "Found AL122017 at 2017-09-17 00:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 75.30244171619415, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.9, 71.6; AI forecast: 31.448862850666046, 71.51712014079094 ; AI error: 27.41554993950877 BCD5 error: 31.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 71.86189442873001, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 35.16428141593933, 69.31331048011779 ; AI error: 103.80564596500145 BCD5 error: 124.6\n",
            "\tIntensity Truth: 60.0, AI forecast: 62.89145588874817, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 38.76515538692475, 63.08402348309755 ; AI error: 389.22650999124284 BCD5 error: 257.8\n",
            "\tIntensity Truth: 55.0, AI forecast: 55.50536811351776, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 42.894669795036314, 55.471799263264984 ; AI error: 624.4798955347173 BCD5 error: 278.0\n",
            "\tIntensity Truth: 45.0, AI forecast: 50.41594296693802, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 47.3144633769989, 47.032610245049 ; AI error: 1031.0831805947737 BCD5 error: 439.4\n",
            "Found AL122017 at 2017-09-17 06:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 76.31074666976929, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 32.7, 71.4; AI forecast: 32.48780385255814, 70.650867202878 ; AI error: 39.978888800449184 BCD5 error: 37.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.11683404445648, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 36.18987832069397, 67.45299322158098 ; AI error: 204.772340317789 BCD5 error: 181.1\n",
            "\tIntensity Truth: 60.0, AI forecast: 59.71725836396217, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 39.60356993675232, 60.179794845730065 ; AI error: 504.3994147711795 BCD5 error: 302.6\n",
            "\tIntensity Truth: 55.0, AI forecast: 52.1709418296814, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 43.88737959861756, 51.46268005296588 ; AI error: 784.3903331834127 BCD5 error: 350.7\n",
            "\tIntensity Truth: 45.0, AI forecast: 46.16096720099449, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 48.227206230163574, 41.80111228823661 ; AI error: 1261.1024226522147 BCD5 error: 565.5\n",
            "Found AL122017 at 2017-09-17 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 77.30927586555481, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 33.5, 71.2; AI forecast: 33.38520565032959, 69.4207550317049 ; AI error: 89.40524841347866 BCD5 error: 45.4\n",
            "\tIntensity Truth: 60.0, AI forecast: 69.07996952533722, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 37.002061295509336, 65.66078245639801 ; AI error: 293.89267216601 BCD5 error: 213.6\n",
            "\tIntensity Truth: 60.0, AI forecast: 57.88117244839668, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 40.68294878005982, 57.717900717444714 ; AI error: 588.3015877526572 BCD5 error: 319.6\n",
            "\tIntensity Truth: 55.0, AI forecast: 49.198825135827065, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 44.97055859565735, 47.91833974421024 ; AI error: 942.7946114449908 BCD5 error: 404.8\n",
            "\tIntensity Truth: 40.0, AI forecast: 43.212388549000025, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 49.0724114894867, 37.46768721044063 ; AI error: 1452.0802671200809 BCD5 error: 642.3\n",
            "Found AL122017 at 2017-09-17 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 78.2489001750946, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 34.2, 71.2; AI forecast: 34.23137371540069, 68.54369591325522 ; AI error: 131.8927467535182 BCD5 error: 59.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 70.19158750772476, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 38.3995775938034, 64.24586636871099 ; AI error: 354.60215192956395 BCD5 error: 235.4\n",
            "\tIntensity Truth: 60.0, AI forecast: 57.97926381230354, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 42.27070107460022, 55.246261313557625 ; AI error: 668.8807984846776 BCD5 error: 322.6\n",
            "\tIntensity Truth: 50.0, AI forecast: 49.12750940769911, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 46.41386599540711, 44.275254516303534 ; AI error: 1116.7465135271111 BCD5 error: 464.8\n",
            "Found AL122017 at 2017-09-18 00:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 79.1592264175415, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 34.9, 71.4; AI forecast: 35.76839470863342, 67.56807968020439 ; AI error: 194.78137190271622 BCD5 error: 99.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 69.24352258443832, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 40.32822020053864, 62.00671977698803 ; AI error: 459.092334046645 BCD5 error: 263.6\n",
            "\tIntensity Truth: 55.0, AI forecast: 56.62290349602699, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 44.06698267459869, 51.36851069703698 ; AI error: 815.2535501948115 BCD5 error: 349.9\n",
            "\tIntensity Truth: 45.0, AI forecast: 46.883721482008696, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 48.2298978805542, 38.990648993849746 ; AI error: 1358.310437245866 BCD5 error: 555.5\n",
            "Found AL122017 at 2017-09-18 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 76.75431281328201, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 35.6, 71.6; AI forecast: 37.086348104476926, 66.67005004286766 ; AI error: 254.5335338835461 BCD5 error: 125.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.42095077037811, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 41.70349884033203, 59.408972267433995 ; AI error: 566.9495049204099 BCD5 error: 284.2\n",
            "\tIntensity Truth: 55.0, AI forecast: 55.96851706504822, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 45.595763492584226, 46.855647486448284 ; AI error: 999.0444985184661 BCD5 error: 386.0\n",
            "\tIntensity Truth: 45.0, AI forecast: 47.005337327718735, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 49.72751893997192, 33.853172862529746 ; AI error: 1585.9264409341254 BCD5 error: 636.8\n",
            "Found AL122017 at 2017-09-18 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 73.651682138443, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 36.3, 71.7; AI forecast: 37.630815505981445, 66.0435162588954 ; AI error: 282.81983891579586 BCD5 error: 122.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 65.92908591032028, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 42.27532434463501, 57.47963015977293 ; AI error: 618.4991243105496 BCD5 error: 243.4\n",
            "\tIntensity Truth: 55.0, AI forecast: 55.35540118813515, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 46.130909252166745, 44.09787629842758 ; AI error: 1114.4739208471922 BCD5 error: 374.6\n",
            "\tIntensity Truth: 40.0, AI forecast: 46.64624838158488, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 50.114488744735716, 31.140605339407916 ; AI error: 1704.9337726111125 BCD5 error: 638.2\n",
            "Found AL122017 at 2017-09-18 18:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 67.75620341300964, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 37.0, 71.5; AI forecast: 38.014662575721744, 65.3673528969288 ; AI error: 298.31411711375915 BCD5 error: 82.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 63.03704500198364, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 42.59466342926025, 55.85793283684179 ; AI error: 646.6416226995839 BCD5 error: 158.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 53.629472479224205, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 46.18130259513855, 42.14249602258205 ; AI error: 1200.5369521370235 BCD5 error: 343.1\n",
            "Found AL122017 at 2017-09-19 00:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 65.02478033304214, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 37.6, 71.2; AI forecast: 38.55875897407532, 64.99170140326024 ; AI error: 298.9424526201388 BCD5 error: 46.3\n",
            "\tIntensity Truth: 55.0, AI forecast: 58.424845188856125, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 42.809124279022214, 54.91137930490076 ; AI error: 647.0691172820148 BCD5 error: 113.2\n",
            "\tIntensity Truth: 45.0, AI forecast: 50.26386484503746, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 45.79514389038086, 41.52753320336342 ; AI error: 1230.3549042215247 BCD5 error: 355.2\n",
            "Found AL122017 at 2017-09-19 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 61.330066472291946, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 38.1, 70.8; AI forecast: 39.02058880329132, 64.6931523680687 ; AI error: 291.9278155312734 BCD5 error: 41.2\n",
            "\tIntensity Truth: 55.0, AI forecast: 55.87986350059509, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 42.80453705787659, 54.65635598786175 ; AI error: 634.0030742514243 BCD5 error: 128.2\n",
            "\tIntensity Truth: 45.0, AI forecast: 48.38714048266411, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 45.28417041301727, 41.90479905307292 ; AI error: 1224.675588966797 BCD5 error: 409.0\n",
            "Found AL122017 at 2017-09-19 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 60.76968461275101, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 38.7, 70.2; AI forecast: 39.30111975669861, 65.20039168447256 ; AI error: 236.02423680231564 BCD5 error: 48.6\n",
            "\tIntensity Truth: 55.0, AI forecast: 56.600437611341476, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 42.76337223052978, 55.71182667315006 ; AI error: 583.896850284476 BCD5 error: 192.4\n",
            "\tIntensity Truth: 40.0, AI forecast: 51.166997477412224, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 45.407969808578486, 42.93824467360973 ; AI error: 1196.5153649601523 BCD5 error: 492.3\n",
            "Found AL122017 at 2017-09-19 18:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 62.18895733356476, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 39.2, 69.4; AI forecast: 39.41101329326629, 66.05905528366566 ; AI error: 155.72103286906994 BCD5 error: 48.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 59.26204204559326, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 43.01308588981628, 57.08521183012053 ; AI error: 535.3748394074036 BCD5 error: 264.3\n",
            "Found AL122017 at 2017-09-20 00:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 65.40721565485, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.4, 68.5; AI forecast: 40.15633909702301, 66.38013492971659 ; AI error: 107.8393709946342 BCD5 error: 45.9\n",
            "\tIntensity Truth: 45.0, AI forecast: 59.14257273077965, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 43.72774310112, 57.1178299405612 ; AI error: 557.8369610571306 BCD5 error: 293.3\n",
            "Found AL122017 at 2017-09-20 06:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 64.17856186628342, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 39.7, 68.1; AI forecast: 40.52381246089935, 65.90738042891026 ; AI error: 112.17012049381599 BCD5 error: 70.8\n",
            "\tIntensity Truth: 45.0, AI forecast: 56.07567384839058, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 43.91406178474426, 56.26047648212406 ; AI error: 609.5263445082512 BCD5 error: 373.6\n",
            "Found AL122017 at 2017-09-20 12:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 60.34437760710716, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 39.7, 68.0; AI forecast: 40.67116379737854, 64.85975323319435 ; AI error: 155.38092923901894 BCD5 error: 134.9\n",
            "\tIntensity Truth: 40.0, AI forecast: 55.13304129242897, BCD5 forecast: 23.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 43.9975795507431, 54.92945294920355 ; AI error: 679.0263975738864 BCD5 error: 476.9\n",
            "Found AL122017 at 2017-09-20 18:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 62.3797932267189, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 39.5, 68.0; AI forecast: 41.291979742050174, 63.620718862116334 ; AI error: 227.27938819095687 BCD5 error: 175.9\n",
            "Found AL122017 at 2017-09-21 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 60.26075899600983, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 39.5, 68.2; AI forecast: 41.218579697608945, 62.96195434108376 ; AI error: 260.8565108970057 BCD5 error: 178.5\n",
            "Found AL122017 at 2017-09-21 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 59.18787583708763, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 39.6, 68.6; AI forecast: 41.037940216064456, 62.31638531535864 ; AI error: 300.2480349091865 BCD5 error: 140.3\n",
            "Found AL122017 at 2017-09-21 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 58.1962987780571, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 39.7, 69.0; AI forecast: 41.12675266265869, 61.397374136745924 ; AI error: 357.81629973403585 BCD5 error: 97.5\n",
            "Found AL122017 at 2017-09-21 18:00:00\n",
            "Found AL122017 at 2017-09-22 00:00:00\n",
            "Found AL122017 at 2017-09-22 06:00:00\n",
            "Found AL122017 at 2017-09-22 12:00:00\n",
            "Found AL132017 at 2017-09-06 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 60.364899188280106, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 21.6, 94.6; AI forecast: 23.924834835529328, 89.18387295603752 ; AI error: 330.7130288505038 BCD5 error: 37.4\n",
            "\tIntensity Truth: 90.0, AI forecast: 54.6601927280426, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 21.1, 96.2; AI forecast: 25.591979551315305, 90.55707947611809 ; AI error: 411.5950184063331 BCD5 error: 183.7\n",
            "Found AL132017 at 2017-09-07 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 62.6736855506897, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 21.5, 95.0; AI forecast: 23.46306358575821, 88.62078828215598 ; AI error: 372.96343674143026 BCD5 error: 49.1\n",
            "\tIntensity Truth: 70.0, AI forecast: 60.554298758506775, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 20.8, 96.9; AI forecast: 24.6937433719635, 90.08017970919609 ; AI error: 444.0049083787983 BCD5 error: 197.1\n",
            "Found AL132017 at 2017-09-07 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 70.95451384782791, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 21.4, 95.3; AI forecast: 23.3986175596714, 87.6419933617115 ; AI error: 441.6359292493452 BCD5 error: 53.1\n",
            "\tIntensity Truth: 35.0, AI forecast: 68.6601334810257, BCD5 forecast: 42.0\n",
            "\tTrajectory Truth: 20.3, 97.4; AI forecast: 24.990312325954434, 88.95079860687255 ; AI error: 546.1276730365867 BCD5 error: 220.9\n",
            "Found AL132017 at 2017-09-07 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 76.60558193922043, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 21.1, 95.7; AI forecast: 23.69046448469162, 86.78898594379424 ; AI error: 518.414097211608 BCD5 error: 71.6\n",
            "Found AL132017 at 2017-09-07 18:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 81.55813813209534, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 21.1, 96.2; AI forecast: 23.399498879909515, 87.38980788588523 ; AI error: 508.5627363474712 BCD5 error: 58.5\n",
            "Found AL132017 at 2017-09-08 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 72.60327279567719, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 20.8, 96.9; AI forecast: 23.295479434728623, 89.133475202322 ; AI error: 457.34688989084793 BCD5 error: 47.6\n",
            "Found AL132017 at 2017-09-08 06:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 72.64382153749466, BCD5 forecast: 54.0\n",
            "\tTrajectory Truth: 20.3, 97.4; AI forecast: 23.167645379900932, 89.41087990403176 ; AI error: 477.570805302162 BCD5 error: 58.6\n",
            "Found AL132017 at 2017-09-08 12:00:00\n",
            "Found AL142017 at 2017-09-16 12:00:00\n",
            "Found AL142017 at 2017-09-16 18:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 51.83391131460667, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 16.8, 44.4; AI forecast: 19.09465487897396, 48.30193922817707 ; AI error: 261.99940462377305 BCD5 error: 24.9\n",
            "Found AL142017 at 2017-09-17 00:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 57.07749605178833, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 17.6, 45.0; AI forecast: 17.62606932222843, 49.294837295264 ; AI error: 245.77560852074703 BCD5 error: 64.2\n",
            "Found AL142017 at 2017-09-17 06:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 55.86506113409996, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 18.3, 45.2; AI forecast: 17.499246701598167, 49.19044679328799 ; AI error: 232.99879983701842 BCD5 error: 113.2\n",
            "Found AL142017 at 2017-09-19 18:00:00\n",
            "Found AL142017 at 2017-09-20 00:00:00\n",
            "Found AL142017 at 2017-09-20 06:00:00\n",
            "Found AL142017 at 2017-09-23 00:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 39.248669892549515, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 31.9, 50.1; AI forecast: 29.72419236898422, 52.3692355081439 ; AI error: 175.36980255782245 BCD5 error: 106.6\n",
            "\tIntensity Truth: 85.0, AI forecast: 41.96332283318043, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 31.2, 49.6; AI forecast: 29.937006986141206, 52.68155701830983 ; AI error: 176.42160632763287 BCD5 error: 284.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 45.5088085308671, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 30.3, 51.0; AI forecast: 29.502529251575467, 53.144612452760335 ; AI error: 121.4559270817465 BCD5 error: 539.4\n",
            "\tIntensity Truth: 95.0, AI forecast: 50.79957157373428, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 29.9, 55.1; AI forecast: 29.799493992328642, 52.51699147969484 ; AI error: 134.64293257845384 BCD5 error: 863.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 52.57689341902733, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 30.88865860700607, 50.72299042418599 ; AI error: 334.1954145700602 BCD5 error: 1061.5\n",
            "Found AL142017 at 2017-09-23 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 40.43218173086643, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 31.7, 50.2; AI forecast: 30.160890865325925, 51.30259723514318 ; AI error: 108.46032282211465 BCD5 error: 108.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 43.51411571726203, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 31.0, 49.5; AI forecast: 30.674562752246857, 51.37117599546909 ; AI error: 98.4211843729732 BCD5 error: 267.7\n",
            "\tIntensity Truth: 85.0, AI forecast: 50.28038464486599, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 30.1, 52.0; AI forecast: 30.677588605880736, 51.19234451800585 ; AI error: 54.3351559311038 BCD5 error: 538.2\n",
            "\tIntensity Truth: 95.0, AI forecast: 53.4714837372303, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 30.1, 56.0; AI forecast: 31.530020165443418, 49.67032726854086 ; AI error: 337.4302751926439 BCD5 error: 838.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 50.867942944169044, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 32.546537256240846, 46.96978262811899 ; AI error: 524.6023236989568 BCD5 error: 979.1\n",
            "Found AL142017 at 2017-09-23 12:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 44.81175958644599, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 31.5, 50.1; AI forecast: 31.21891975402832, 50.42987924218178 ; AI error: 23.892421444492612 BCD5 error: 104.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 50.707643777132034, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 30.8, 49.7; AI forecast: 32.24200750589371, 49.90223748385906 ; AI error: 87.19529425154273 BCD5 error: 261.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 53.862877041101456, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 29.9, 53.2; AI forecast: 32.755942249298094, 48.787958750128745 ; AI error: 283.8591764171114 BCD5 error: 540.7\n",
            "\tIntensity Truth: 100.0, AI forecast: 51.878383085131645, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 30.3, 56.6; AI forecast: 33.41684455871582, 46.24844358116388 ; AI error: 559.7520365634863 BCD5 error: 797.6\n",
            "\tIntensity Truth: 85.0, AI forecast: 52.876852825284004, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 34.68365659713745, 42.58507037460804 ; AI error: 735.1281764927709 BCD5 error: 871.6\n",
            "Found AL142017 at 2017-09-23 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 53.17817449569702, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 31.3, 49.8; AI forecast: 32.349811708927156, 49.96665698811412 ; AI error: 63.60196061144102 BCD5 error: 98.8\n",
            "\tIntensity Truth: 75.0, AI forecast: 56.61687135696411, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 30.6, 50.2; AI forecast: 33.968906259536745, 48.80656793415546 ; AI error: 214.27264306966242 BCD5 error: 228.5\n",
            "\tIntensity Truth: 95.0, AI forecast: 54.67309460043907, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 29.9, 54.2; AI forecast: 34.31647312641144, 46.847089025378224 ; AI error: 458.19722494616207 BCD5 error: 471.2\n",
            "\tIntensity Truth: 100.0, AI forecast: 55.67701756954193, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 30.8, 57.0; AI forecast: 35.26074223518371, 43.537937244772905 ; AI error: 727.8508023710859 BCD5 error: 676.8\n",
            "\tIntensity Truth: 80.0, AI forecast: 54.89314407110214, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 36.857617902755734, 39.84359536468982 ; AI error: 831.3865021617577 BCD5 error: 672.6\n",
            "Found AL142017 at 2017-09-24 00:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 55.90121850371361, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 31.2, 49.6; AI forecast: 33.151934790611264, 49.80137033089995 ; AI error: 117.64095192043185 BCD5 error: 73.9\n",
            "\tIntensity Truth: 80.0, AI forecast: 55.1724249124527, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 30.3, 51.0; AI forecast: 34.43697104454041, 48.17336390912533 ; AI error: 286.7434685001435 BCD5 error: 186.3\n",
            "\tIntensity Truth: 95.0, AI forecast: 56.34400129318237, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 29.9, 55.1; AI forecast: 35.03682456016541, 45.75383507460356 ; AI error: 564.6101292608669 BCD5 error: 431.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 55.71394056081772, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 36.26956138610839, 42.5312811166048 ; AI error: 786.765700252561 BCD5 error: 583.7\n",
            "\tIntensity Truth: 75.0, AI forecast: 54.08580645918846, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 38.18776483535767, 38.21845942735672 ; AI error: 841.3166176857776 BCD5 error: 501.0\n",
            "Found AL142017 at 2017-09-24 06:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 60.2881033718586, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 31.0, 49.5; AI forecast: 32.74911173582077, 49.696701301634306 ; AI error: 105.49523125948188 BCD5 error: 55.2\n",
            "\tIntensity Truth: 85.0, AI forecast: 60.368701219558716, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 30.1, 52.0; AI forecast: 34.052191209793094, 48.63259054720402 ; AI error: 292.6241486686707 BCD5 error: 207.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 56.30539685487747, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 30.1, 56.0; AI forecast: 34.69693760871887, 47.12294804900884 ; AI error: 527.5778764456978 BCD5 error: 467.8\n",
            "\tIntensity Truth: 90.0, AI forecast: 52.85669006407261, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 36.04489419460297, 43.84971145093441 ; AI error: 708.8700435687315 BCD5 error: 587.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 53.43118794262409, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 37.87294538021088, 39.8774205327034 ; AI error: 662.687195911389 BCD5 error: 410.6\n",
            "Found AL142017 at 2017-09-24 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 71.45749598741531, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 30.8, 49.7; AI forecast: 32.91776421070099, 49.33619753792882 ; AI error: 128.49756863780118 BCD5 error: 44.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 67.06490963697433, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 29.9, 53.2; AI forecast: 34.335325193405154, 48.9646272957325 ; AI error: 342.4113706334879 BCD5 error: 292.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 59.1425059735775, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 30.3, 56.6; AI forecast: 35.175039005279544, 47.37960861474275 ; AI error: 549.6212822904006 BCD5 error: 538.0\n",
            "\tIntensity Truth: 85.0, AI forecast: 56.13908797502518, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 36.49603102207183, 44.5917268037796 ; AI error: 655.8584480753454 BCD5 error: 613.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 50.82315042614937, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 38.64650647640228, 41.217424225807186 ; AI error: 467.19810785388404 BCD5 error: 322.5\n",
            "Found AL142017 at 2017-09-24 18:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 76.70200020074844, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 30.6, 50.2; AI forecast: 32.49946792125702, 49.60254720747471 ; AI error: 118.07005293023241 BCD5 error: 85.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 71.27673834562302, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 29.9, 54.2; AI forecast: 33.89270641803741, 49.192846706509584 ; AI error: 350.06495254828997 BCD5 error: 366.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 64.89506274461746, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 30.8, 57.0; AI forecast: 34.69642541408539, 47.96773191690445 ; AI error: 512.3047168755899 BCD5 error: 577.2\n",
            "\tIntensity Truth: 80.0, AI forecast: 56.66217863559723, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 36.343945264816284, 45.39670543372631 ; AI error: 562.0225386050719 BCD5 error: 594.5\n",
            "\tIntensity Truth: 55.0, AI forecast: 49.84310984611511, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 38.775976061820984, 41.98773998618125 ; AI error: 315.70893806311955 BCD5 error: 215.7\n",
            "Found AL142017 at 2017-09-25 00:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 80.57963609695435, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 30.3, 51.0; AI forecast: 32.02455384731293, 48.93929275870323 ; AI error: 148.08147100558668 BCD5 error: 125.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 76.61951541900635, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 29.9, 55.1; AI forecast: 33.275510382652286, 48.453382612764834 ; AI error: 395.63402688930836 BCD5 error: 388.4\n",
            "\tIntensity Truth: 95.0, AI forecast: 64.15378868579865, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 34.23364629745483, 47.11875954121351 ; AI error: 536.0874218729394 BCD5 error: 545.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 54.168869107961655, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 36.04850959777832, 44.351966103911394 ; AI error: 542.4712296133836 BCD5 error: 481.2\n",
            "\tIntensity Truth: 50.0, AI forecast: 48.11213783919811, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 38.616645979881284, 41.601218721270556 ; AI error: 304.955259612281 BCD5 error: 157.0\n",
            "Found AL142017 at 2017-09-25 06:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 86.51950001716614, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 30.1, 52.0; AI forecast: 31.842531740665436, 48.5570193529129 ; AI error: 205.80151867261267 BCD5 error: 133.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 78.50502043962479, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 30.1, 56.0; AI forecast: 33.47598125934601, 47.441151699423784 ; AI error: 481.3106663091836 BCD5 error: 353.6\n",
            "\tIntensity Truth: 90.0, AI forecast: 65.18017441034317, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 34.89165267944336, 45.49419337660074 ; AI error: 613.9754067479709 BCD5 error: 443.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 55.989384949207306, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 37.12935292720795, 43.13595326393842 ; AI error: 509.7313002384809 BCD5 error: 291.1\n",
            "Found AL142017 at 2017-09-25 12:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 82.12579548358917, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 29.9, 53.2; AI forecast: 31.521730422973633, 48.13179763257503 ; AI error: 279.11546266606314 BCD5 error: 133.3\n",
            "\tIntensity Truth: 100.0, AI forecast: 74.81343060731888, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 30.3, 56.6; AI forecast: 33.31206545829773, 46.54984487146139 ; AI error: 543.467111754457 BCD5 error: 278.4\n",
            "\tIntensity Truth: 85.0, AI forecast: 64.13382411003113, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 35.02996926307678, 44.937555104494095 ; AI error: 621.8846036861946 BCD5 error: 308.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 53.59241805970669, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 37.46864657402038, 41.93747450113296 ; AI error: 446.1858992104089 BCD5 error: 116.7\n",
            "Found AL142017 at 2017-09-25 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 76.04991376399994, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 29.9, 54.2; AI forecast: 31.094142532348634, 48.343458871543405 ; AI error: 311.30691047501455 BCD5 error: 95.2\n",
            "\tIntensity Truth: 100.0, AI forecast: 73.63762140274048, BCD5 forecast: -35.0\n",
            "\tTrajectory Truth: 30.8, 57.0; AI forecast: 33.17408082485199, 46.95935036092996 ; AI error: 530.5570188693246 BCD5 error: 165.1\n",
            "\tIntensity Truth: 80.0, AI forecast: 62.09912613034248, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 35.02885475158691, 44.640857830643654 ; AI error: 591.8424847307573 BCD5 error: 119.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 56.760192811489105, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 37.604740142822266, 41.50400972962379 ; AI error: 371.7630755007225 BCD5 error: 348.2\n",
            "Found AL142017 at 2017-09-26 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 76.32207661867142, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 29.9, 55.1; AI forecast: 30.950679218769075, 49.48586631789803 ; AI error: 297.3862428096206 BCD5 error: 50.1\n",
            "\tIntensity Truth: 95.0, AI forecast: 68.08846235275269, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 32.80821163654328, 47.72632464170456 ; AI error: 489.00769975938954 BCD5 error: 46.5\n",
            "\tIntensity Truth: 75.0, AI forecast: 61.11483708024025, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 34.56528105735779, 45.645655414462084 ; AI error: 488.4295435715411 BCD5 error: 128.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 52.34305180609226, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 36.98010301589966, 43.270290748775 ; AI error: 377.680054503473 BCD5 error: 662.6\n",
            "Found AL142017 at 2017-09-26 06:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 70.77278405427933, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 30.1, 56.0; AI forecast: 30.096008121967316, 50.600184477865696 ; AI error: 280.4682322173783 BCD5 error: 12.0\n",
            "\tIntensity Truth: 90.0, AI forecast: 68.24843555688858, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 31.766437792778014, 49.54832426086068 ; AI error: 395.40285208637226 BCD5 error: 54.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 57.25847691297531, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 33.22113513946533, 48.47494155764579 ; AI error: 357.82606223613567 BCD5 error: 314.3\n",
            "Found AL142017 at 2017-09-26 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 80.10305643081665, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 30.3, 56.6; AI forecast: 29.91734607219696, 52.133455586433406 ; AI error: 233.10956895690248 BCD5 error: 16.7\n",
            "\tIntensity Truth: 85.0, AI forecast: 72.03361481428146, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 31.50768337249756, 51.926505570858716 ; AI error: 282.3175222748618 BCD5 error: 113.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 66.69590294361115, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 32.914752626419066, 51.55262317061424 ; AI error: 365.7618943187573 BCD5 error: 477.5\n",
            "Found AL142017 at 2017-09-26 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 81.14887058734894, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 30.8, 57.0; AI forecast: 29.434716033935544, 53.83998359926045 ; AI error: 183.43674634254077 BCD5 error: 16.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 80.59779822826385, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 30.801466858386995, 54.50053332336247 ; AI error: 235.38020593837405 BCD5 error: 154.3\n",
            "\tIntensity Truth: 55.0, AI forecast: 71.57097488641739, BCD5 forecast: 24.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 32.09159201383591, 55.06038875523954 ; AI error: 628.4528096093312 BCD5 error: 605.6\n",
            "Found AL142017 at 2017-09-27 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 93.07749807834625, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 31.4, 57.2; AI forecast: 29.393928217887876, 55.62111999010667 ; AI error: 145.57233101008924 BCD5 error: 43.2\n",
            "\tIntensity Truth: 75.0, AI forecast: 88.13362240791321, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 30.84727598428726, 56.90029260246083 ; AI error: 299.7074723141874 BCD5 error: 233.2\n",
            "\tIntensity Truth: 50.0, AI forecast: 77.19623297452927, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 32.267379915714265, 57.701537133194506 ; AI error: 905.4383564294372 BCD5 error: 781.8\n",
            "Found AL142017 at 2017-09-27 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 94.04961228370667, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 32.1, 57.3; AI forecast: 29.41496274471283, 57.17336423350498 ; AI error: 161.34322425495 BCD5 error: 50.6\n",
            "\tIntensity Truth: 70.0, AI forecast: 89.26090955734253, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 30.894313025474545, 58.60767702311277 ; AI error: 452.66085991605013 BCD5 error: 314.7\n",
            "Found AL142017 at 2017-09-27 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 94.91246581077576, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 33.0, 57.2; AI forecast: 29.484190130233763, 58.091984170302744 ; AI error: 215.99673041012255 BCD5 error: 49.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 86.79064691066742, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 31.250123822689055, 59.489702787622804 ; AI error: 617.6582164109682 BCD5 error: 378.5\n",
            "Found AL142017 at 2017-09-27 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 91.97884976863861, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 34.3, 56.6; AI forecast: 30.306023693084718, 58.168172830343245 ; AI error: 252.6493656071528 BCD5 error: 54.1\n",
            "\tIntensity Truth: 55.0, AI forecast: 81.00099921226501, BCD5 forecast: 27.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 32.21247370243073, 58.702498183771965 ; AI error: 730.076747194838 BCD5 error: 435.4\n",
            "Found AL142017 at 2017-09-28 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 85.05412936210632, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 35.7, 55.5; AI forecast: 31.017307329177854, 58.219243585690855 ; AI error: 312.4368057004734 BCD5 error: 94.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 76.427181661129, BCD5 forecast: 26.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 33.2608820438385, 57.75979151856154 ; AI error: 862.3948340912796 BCD5 error: 559.8\n",
            "Found AL142017 at 2017-09-28 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 82.13421106338501, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 37.3, 53.8; AI forecast: 32.47894108295441, 57.80995737817138 ; AI error: 350.3209892165719 BCD5 error: 143.8\n",
            "Found AL142017 at 2017-09-28 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 79.981310069561, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 39.0, 51.2; AI forecast: 33.672958397865294, 56.89995239218697 ; AI error: 422.0560663283692 BCD5 error: 167.6\n",
            "Found AL142017 at 2017-09-28 18:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 69.30972903966904, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 41.0, 48.2; AI forecast: 35.74943599700928, 54.38196251802146 ; AI error: 428.7764645633876 BCD5 error: 169.9\n",
            "Found AL142017 at 2017-09-29 00:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 59.79249894618988, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 43.2, 44.5; AI forecast: 37.16990041732788, 51.980392758548255 ; AI error: 498.3958419941069 BCD5 error: 190.8\n",
            "Found AL142017 at 2017-09-29 06:00:00\n",
            "Found AL152017 at 2017-09-17 18:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 86.24756872653961, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 14.9, 60.4; AI forecast: 15.381114423274994, 60.209913523495196 ; AI error: 30.915819302596947 BCD5 error: 66.2\n",
            "\tIntensity Truth: 145.0, AI forecast: 93.1589126586914, BCD5 forecast: -61.0\n",
            "\tTrajectory Truth: 16.6, 63.5; AI forecast: 16.857316434383392, 65.51270513981581 ; AI error: 116.7555578412622 BCD5 error: 114.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 96.51551485061646, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 18.6, 67.0; AI forecast: 19.45833926051855, 70.95150879621505 ; AI error: 230.12258196049845 BCD5 error: 94.9\n",
            "\tIntensity Truth: 105.0, AI forecast: 93.73748064041138, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 20.5, 69.5; AI forecast: 21.747664052248002, 75.46769525706767 ; AI error: 342.49011891940773 BCD5 error: 115.8\n",
            "\tIntensity Truth: 110.0, AI forecast: 91.41105890274048, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 23.626581346988676, 78.44255324304103 ; AI error: 402.6661764738856 BCD5 error: 139.4\n",
            "Found AL152017 at 2017-09-18 00:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 85.81501126289368, BCD5 forecast: -57.0\n",
            "\tTrajectory Truth: 15.3, 61.1; AI forecast: 16.37151334285736, 59.784502090886235 ; AI error: 99.56134635713637 BCD5 error: 57.3\n",
            "\tIntensity Truth: 150.0, AI forecast: 94.51170563697815, BCD5 forecast: -57.0\n",
            "\tTrajectory Truth: 17.0, 64.3; AI forecast: 18.241580259799957, 64.95171132832766 ; AI error: 83.3527172230858 BCD5 error: 85.5\n",
            "\tIntensity Truth: 95.0, AI forecast: 91.18155181407928, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 19.0, 67.6; AI forecast: 21.106457203626633, 69.48563450872898 ; AI error: 165.23896412973178 BCD5 error: 70.5\n",
            "\tIntensity Truth: 110.0, AI forecast: 88.63848626613617, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 20.8, 70.0; AI forecast: 23.697369349002837, 72.59167079329491 ; AI error: 225.82419400421577 BCD5 error: 105.9\n",
            "\tIntensity Truth: 105.0, AI forecast: 86.34054481983185, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 26.284411871433257, 74.97877091169357 ; AI error: 240.56961928169122 BCD5 error: 98.5\n",
            "Found AL152017 at 2017-09-18 06:00:00\n",
            "\tIntensity Truth: 135.0, AI forecast: 94.86096262931824, BCD5 forecast: -43.0\n",
            "\tTrajectory Truth: 15.7, 61.9; AI forecast: 17.299194142222404, 60.51159461289644 ; AI error: 124.92823114556231 BCD5 error: 40.4\n",
            "\tIntensity Truth: 140.0, AI forecast: 97.85456418991089, BCD5 forecast: -45.0\n",
            "\tTrajectory Truth: 17.6, 65.1; AI forecast: 19.574204051494597, 65.26506597548723 ; AI error: 118.90375174999065 BCD5 error: 45.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 94.490447640419, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 19.4, 68.2; AI forecast: 22.707495579123496, 68.76361970454454 ; AI error: 201.07811940389232 BCD5 error: 37.6\n",
            "\tIntensity Truth: 110.0, AI forecast: 90.92078149318695, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 21.2, 70.5; AI forecast: 25.918571972846983, 71.55682787299156 ; AI error: 289.2090574164714 BCD5 error: 71.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 85.04289746284485, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 28.62205240726471, 73.12816437780857 ; AI error: 261.9348712782691 BCD5 error: 62.2\n",
            "Found AL152017 at 2017-09-18 12:00:00\n",
            "\tIntensity Truth: 140.0, AI forecast: 98.26763689517975, BCD5 forecast: -31.0\n",
            "\tTrajectory Truth: 16.1, 62.7; AI forecast: 17.64421806037426, 60.69139351695776 ; AI error: 148.0328094233388 BCD5 error: 5.8\n",
            "\tIntensity Truth: 115.0, AI forecast: 99.15020883083344, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 18.2, 66.2; AI forecast: 19.764270129799844, 65.40288399159908 ; AI error: 104.25385213375175 BCD5 error: 22.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 93.04189562797546, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 19.9, 68.8; AI forecast: 23.10672651231289, 69.15653819441795 ; AI error: 193.56041112374712 BCD5 error: 66.8\n",
            "\tIntensity Truth: 110.0, AI forecast: 86.27775967121124, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 21.9, 70.9; AI forecast: 26.07124878168106, 71.36668996810913 ; AI error: 251.74792240355183 BCD5 error: 98.0\n",
            "\tIntensity Truth: 100.0, AI forecast: 83.59048187732697, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 28.18790737390518, 72.7572253793478 ; AI error: 188.7230360217092 BCD5 error: 66.9\n",
            "Found AL152017 at 2017-09-18 18:00:00\n",
            "\tIntensity Truth: 145.0, AI forecast: 104.72852408885956, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 16.6, 63.5; AI forecast: 17.687144401669503, 60.90611643642187 ; AI error: 162.50004764941127 BCD5 error: 18.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 104.26605105400085, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 18.6, 67.0; AI forecast: 19.968613915145397, 66.00306239575147 ; AI error: 99.72010873888937 BCD5 error: 57.8\n",
            "\tIntensity Truth: 105.0, AI forecast: 94.92582976818085, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 20.5, 69.5; AI forecast: 22.942493405938148, 69.34143283367158 ; AI error: 146.9148794302359 BCD5 error: 112.3\n",
            "\tIntensity Truth: 110.0, AI forecast: 90.42910635471344, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 25.350657773017883, 71.31328055858611 ; AI error: 153.26852895238264 BCD5 error: 139.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 82.02933132648468, BCD5 forecast: -22.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 27.569923520088196, 72.87232455611229 ; AI error: 104.8540752742569 BCD5 error: 118.3\n",
            "Found AL152017 at 2017-09-19 00:00:00\n",
            "\tIntensity Truth: 150.0, AI forecast: 110.65839946269989, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 17.0, 64.3; AI forecast: 18.484697911143304, 60.241658851504326 ; AI error: 248.59436333188563 BCD5 error: 24.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 102.91310787200928, BCD5 forecast: 20.0\n",
            "\tTrajectory Truth: 19.0, 67.6; AI forecast: 20.287027134746314, 64.77528697103261 ; AI error: 177.43131132690743 BCD5 error: 64.1\n",
            "\tIntensity Truth: 110.0, AI forecast: 93.57564866542816, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 20.8, 70.0; AI forecast: 22.43706563711166, 67.50074475407601 ; AI error: 170.64336600428865 BCD5 error: 139.3\n",
            "\tIntensity Truth: 105.0, AI forecast: 84.16161060333252, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 24.57230142354965, 69.28471501171589 ; AI error: 137.24087336561496 BCD5 error: 163.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 78.13521265983582, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 25.387474954128265, 71.54260390400887 ; AI error: 86.25999634862072 BCD5 error: 176.0\n",
            "Found AL152017 at 2017-09-19 06:00:00\n",
            "\tIntensity Truth: 140.0, AI forecast: 123.32056760787964, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 17.6, 65.1; AI forecast: 18.944074913859367, 61.84952440112829 ; AI error: 202.12068142224996 BCD5 error: 13.3\n",
            "\tIntensity Truth: 100.0, AI forecast: 116.82934761047363, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 19.4, 68.2; AI forecast: 21.054295118153096, 66.71242010593414 ; AI error: 129.95480550431878 BCD5 error: 54.3\n",
            "\tIntensity Truth: 110.0, AI forecast: 105.97134292125702, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 21.2, 70.5; AI forecast: 22.667836168408392, 71.12031753063202 ; AI error: 94.65889930946354 BCD5 error: 127.9\n",
            "\tIntensity Truth: 100.0, AI forecast: 91.03070974349976, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 24.792408430576323, 74.24955848753453 ; AI error: 130.41247627912566 BCD5 error: 121.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 77.84568667411804, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 28.519433987140655, 73.22326215803623 ; AI error: 69.55409765369421 BCD5 error: 126.1\n",
            "Found AL152017 at 2017-09-19 12:00:00\n",
            "\tIntensity Truth: 115.0, AI forecast: 120.68749666213989, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 18.2, 66.2; AI forecast: 19.710812255740166, 63.2687086969614 ; AI error: 189.55683688314315 BCD5 error: 12.9\n",
            "\tIntensity Truth: 100.0, AI forecast: 119.19443011283875, BCD5 forecast: 28.0\n",
            "\tTrajectory Truth: 19.9, 68.8; AI forecast: 20.923942898213863, 69.89420903027057 ; AI error: 87.00839806873675 BCD5 error: 72.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 101.67662680149078, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 21.9, 70.9; AI forecast: 23.388492327928542, 73.85572174787521 ; AI error: 186.5706770969203 BCD5 error: 139.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 86.99276685714722, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 27.5402402639389, 72.71999005377293 ; AI error: 150.26335683046472 BCD5 error: 141.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 79.02092427015305, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 31.78817001581192, 70.77615615427494 ; AI error: 228.97355405423886 BCD5 error: 134.6\n",
            "Found AL152017 at 2017-09-19 18:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 127.55630373954773, BCD5 forecast: 40.0\n",
            "\tTrajectory Truth: 18.6, 67.0; AI forecast: 18.649186739325522, 66.94346972703934 ; AI error: 4.366501464862849 BCD5 error: 24.8\n",
            "\tIntensity Truth: 105.0, AI forecast: 114.91515576839447, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 20.5, 69.5; AI forecast: 20.338670313358307, 73.34411332607269 ; AI error: 216.51151700238702 BCD5 error: 72.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 96.91565334796906, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 24.298177409172055, 73.4584748506546 ; AI error: 153.43316720495315 BCD5 error: 126.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 87.72661209106445, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 28.624293446540833, 72.33965997993946 ; AI error: 163.5815741768758 BCD5 error: 134.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 77.59465456008911, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 32.803125739097595, 70.64187123775483 ; AI error: 250.8846299361746 BCD5 error: 140.4\n",
            "Found AL152017 at 2017-09-20 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 114.28170084953308, BCD5 forecast: 42.0\n",
            "\tTrajectory Truth: 19.0, 67.6; AI forecast: 19.944677828252313, 67.18686442375183 ; AI error: 61.35084553782477 BCD5 error: 30.5\n",
            "\tIntensity Truth: 110.0, AI forecast: 102.10383117198944, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 20.8, 70.0; AI forecast: 23.712602257728577, 68.93180864155292 ; AI error: 184.66992335502218 BCD5 error: 96.0\n",
            "\tIntensity Truth: 105.0, AI forecast: 92.90685832500458, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 28.6814782500267, 68.356695997715 ; AI error: 346.34247965756913 BCD5 error: 121.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 83.52190136909485, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 33.81288666725159, 66.6161709100008 ; AI error: 526.643162636768 BCD5 error: 152.3\n",
            "\tIntensity Truth: 85.0, AI forecast: 75.1759722828865, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 38.76800625324249, 63.398957723379134 ; AI error: 719.4243662103063 BCD5 error: 165.9\n",
            "Found AL152017 at 2017-09-20 06:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 117.88171648979187, BCD5 forecast: 30.0\n",
            "\tTrajectory Truth: 19.4, 68.2; AI forecast: 20.702655290067195, 68.19943703114987 ; AI error: 78.21205404050116 BCD5 error: 60.9\n",
            "\tIntensity Truth: 110.0, AI forecast: 111.13109052181244, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 21.2, 70.5; AI forecast: 24.15934863090515, 71.93654578030109 ; AI error: 194.68297524439492 BCD5 error: 143.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 97.56983637809753, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 28.4637752532959, 72.06370005905629 ; AI error: 244.14958247572838 BCD5 error: 145.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 84.74475979804993, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 33.777288389205935, 69.00175841748714 ; AI error: 420.1485444417594 BCD5 error: 156.1\n",
            "\tIntensity Truth: 75.0, AI forecast: 71.41467124223709, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 40.68823444843292, 61.56397033855319 ; AI error: 832.8433607497914 BCD5 error: 188.5\n",
            "Found AL152017 at 2017-09-20 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 105.71779489517212, BCD5 forecast: 18.0\n",
            "\tTrajectory Truth: 19.9, 68.8; AI forecast: 21.495874390006065, 71.96477157473564 ; AI error: 201.92112670303734 BCD5 error: 62.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 105.54442644119263, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 21.9, 70.9; AI forecast: 26.35655846595764, 72.34177119135856 ; AI error: 278.9838101362884 BCD5 error: 107.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 91.06769323348999, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 33.56959271430969, 65.50096036046743 ; AI error: 614.3612903654243 BCD5 error: 105.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 82.80920565128326, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 41.45963711738587, 56.713542386423796 ; AI error: 1110.7549376279303 BCD5 error: 76.4\n",
            "\tIntensity Truth: 70.0, AI forecast: 53.73877592384815, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 50.39661035537719, 45.980622422695156 ; AI error: 1685.780137419872 BCD5 error: 92.3\n",
            "Found AL152017 at 2017-09-20 18:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 88.61804604530334, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 20.5, 69.5; AI forecast: 22.6556893825531, 71.15202131271363 ; AI error: 158.92722887950643 BCD5 error: 8.2\n",
            "\tIntensity Truth: 110.0, AI forecast: 83.59184205532074, BCD5 forecast: -27.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 29.000241303443907, 66.92623410224914 ; AI error: 437.91365310025424 BCD5 error: 28.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 79.24995988607407, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 36.88021154403687, 57.88103122580796 ; AI error: 988.1447907100556 BCD5 error: 21.6\n",
            "\tIntensity Truth: 90.0, AI forecast: 49.320175126194954, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 45.62874221801758, 46.42364106178283 ; AI error: 1593.717873762899 BCD5 error: 24.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 38.879579231143, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 52.428400850296015, 36.033754110336304 ; AI error: 2046.282729330231 BCD5 error: 25.6\n",
            "Found AL152017 at 2017-09-21 00:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 64.36421781778336, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 20.8, 70.0; AI forecast: 22.989663752913476, 68.94553999453782 ; AI error: 143.99354868901725 BCD5 error: 16.4\n",
            "\tIntensity Truth: 105.0, AI forecast: 68.9670142531395, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 29.008350801467895, 64.13137342333793 ; AI error: 512.629320446552 BCD5 error: 13.2\n",
            "\tIntensity Truth: 100.0, AI forecast: 40.588805601000786, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 36.73547375202179, 54.400994368270034 ; AI error: 1099.8264763429033 BCD5 error: 12.0\n",
            "\tIntensity Truth: 85.0, AI forecast: 34.79883298277855, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 43.77434380054474, 44.79609800428152 ; AI error: 1582.6097966890447 BCD5 error: 39.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 35.659784972667694, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 50.12292718887329, 37.30904281437397 ; AI error: 1923.4683064265143 BCD5 error: 62.3\n",
            "Found AL152017 at 2017-09-21 06:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 73.03055435419083, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 21.2, 70.5; AI forecast: 22.549184253811834, 69.89508319050074 ; AI error: 87.737310277965 BCD5 error: 24.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 44.75981681142002, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 28.023899948596952, 65.95388727933168 ; AI error: 387.10877577201103 BCD5 error: 12.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 39.670883640646935, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 34.356811833381656, 58.327190503850574 ; AI error: 841.2910893935458 BCD5 error: 18.8\n",
            "\tIntensity Truth: 75.0, AI forecast: 40.8520720154047, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 41.028352713584894, 51.428098579496144 ; AI error: 1224.4851932160852 BCD5 error: 33.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 39.424279406666756, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 47.14932942390442, 44.45713580399752 ; AI error: 1568.6980300874711 BCD5 error: 76.7\n",
            "Found AL152017 at 2017-09-21 12:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 36.99367731809616, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 21.9, 70.9; AI forecast: 22.24952316880226, 69.82966022491455 ; AI error: 63.141899730276286 BCD5 error: 30.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 39.07786168158054, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 26.16016111373901, 67.39035976827145 ; AI error: 262.7531247812448 BCD5 error: 20.2\n",
            "\tIntensity Truth: 95.0, AI forecast: 42.537398263812065, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 31.51776204109192, 62.19493809044361 ; AI error: 582.2346314176053 BCD5 error: 12.1\n",
            "\tIntensity Truth: 70.0, AI forecast: 42.88649892434478, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 37.38459558486939, 55.9615465783514 ; AI error: 932.9848030586257 BCD5 error: 52.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 38.90013001859188, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 43.78534171581268, 47.844674575328824 ; AI error: 1335.125889254595 BCD5 error: 111.0\n",
            "Found AL152017 at 2017-09-21 18:00:00\n",
            "\tIntensity Truth: 110.0, AI forecast: 79.59963619709015, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 22.8, 71.2; AI forecast: 22.746786990761755, 69.10252255350352 ; AI error: 116.15911754216465 BCD5 error: 22.9\n",
            "\tIntensity Truth: 100.0, AI forecast: 83.5904860496521, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 26.891537499427795, 69.00072231292725 ; AI error: 187.15059887320186 BCD5 error: 44.2\n",
            "\tIntensity Truth: 90.0, AI forecast: 77.17916190624237, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 31.77473429441452, 66.33541370630265 ; AI error: 375.7573370252085 BCD5 error: 37.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 68.63218933343887, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 37.716269397735594, 61.59456764236092 ; AI error: 682.8767224167094 BCD5 error: 30.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 63.44487100839615, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 43.73191125392914, 55.61416783025488 ; AI error: 1005.3460607212916 BCD5 error: 85.4\n",
            "Found AL152017 at 2017-09-22 00:00:00\n",
            "\tIntensity Truth: 105.0, AI forecast: 99.81661260128021, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 23.7, 71.6; AI forecast: 23.813570314645766, 69.21700617223978 ; AI error: 131.12821070432267 BCD5 error: 55.1\n",
            "\tIntensity Truth: 100.0, AI forecast: 93.61207723617554, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 27.560358548164366, 70.61084066927432 ; AI error: 111.67907864239609 BCD5 error: 84.5\n",
            "\tIntensity Truth: 85.0, AI forecast: 79.18946325778961, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 32.536807811260225, 68.4671912074089 ; AI error: 284.42280007163043 BCD5 error: 80.8\n",
            "\tIntensity Truth: 70.0, AI forecast: 69.41716015338898, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 37.95800724029541, 64.90303221940994 ; AI error: 538.6290281111617 BCD5 error: 46.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 61.411827355623245, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 42.96322727203369, 60.23361335545778 ; AI error: 787.296812828096 BCD5 error: 73.5\n",
            "Found AL152017 at 2017-09-22 06:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 102.7661669254303, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 24.4, 71.9; AI forecast: 24.247036802768704, 69.87994266748429 ; AI error: 110.89915191889523 BCD5 error: 36.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 93.27163636684418, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 28.488092482089996, 71.05581451058387 ; AI error: 101.0996244798156 BCD5 error: 42.0\n",
            "\tIntensity Truth: 75.0, AI forecast: 80.20366787910461, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 33.51796770095825, 68.86556408405303 ; AI error: 282.103757068096 BCD5 error: 40.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 69.42251116037369, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 38.463507294654846, 65.21652252674103 ; AI error: 521.4430148467633 BCD5 error: 108.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 59.57943081855774, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 44.128060007095335, 59.62907580845058 ; AI error: 825.6286901137971 BCD5 error: 178.0\n",
            "Found AL152017 at 2017-09-22 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 99.26468074321747, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 25.1, 72.1; AI forecast: 25.18671118021011, 69.88712672144175 ; AI error: 120.38423235497824 BCD5 error: 24.8\n",
            "\tIntensity Truth: 95.0, AI forecast: 92.57258415222168, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 29.572621059417724, 70.68980675935745 ; AI error: 131.29418224602338 BCD5 error: 69.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 79.0790468454361, BCD5 forecast: 17.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 34.12168896198273, 67.96822122633458 ; AI error: 323.5745511386484 BCD5 error: 173.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 66.84819281101227, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 39.8890380859375, 62.869901707768435 ; AI error: 631.4689790384058 BCD5 error: 275.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 57.436459958553314, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 46.268567943573, 55.64959654388949 ; AI error: 1012.3980271871909 BCD5 error: 366.8\n",
            "Found AL152017 at 2017-09-22 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 100.67231893539429, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 25.9, 72.3; AI forecast: 25.880976736545563, 70.48441386818885 ; AI error: 98.07336016038762 BCD5 error: 41.9\n",
            "\tIntensity Truth: 90.0, AI forecast: 92.31832265853882, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 29.54393140077591, 71.40231093764305 ; AI error: 82.80730295554908 BCD5 error: 114.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 76.52126967906952, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 34.57122762203217, 67.82950022220612 ; AI error: 326.5780069598722 BCD5 error: 244.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 64.29702252149582, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 40.96817510128021, 61.456045041233295 ; AI error: 697.7050474081695 BCD5 error: 358.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 54.514979124069214, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 47.49862813949585, 52.25657694265246 ; AI error: 1137.3990913780087 BCD5 error: 458.9\n",
            "Found AL152017 at 2017-09-23 00:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 97.88299024105072, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 26.6, 72.4; AI forecast: 26.099238491058347, 70.8889151930809 ; AI error: 86.68048098069676 BCD5 error: 31.9\n",
            "\tIntensity Truth: 85.0, AI forecast: 87.69585371017456, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 30.42319985628128, 70.78729894161225 ; AI error: 118.05712242825086 BCD5 error: 90.2\n",
            "\tIntensity Truth: 70.0, AI forecast: 72.8515887260437, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 36.34075193405151, 65.45998505353927 ; AI error: 460.1409526016967 BCD5 error: 213.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 60.74822634458542, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 43.207333827018736, 56.44033296226989 ; AI error: 935.3279836997573 BCD5 error: 324.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 51.19425527751446, BCD5 forecast: -13.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 49.9038731098175, 44.67157977223396 ; AI error: 1431.8453150663431 BCD5 error: 411.8\n",
            "Found AL152017 at 2017-09-23 06:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 91.24632716178894, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 27.5, 72.6; AI forecast: 28.01556589603424, 70.1312777236104 ; AI error: 134.76658085367913 BCD5 error: 21.3\n",
            "\tIntensity Truth: 75.0, AI forecast: 82.84523367881775, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 33.88881313800812, 67.3808254018426 ; AI error: 353.78766893648253 BCD5 error: 83.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 68.71335357427597, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 40.65307927131653, 58.32419283799827 ; AI error: 858.7556170862952 BCD5 error: 203.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 57.07373574376106, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 47.65893754959106, 46.02784403264522 ; AI error: 1423.003188940362 BCD5 error: 313.3\n",
            "\tIntensity Truth: 60.0, AI forecast: 49.17024679481983, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 53.49107403755188, 33.234625223279 ; AI error: 1869.6545421793137 BCD5 error: 381.1\n",
            "Found AL152017 at 2017-09-23 12:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 88.34944009780884, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 28.4, 72.8; AI forecast: 30.375109136104584, 68.86587864607573 ; AI error: 237.50571723219116 BCD5 error: 36.9\n",
            "\tIntensity Truth: 70.0, AI forecast: 78.48494529724121, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 36.99619433879852, 63.529483157396314 ; AI error: 600.4034931186839 BCD5 error: 123.3\n",
            "\tIntensity Truth: 65.0, AI forecast: 63.75842750072479, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 43.76776337623596, 51.92609477490186 ; AI error: 1171.1264417654795 BCD5 error: 250.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 53.68405170738697, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 49.950922918319705, 38.71433974802494 ; AI error: 1718.343666933454 BCD5 error: 375.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 47.326894253492355, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 55.06036028861999, 26.8780559182167 ; AI error: 2061.3444881289524 BCD5 error: 409.8\n",
            "Found AL152017 at 2017-09-23 18:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 85.96222758293152, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 29.1, 72.9; AI forecast: 31.833984398841856, 68.11313003003598 ; AI error: 297.11628348255203 BCD5 error: 47.5\n",
            "\tIntensity Truth: 70.0, AI forecast: 74.76450175046921, BCD5 forecast: 16.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 38.42815985679626, 61.41721401289105 ; AI error: 712.394141499086 BCD5 error: 150.6\n",
            "\tIntensity Truth: 65.0, AI forecast: 61.55563488602638, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 44.51065888404846, 49.08564295172691 ; AI error: 1279.0777680155688 BCD5 error: 284.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 52.848264053463936, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 50.297833395004275, 36.26863613128661 ; AI error: 1783.6866217522281 BCD5 error: 408.1\n",
            "\tIntensity Truth: 55.0, AI forecast: 46.018241345882416, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 55.24846339225769, 24.820162338018413 ; AI error: 2085.951683563033 BCD5 error: 420.2\n",
            "Found AL152017 at 2017-09-24 00:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 83.01475524902344, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 29.7, 72.9; AI forecast: 32.35016844272614, 68.21335322111845 ; AI error: 288.8485754338136 BCD5 error: 52.6\n",
            "\tIntensity Truth: 70.0, AI forecast: 72.48336851596832, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 38.21009712219238, 61.6108884088695 ; AI error: 675.5495989015538 BCD5 error: 175.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 60.14398813247681, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 43.99064764976501, 50.08074719235301 ; AI error: 1206.1668826525681 BCD5 error: 321.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 50.55068872869015, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 49.76534023284912, 37.58678836226463 ; AI error: 1688.535801922742 BCD5 error: 437.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 44.10407688468695, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 54.65724959373473, 26.254483765363688 ; AI error: 1964.764283631549 BCD5 error: 428.5\n",
            "Found AL152017 at 2017-09-24 06:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 83.24924409389496, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 30.3, 72.9; AI forecast: 32.493232214450835, 68.78778162896633 ; AI error: 248.47289519710813 BCD5 error: 72.8\n",
            "\tIntensity Truth: 65.0, AI forecast: 73.71480733156204, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 38.272619414329526, 62.84994355291128 ; AI error: 605.5380056622313 BCD5 error: 211.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 59.930361956357956, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 44.107659339904785, 51.46017161160707 ; AI error: 1131.797770848409 BCD5 error: 368.9\n",
            "\tIntensity Truth: 60.0, AI forecast: 50.162636414170265, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 49.83662090301513, 38.929311943054195 ; AI error: 1602.3153459079901 BCD5 error: 476.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 42.90637483820319, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 55.10221905708313, 26.388356614112848 ; AI error: 1891.5263009491043 BCD5 error: 453.5\n",
            "Found AL152017 at 2017-09-24 12:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 85.02716362476349, BCD5 forecast: 19.0\n",
            "\tTrajectory Truth: 30.8, 73.0; AI forecast: 33.51746301651001, 68.79143597483635 ; AI error: 268.9910331776987 BCD5 error: 77.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.74487316608429, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 39.42634909152984, 62.517373490333554 ; AI error: 629.4257388611882 BCD5 error: 202.9\n",
            "\tIntensity Truth: 65.0, AI forecast: 58.25476139783859, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 44.961360120773314, 50.84689097851515 ; AI error: 1153.714934957742 BCD5 error: 361.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 47.68315006047487, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 50.81788668632507, 37.06505630016326 ; AI error: 1636.8517480771566 BCD5 error: 432.0\n",
            "\tIntensity Truth: 50.0, AI forecast: 41.63115359842777, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 55.390045404434204, 24.68419693112373 ; AI error: 1868.5122409579135 BCD5 error: 399.7\n",
            "Found AL152017 at 2017-09-24 18:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 83.47474575042725, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 31.4, 73.1; AI forecast: 34.636145663261416, 68.52492478489876 ; AI error: 301.26823389808646 BCD5 error: 70.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.13307052850723, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 40.5296493768692, 61.537153714150186 ; AI error: 680.2927630110596 BCD5 error: 185.2\n",
            "\tIntensity Truth: 65.0, AI forecast: 57.548020631074905, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 46.44166870117188, 48.192269001901145 ; AI error: 1260.5077052028607 BCD5 error: 322.9\n",
            "\tIntensity Truth: 55.0, AI forecast: 48.61650727689266, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 51.69189801216125, 34.31171441674232 ; AI error: 1701.5133798137315 BCD5 error: 354.7\n",
            "\tIntensity Truth: 50.0, AI forecast: 41.86146829277277, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 56.1681366443634, 22.271256941556928 ; AI error: 1848.831086063322 BCD5 error: 312.0\n",
            "Found AL152017 at 2017-09-25 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 81.55654013156891, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 32.0, 73.1; AI forecast: 35.06698546409607, 68.65186031162739 ; AI error: 288.8471180221439 BCD5 error: 72.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 69.5627909898758, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 41.42519690990448, 60.34551646336913 ; AI error: 731.7124546071036 BCD5 error: 187.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 57.196974754333496, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 46.825475215911865, 46.72860343158245 ; AI error: 1287.988448954159 BCD5 error: 313.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 47.533953972160816, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 51.90057601928711, 32.97838520407676 ; AI error: 1684.9756740390917 BCD5 error: 301.8\n",
            "\tIntensity Truth: 50.0, AI forecast: 41.38857617974281, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 56.33645491600036, 21.251217937469477 ; AI error: 1747.6457868181851 BCD5 error: 302.8\n",
            "Found AL152017 at 2017-09-25 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 76.65961980819702, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 32.6, 73.1; AI forecast: 36.119620537757875, 67.8391575306654 ; AI error: 335.5281200169886 BCD5 error: 58.4\n",
            "\tIntensity Truth: 65.0, AI forecast: 69.72776472568512, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 42.26551604270935, 58.41968318484723 ; AI error: 809.0978534232006 BCD5 error: 167.1\n",
            "\tIntensity Truth: 60.0, AI forecast: 57.5908787548542, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 47.43382425308228, 44.08883709162473 ; AI error: 1363.4435645580113 BCD5 error: 255.5\n",
            "\tIntensity Truth: 50.0, AI forecast: 48.884863667190075, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 52.46607193946838, 30.168778264522548 ; AI error: 1713.0998450125617 BCD5 error: 217.1\n",
            "\tIntensity Truth: 50.0, AI forecast: 41.99243940412998, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 56.77307906150817, 18.778234755992884 ; AI error: 1684.5121759362194 BCD5 error: 364.1\n",
            "Found AL152017 at 2017-09-25 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 76.80574715137482, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 33.3, 73.1; AI forecast: 36.04890162944794, 68.40273651629687 ; AI error: 284.6051563897626 BCD5 error: 45.1\n",
            "\tIntensity Truth: 65.0, AI forecast: 68.69134664535522, BCD5 forecast: -7.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 41.690778088569644, 59.17395882569253 ; AI error: 741.7080909966207 BCD5 error: 150.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 57.53392964601517, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 46.77939872741699, 44.932719667255874 ; AI error: 1264.2890636471016 BCD5 error: 197.7\n",
            "\tIntensity Truth: 50.0, AI forecast: 48.157823234796524, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 51.62059030532836, 30.971543937921517 ; AI error: 1577.6492099269465 BCD5 error: 212.5\n",
            "\tIntensity Truth: 50.0, AI forecast: 43.36109399795532, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 55.53401565551758, 20.160618686676017 ; AI error: 1467.0787084217009 BCD5 error: 482.5\n",
            "Found AL152017 at 2017-09-25 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 72.27020859718323, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 33.9, 73.1; AI forecast: 35.4135684967041, 68.52943633347749 ; AI error: 243.30273658188887 BCD5 error: 56.0\n",
            "\tIntensity Truth: 65.0, AI forecast: 65.52037954330444, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 40.57306125164032, 59.53121225647628 ; AI error: 673.4258140112341 BCD5 error: 175.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 54.12683501839638, BCD5 forecast: 6.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 45.03571546077728, 45.55236700177193 ; AI error: 1151.2649160560873 BCD5 error: 208.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 47.96554893255234, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 49.27379260063171, 32.53328418135642 ; AI error: 1347.2999958134005 BCD5 error: 255.8\n",
            "Found AL152017 at 2017-09-26 00:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 69.94800299406052, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 34.4, 73.0; AI forecast: 35.45156252384186, 68.45383461564779 ; AI error: 232.5029062196376 BCD5 error: 66.7\n",
            "\tIntensity Truth: 65.0, AI forecast: 61.11633390188217, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 39.91258852481842, 59.4954479958862 ; AI error: 630.460306823053 BCD5 error: 179.1\n",
            "\tIntensity Truth: 55.0, AI forecast: 52.92860843241215, BCD5 forecast: 8.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 43.58713290691375, 46.53775076717138 ; AI error: 1008.2891208411199 BCD5 error: 192.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 47.31842704117298, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 47.738209056854245, 34.0226191997528 ; AI error: 1105.6037615216808 BCD5 error: 311.7\n",
            "Found AL152017 at 2017-09-26 06:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 64.78212624788284, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 34.9, 72.9; AI forecast: 35.58618710041046, 68.3202678501606 ; AI error: 228.29545547174627 BCD5 error: 70.5\n",
            "\tIntensity Truth: 60.0, AI forecast: 61.35409697890282, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 39.508031368255615, 60.02343293204903 ; AI error: 558.3477788720563 BCD5 error: 170.0\n",
            "\tIntensity Truth: 50.0, AI forecast: 54.77997601032257, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 43.152016806602475, 47.46379422992467 ; AI error: 867.4434898619156 BCD5 error: 195.6\n",
            "\tIntensity Truth: 50.0, AI forecast: 48.30751869827509, BCD5 forecast: 9.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 47.320180130004886, 34.882965832948685 ; AI error: 901.218062087886 BCD5 error: 386.2\n",
            "Found AL152017 at 2017-09-26 12:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 68.51137965917587, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 35.4, 72.8; AI forecast: 36.015684080123904, 68.82788531929255 ; AI error: 197.13415155165234 BCD5 error: 76.0\n",
            "\tIntensity Truth: 60.0, AI forecast: 64.2547881603241, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 40.060901165008545, 61.1719573803246 ; AI error: 458.76820790771177 BCD5 error: 155.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 54.887083768844604, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 43.61995992660522, 48.773404109477994 ; AI error: 722.993661614352 BCD5 error: 230.2\n",
            "\tIntensity Truth: 50.0, AI forecast: 46.39946447685361, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 47.48710751533508, 35.860816219449035 ; AI error: 717.4623179000548 BCD5 error: 489.8\n",
            "Found AL152017 at 2017-09-26 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 70.37713706493378, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 36.0, 72.6; AI forecast: 36.96202750205994, 68.54001019597054 ; AI error: 204.31323990920308 BCD5 error: 62.9\n",
            "\tIntensity Truth: 55.0, AI forecast: 64.20454502105713, BCD5 forecast: 11.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 41.048419928550715, 60.46311645060777 ; AI error: 457.4163765911912 BCD5 error: 131.6\n",
            "\tIntensity Truth: 50.0, AI forecast: 53.31517644226551, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 44.299733829498294, 47.57362385690212 ; AI error: 662.9301610541895 BCD5 error: 277.6\n",
            "Found AL152017 at 2017-09-27 00:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 68.47571045160294, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 36.6, 72.2; AI forecast: 37.47947626113891, 68.08049273639918 ; AI error: 204.34851322365625 BCD5 error: 48.5\n",
            "\tIntensity Truth: 55.0, AI forecast: 60.513146072626114, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 41.2137371301651, 59.56937643028795 ; AI error: 424.858332595269 BCD5 error: 136.1\n",
            "\tIntensity Truth: 50.0, AI forecast: 52.59398274123669, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 44.289320206642145, 46.689323028922075 ; AI error: 547.2650727704906 BCD5 error: 379.2\n",
            "Found AL152017 at 2017-09-27 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 64.8799380660057, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 36.7, 71.3; AI forecast: 37.638992595672605, 67.81426897644997 ; AI error: 176.02706015928928 BCD5 error: 33.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 60.65353021025658, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 41.37454371452331, 59.332624630630015 ; AI error: 359.1195236120986 BCD5 error: 198.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 51.662755236029625, BCD5 forecast: 12.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 44.0662902355194, 46.81441696882248 ; AI error: 383.75424390885405 BCD5 error: 487.7\n",
            "Found AL152017 at 2017-09-27 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 68.09310615062714, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 36.8, 70.0; AI forecast: 38.377539706230166, 67.63937333971262 ; AI error: 146.9080413712409 BCD5 error: 71.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 61.481977701187134, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 41.82011699676514, 59.390127373859286 ; AI error: 313.66215982556486 BCD5 error: 285.9\n",
            "\tIntensity Truth: 50.0, AI forecast: 51.822247579693794, BCD5 forecast: 13.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 44.47187328338623, 46.9594156473875 ; AI error: 280.66183280410235 BCD5 error: 577.1\n",
            "Found AL152017 at 2017-09-27 18:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 67.08144456148148, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 36.8, 68.6; AI forecast: 38.143903851509094, 67.83535565435886 ; AI error: 88.53292021966807 BCD5 error: 125.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 61.04985103011131, BCD5 forecast: 15.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 41.36750516891479, 60.00355320572853 ; AI error: 242.71516876087526 BCD5 error: 356.4\n",
            "Found AL152017 at 2017-09-28 00:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 65.92296928167343, BCD5 forecast: 10.0\n",
            "\tTrajectory Truth: 36.9, 66.8; AI forecast: 38.33491790294647, 67.45720801353454 ; AI error: 91.64681418945338 BCD5 error: 122.4\n",
            "\tIntensity Truth: 50.0, AI forecast: 59.52649652957916, BCD5 forecast: 14.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 41.36472189426422, 59.622210343927144 ; AI error: 269.96159436555894 BCD5 error: 398.4\n",
            "Found AL152017 at 2017-09-28 06:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 63.80424380302429, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 37.0, 64.6; AI forecast: 38.562019896507266, 66.93142140805722 ; AI error: 145.0263910240552 BCD5 error: 91.3\n",
            "\tIntensity Truth: 50.0, AI forecast: 59.06897082924843, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 41.62041215896606, 58.998787629231806 ; AI error: 345.7629685401043 BCD5 error: 427.9\n",
            "Found AL152017 at 2017-09-28 12:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 62.194394916296005, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 37.0, 62.0; AI forecast: 38.68499166965485, 65.89855743050575 ; AI error: 210.69197439407148 BCD5 error: 111.0\n",
            "\tIntensity Truth: 50.0, AI forecast: 57.2358962893486, BCD5 forecast: 5.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 41.814889907836914, 57.75436256807297 ; AI error: 420.452231355551 BCD5 error: 469.4\n",
            "Found AL152017 at 2017-09-28 18:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 58.69463086128235, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 37.4, 59.0; AI forecast: 38.71675975322724, 64.34479400515556 ; AI error: 264.7056087220786 BCD5 error: 143.3\n",
            "Found AL152017 at 2017-09-29 00:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 56.01097255945206, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 38.1, 55.6; AI forecast: 38.34709191322327, 62.41772091984748 ; AI error: 321.8467017768 BCD5 error: 151.8\n",
            "Found AL152017 at 2017-09-29 06:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 49.26013484597206, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 39.1, 52.2; AI forecast: 36.638460183143614, 60.437633111327884 ; AI error: 417.2753877370716 BCD5 error: 162.4\n",
            "Found AL152017 at 2017-09-29 12:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 50.87420977652073, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 40.0, 48.8; AI forecast: 37.14723317623138, 56.96151280533522 ; AI error: 419.4281033496694 BCD5 error: 146.9\n",
            "Found AL152017 at 2017-09-29 18:00:00\n",
            "Found AL152017 at 2017-09-30 00:00:00\n",
            "Found AL152017 at 2017-09-30 06:00:00\n",
            "Found AL152017 at 2017-09-30 12:00:00\n",
            "Found AL162017 at 2017-10-05 06:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 48.077420964837074, BCD5 forecast: -11.0\n",
            "\tTrajectory Truth: 16.3, 84.7; AI forecast: 14.691255843639373, 81.72900738120079 ; AI error: 197.16721297372956 BCD5 error: 54.3\n",
            "\tIntensity Truth: 70.0, AI forecast: 55.92664361000061, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 23.5, 86.5; AI forecast: 15.161486566066742, 84.3447695016861 ; AI error: 515.2886154594181 BCD5 error: 374.1\n",
            "\tIntensity Truth: 60.0, AI forecast: 65.37254571914673, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 30.5, 88.9; AI forecast: 16.92103660106659, 83.87515129446983 ; AI error: 860.4780129544538 BCD5 error: 684.3\n",
            "Found AL162017 at 2017-10-05 12:00:00\n",
            "\tIntensity Truth: 40.0, AI forecast: 50.30213341116905, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 17.9, 84.7; AI forecast: 15.114870846271515, 82.23823327124119 ; AI error: 219.18067792082027 BCD5 error: 98.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 62.429824620485306, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 25.7, 87.9; AI forecast: 15.824347531795501, 84.87928753495217 ; AI error: 616.6307594616185 BCD5 error: 455.2\n",
            "\tIntensity Truth: 35.0, AI forecast: 66.3300108909607, BCD5 forecast: 7.0\n",
            "\tTrajectory Truth: 32.2, 88.0; AI forecast: 17.797300419211386, 83.99793731570244 ; AI error: 891.5114731036085 BCD5 error: 727.0\n",
            "Found AL162017 at 2017-10-05 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 55.84880366921425, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 19.5, 85.2; AI forecast: 15.853519344329833, 82.58068872094154 ; AI error: 265.28242568086876 BCD5 error: 156.9\n",
            "\tIntensity Truth: 80.0, AI forecast: 63.13575804233551, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 27.6, 88.9; AI forecast: 17.025706803798673, 84.78175126314163 ; AI error: 674.668944291451 BCD5 error: 538.5\n",
            "Found AL162017 at 2017-10-06 00:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 51.08308106660843, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 21.3, 85.9; AI forecast: 16.8362496137619, 82.39349294602872 ; AI error: 333.7541279632672 BCD5 error: 182.2\n",
            "\tIntensity Truth: 75.0, AI forecast: 63.4056094288826, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 29.1, 89.2; AI forecast: 18.807560774683953, 83.98980618417264 ; AI error: 680.6217614815895 BCD5 error: 548.2\n",
            "Found AL162017 at 2017-10-06 06:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 59.65377330780029, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 23.5, 86.5; AI forecast: 18.550135657191277, 82.00907780528068 ; AI error: 389.3684871417508 BCD5 error: 216.8\n",
            "\tIntensity Truth: 60.0, AI forecast: 65.18508315086365, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 30.5, 88.9; AI forecast: 21.644278091192245, 82.4428885102272 ; AI error: 635.2447952935029 BCD5 error: 516.8\n",
            "Found AL162017 at 2017-10-06 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 54.77961927652359, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 25.7, 87.9; AI forecast: 20.572564377635718, 80.99011390805245 ; AI error: 490.04377232880637 BCD5 error: 275.0\n",
            "\tIntensity Truth: 35.0, AI forecast: 60.924015790224075, BCD5 forecast: 22.0\n",
            "\tTrajectory Truth: 32.2, 88.0; AI forecast: 23.777246928215025, 80.78071395456791 ; AI error: 633.8045233035572 BCD5 error: 480.5\n",
            "Found AL162017 at 2017-10-06 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 57.6962111890316, BCD5 forecast: -23.0\n",
            "\tTrajectory Truth: 27.6, 88.9; AI forecast: 21.860913515090942, 80.93681472241879 ; AI error: 554.0822941271718 BCD5 error: 273.0\n",
            "Found AL162017 at 2017-10-07 00:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 63.0657359957695, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 29.1, 89.2; AI forecast: 25.08820308446884, 79.39617811441421 ; AI error: 576.4428269508347 BCD5 error: 249.7\n",
            "Found AL162017 at 2017-10-07 06:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 70.4786205291748, BCD5 forecast: 30.0\n",
            "\tTrajectory Truth: 30.5, 88.9; AI forecast: 28.502744102478026, 77.71693920493126 ; AI error: 596.2793427544004 BCD5 error: 73.4\n",
            "Found AL162017 at 2017-10-07 12:00:00\n",
            "\tIntensity Truth: 35.0, AI forecast: 69.28920954465866, BCD5 forecast: 21.0\n",
            "\tTrajectory Truth: 32.2, 88.0; AI forecast: 32.36162673234939, 75.10881983935833 ; AI error: 654.0413712284419 BCD5 error: 66.0\n",
            "Found AL162017 at 2017-10-07 18:00:00\n",
            "Found AL162017 at 2017-10-08 00:00:00\n",
            "Found AL162017 at 2017-10-08 06:00:00\n",
            "Found AL162017 at 2017-10-08 12:00:00\n",
            "Found AL172017 at 2017-10-09 06:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 46.77815292030573, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.9, 38.8; AI forecast: 28.44184325933456, 42.55135627835989 ; AI error: 284.6042419494417 BCD5 error: 59.7\n",
            "\tIntensity Truth: 55.0, AI forecast: 48.14462721347809, BCD5 forecast: -9.0\n",
            "\tTrajectory Truth: 30.4, 37.2; AI forecast: 28.41295232772827, 41.76935424208641 ; AI error: 267.0867053028471 BCD5 error: 272.3\n",
            "\tIntensity Truth: 75.0, AI forecast: 49.831535294651985, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 30.2, 35.7; AI forecast: 27.77628012895584, 42.58983035087585 ; AI error: 389.9142833322331 BCD5 error: 411.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 49.6572744846344, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 30.9, 34.4; AI forecast: 27.657587659358978, 42.911975777149195 ; AI error: 486.2339055108602 BCD5 error: 504.4\n",
            "\tIntensity Truth: 95.0, AI forecast: 46.988384779542685, BCD5 forecast: -43.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 27.690029406547545, 43.32349666804075 ; AI error: 782.5207792881555 BCD5 error: 494.6\n",
            "Found AL172017 at 2017-10-09 12:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 40.87022319436073, BCD5 forecast: -6.0\n",
            "\tTrajectory Truth: 31.6, 38.5; AI forecast: 28.076122021675108, 42.47438524067401 ; AI error: 295.9361835354128 BCD5 error: 97.2\n",
            "\tIntensity Truth: 60.0, AI forecast: 43.43805331736803, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 30.0, 36.7; AI forecast: 27.78820855617523, 42.41568338871002 ; AI error: 328.4321586747116 BCD5 error: 318.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 45.87548902258277, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 30.4, 35.7; AI forecast: 26.932845020294188, 43.44876696616411 ; AI error: 458.0592781553968 BCD5 error: 420.0\n",
            "\tIntensity Truth: 80.0, AI forecast: 45.72221552953124, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 31.4, 33.4; AI forecast: 26.17969059944153, 44.1013373285532 ; AI error: 643.9564913167014 BCD5 error: 492.6\n",
            "\tIntensity Truth: 100.0, AI forecast: 43.69945613667369, BCD5 forecast: -48.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 25.692592751979827, 44.07596378922462 ; AI error: 991.3757533337318 BCD5 error: 481.2\n",
            "Found AL172017 at 2017-10-09 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 43.502287957817316, BCD5 forecast: 2.0\n",
            "\tTrajectory Truth: 31.3, 38.2; AI forecast: 28.688396632671356, 42.342208144068714 ; AI error: 266.3831382475724 BCD5 error: 135.5\n",
            "\tIntensity Truth: 65.0, AI forecast: 45.2007150137797, BCD5 forecast: -12.0\n",
            "\tTrajectory Truth: 29.8, 36.2; AI forecast: 28.497470450401305, 42.081774529814716 ; AI error: 318.13855523513377 BCD5 error: 349.8\n",
            "\tIntensity Truth: 85.0, AI forecast: 45.714443791657686, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 30.5, 35.6; AI forecast: 27.078632736206053, 43.167219541966915 ; AI error: 447.8870128599403 BCD5 error: 431.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 44.936610483564436, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 32.0, 32.5; AI forecast: 25.799036109447478, 43.24877928495407 ; AI error: 676.0294389620851 BCD5 error: 468.0\n",
            "\tIntensity Truth: 100.0, AI forecast: 45.59552600607276, BCD5 forecast: -47.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 25.185197126865386, 41.95077089369296 ; AI error: 1057.5255395656484 BCD5 error: 453.2\n",
            "Found AL172017 at 2017-10-10 00:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 44.30436100810766, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 30.9, 37.8; AI forecast: 30.025259172916414, 41.1598941385746 ; AI error: 181.6319691720075 BCD5 error: 121.0\n",
            "\tIntensity Truth: 70.0, AI forecast: 45.216970392502844, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 29.9, 35.8; AI forecast: 29.69692288637161, 40.5246077299118 ; AI error: 246.4451426581607 BCD5 error: 282.2\n",
            "\tIntensity Truth: 90.0, AI forecast: 45.56957362219691, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 30.5, 35.1; AI forecast: 27.975668036937712, 40.946381986141205 ; AI error: 341.67473674681304 BCD5 error: 366.0\n",
            "\tIntensity Truth: 85.0, AI forecast: 47.33166478574276, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 32.6, 31.5; AI forecast: 26.780791556835172, 39.83299013078212 ; AI error: 557.2921248808666 BCD5 error: 392.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 44.009065348654985, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 26.13798503875732, 38.146027070283886 ; AI error: 1005.7176996204967 BCD5 error: 496.8\n",
            "Found AL172017 at 2017-10-10 06:00:00\n",
            "\tIntensity Truth: 55.0, AI forecast: 47.471120320260525, BCD5 forecast: -5.0\n",
            "\tTrajectory Truth: 30.4, 37.2; AI forecast: 31.11635390520096, 40.42258449792861 ; AI error: 171.7337161617098 BCD5 error: 91.4\n",
            "\tIntensity Truth: 75.0, AI forecast: 47.96385157853365, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 30.2, 35.7; AI forecast: 30.849907553195955, 39.579592192172996 ; AI error: 204.3971300825866 BCD5 error: 192.2\n",
            "\tIntensity Truth: 90.0, AI forecast: 49.801595732569695, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 30.9, 34.4; AI forecast: 29.52411127090454, 39.104376307129854 ; AI error: 257.6596883512287 BCD5 error: 283.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 46.83184748515487, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 28.504635167121886, 37.69848755598068 ; AI error: 505.685952805352 BCD5 error: 386.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 46.430536564439535, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 27.915826129913327, 34.869654178619385 ; AI error: 965.4357247130081 BCD5 error: 685.7\n",
            "Found AL172017 at 2017-10-10 12:00:00\n",
            "\tIntensity Truth: 60.0, AI forecast: 49.115649573504925, BCD5 forecast: -10.0\n",
            "\tTrajectory Truth: 30.0, 36.7; AI forecast: 31.597858166694643, 39.922246044874186 ; AI error: 191.87017945969123 BCD5 error: 88.6\n",
            "\tIntensity Truth: 80.0, AI forecast: 52.92756587266922, BCD5 forecast: -26.0\n",
            "\tTrajectory Truth: 30.4, 35.7; AI forecast: 31.96277881860733, 38.42278569042682 ; AI error: 168.40845248841347 BCD5 error: 143.5\n",
            "\tIntensity Truth: 80.0, AI forecast: 50.78226059675217, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 31.4, 33.4; AI forecast: 31.102768731117248, 37.750411486625666 ; AI error: 223.9974360709119 BCD5 error: 262.5\n",
            "\tIntensity Truth: 100.0, AI forecast: 50.97604110836983, BCD5 forecast: -41.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 30.399704241752623, 35.31854690313339 ; AI error: 448.7429627838546 BCD5 error: 450.9\n",
            "\tIntensity Truth: 85.0, AI forecast: 46.58783895894885, BCD5 forecast: -33.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 30.53752290010452, 32.098062610626215 ; AI error: 927.5318431303638 BCD5 error: 833.1\n",
            "Found AL172017 at 2017-10-10 18:00:00\n",
            "\tIntensity Truth: 65.0, AI forecast: 53.422144427895546, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 29.8, 36.2; AI forecast: 31.967405092716216, 38.74746796190738 ; AI error: 184.82337521863946 BCD5 error: 78.5\n",
            "\tIntensity Truth: 85.0, AI forecast: 53.384258672595024, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 30.5, 35.6; AI forecast: 32.60910986661911, 37.04795463979244 ; AI error: 146.70500767367213 BCD5 error: 106.1\n",
            "\tIntensity Truth: 80.0, AI forecast: 54.84515190124512, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 32.0, 32.5; AI forecast: 31.95875786542892, 35.609604105353355 ; AI error: 158.38187288683665 BCD5 error: 255.7\n",
            "\tIntensity Truth: 100.0, AI forecast: 51.128075420856476, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 31.938761246204376, 32.89415719509124 ; AI error: 434.22024692988526 BCD5 error: 538.4\n",
            "\tIntensity Truth: 80.0, AI forecast: 47.57673777639866, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 32.563281214237215, 30.21183341145515 ; AI error: 981.6680053958264 BCD5 error: 977.9\n",
            "Found AL172017 at 2017-10-11 00:00:00\n",
            "\tIntensity Truth: 70.0, AI forecast: 51.76818050444126, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 29.9, 35.8; AI forecast: 31.416708743572237, 38.622282451391214 ; AI error: 171.86229846199655 BCD5 error: 36.4\n",
            "\tIntensity Truth: 90.0, AI forecast: 55.15303194522858, BCD5 forecast: -35.0\n",
            "\tTrajectory Truth: 30.5, 35.1; AI forecast: 31.721947455406188, 36.894199097156516 ; AI error: 117.84817417688123 BCD5 error: 61.6\n",
            "\tIntensity Truth: 85.0, AI forecast: 51.92094497382641, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 32.6, 31.5; AI forecast: 31.357290410995482, 35.58353530764579 ; AI error: 220.92998357612157 BCD5 error: 243.8\n",
            "\tIntensity Truth: 95.0, AI forecast: 48.99465762078762, BCD5 forecast: -38.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 31.489315712451933, 33.496540924906725 ; AI error: 617.0827593215137 BCD5 error: 636.9\n",
            "Found AL172017 at 2017-10-11 06:00:00\n",
            "\tIntensity Truth: 75.0, AI forecast: 58.82547706365585, BCD5 forecast: -14.0\n",
            "\tTrajectory Truth: 30.2, 35.7; AI forecast: 30.57386093139648, 38.35813005268574 ; AI error: 139.48595108562716 BCD5 error: 42.3\n",
            "\tIntensity Truth: 90.0, AI forecast: 57.13639050722122, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 30.9, 34.4; AI forecast: 31.19125748872757, 36.734001392126075 ; AI error: 121.32564357488064 BCD5 error: 56.9\n",
            "\tIntensity Truth: 95.0, AI forecast: 53.9168544113636, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 31.13936285972595, 36.09515051245689 ; AI error: 351.8048866828568 BCD5 error: 299.7\n",
            "\tIntensity Truth: 90.0, AI forecast: 53.05691055953503, BCD5 forecast: -34.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 31.607983398437497, 34.28223885297775 ; AI error: 806.8125650529098 BCD5 error: 746.9\n",
            "Found AL172017 at 2017-10-11 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 57.181004136800766, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 30.4, 35.7; AI forecast: 29.84476764202118, 37.70431585609913 ; AI error: 109.29477633953184 BCD5 error: 52.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 56.37245237827301, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 31.4, 33.4; AI forecast: 30.420341479778287, 36.47114963531494 ; AI error: 168.77660910464223 BCD5 error: 77.9\n",
            "\tIntensity Truth: 100.0, AI forecast: 55.634393095970154, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 30.435855269432068, 36.030427044630045 ; AI error: 479.06935509363313 BCD5 error: 364.4\n",
            "\tIntensity Truth: 85.0, AI forecast: 52.66153998672962, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 30.95920853614807, 33.98611135184764 ; AI error: 986.6563002422095 BCD5 error: 851.3\n",
            "Found AL172017 at 2017-10-11 18:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 61.15055426955223, BCD5 forecast: -18.0\n",
            "\tTrajectory Truth: 30.5, 35.6; AI forecast: 29.512537324428557, 37.02123467326164 ; AI error: 94.73709730539768 BCD5 error: 43.1\n",
            "\tIntensity Truth: 80.0, AI forecast: 62.34692886471748, BCD5 forecast: -17.0\n",
            "\tTrajectory Truth: 32.0, 32.5; AI forecast: 30.095981085300444, 35.60126778781414 ; AI error: 196.240949085611 BCD5 error: 71.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 58.28261584043503, BCD5 forecast: -42.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 30.071973276138305, 34.73619678914547 ; AI error: 574.655692191487 BCD5 error: 416.1\n",
            "\tIntensity Truth: 80.0, AI forecast: 54.49852764606476, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 30.631442272663115, 33.11838989257812 ; AI error: 1168.839314465054 BCD5 error: 919.3\n",
            "Found AL172017 at 2017-10-12 00:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 68.16096693277359, BCD5 forecast: -20.0\n",
            "\tTrajectory Truth: 30.5, 35.1; AI forecast: 29.30645784139633, 36.28062340319156 ; AI error: 94.39728132921739 BCD5 error: 50.4\n",
            "\tIntensity Truth: 85.0, AI forecast: 65.39958447217941, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 32.6, 31.5; AI forecast: 29.611656749248503, 34.23524786531925 ; AI error: 227.93320242979075 BCD5 error: 127.7\n",
            "\tIntensity Truth: 95.0, AI forecast: 59.41215083003044, BCD5 forecast: -37.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 29.4002645611763, 33.729645797610274 ; AI error: 700.1668156371483 BCD5 error: 514.4\n",
            "Found AL172017 at 2017-10-12 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 69.77138638496399, BCD5 forecast: -16.0\n",
            "\tTrajectory Truth: 30.9, 34.4; AI forecast: 28.98981190919876, 35.413561591505996 ; AI error: 126.22796822682872 BCD5 error: 71.0\n",
            "\tIntensity Truth: 95.0, AI forecast: 65.7277524471283, BCD5 forecast: -29.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 29.047360205650328, 33.730507239699364 ; AI error: 333.27497436498027 BCD5 error: 238.2\n",
            "\tIntensity Truth: 90.0, AI forecast: 58.207098841667175, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 28.660696065425874, 33.630898758769035 ; AI error: 886.8855122352143 BCD5 error: 679.1\n",
            "Found AL172017 at 2017-10-12 12:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 71.96466088294983, BCD5 forecast: -2.0\n",
            "\tTrajectory Truth: 31.4, 33.4; AI forecast: 29.21208034753799, 35.238283562660214 ; AI error: 162.277126326455 BCD5 error: 103.8\n",
            "\tIntensity Truth: 100.0, AI forecast: 66.04007810354233, BCD5 forecast: -31.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 29.36327689886093, 33.81550681591034 ; AI error: 426.148750153357 BCD5 error: 348.5\n",
            "\tIntensity Truth: 85.0, AI forecast: 57.48993143439293, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 29.015290963649747, 34.328331092 ; AI error: 1075.1049749236588 BCD5 error: 847.0\n",
            "Found AL172017 at 2017-10-12 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 73.64532351493835, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 32.0, 32.5; AI forecast: 29.87763822078705, 35.05533055961132 ; AI error: 183.16612538071553 BCD5 error: 106.4\n",
            "\tIntensity Truth: 100.0, AI forecast: 66.46209418773651, BCD5 forecast: -30.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 30.35694274902344, 34.19665424227714 ; AI error: 541.9331975530471 BCD5 error: 429.7\n",
            "\tIntensity Truth: 80.0, AI forecast: 57.414557337760925, BCD5 forecast: -19.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 29.941523611545563, 34.74607793688774 ; AI error: 1257.9309855158706 BCD5 error: 975.9\n",
            "Found AL172017 at 2017-10-13 00:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 75.14783412218094, BCD5 forecast: -1.0\n",
            "\tTrajectory Truth: 32.6, 31.5; AI forecast: 30.817987763881682, 35.281158959865564 ; AI error: 220.76744001955245 BCD5 error: 79.6\n",
            "\tIntensity Truth: 95.0, AI forecast: 66.85402154922485, BCD5 forecast: -25.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 31.353670501708983, 34.601318359375 ; AI error: 669.8793013028732 BCD5 error: 430.8\n",
            "Found AL172017 at 2017-10-13 06:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 75.68367958068848, BCD5 forecast: -15.0\n",
            "\tTrajectory Truth: 33.4, 29.7; AI forecast: 31.06720349788666, 35.60218010246753 ; AI error: 330.78740273878316 BCD5 error: 75.1\n",
            "\tIntensity Truth: 90.0, AI forecast: 67.47702896595001, BCD5 forecast: -24.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 31.475922799110414, 34.766530403494826 ; AI error: 832.0533051284618 BCD5 error: 417.9\n",
            "Found AL172017 at 2017-10-13 12:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 73.64966690540314, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 34.2, 27.7; AI forecast: 30.868672502040862, 35.52255266904831 ; AI error: 443.4239382878357 BCD5 error: 99.2\n",
            "\tIntensity Truth: 85.0, AI forecast: 67.41834938526154, BCD5 forecast: -32.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 31.476405704021452, 34.326811698079105 ; AI error: 982.1281038227596 BCD5 error: 469.9\n",
            "Found AL172017 at 2017-10-13 18:00:00\n",
            "\tIntensity Truth: 100.0, AI forecast: 71.3774499297142, BCD5 forecast: -36.0\n",
            "\tTrajectory Truth: 35.3, 25.2; AI forecast: 30.896002817153928, 34.57728785574436 ; AI error: 540.3544349830942 BCD5 error: 178.3\n",
            "\tIntensity Truth: 80.0, AI forecast: 65.12584865093231, BCD5 forecast: -28.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 31.49555968046188, 32.919617217779155 ; AI error: 1125.3054683415269 BCD5 error: 580.2\n",
            "Found AL172017 at 2017-10-14 00:00:00\n",
            "\tIntensity Truth: 95.0, AI forecast: 68.75188708305359, BCD5 forecast: -21.0\n",
            "\tTrajectory Truth: 36.4, 22.6; AI forecast: 31.523447251319887, 33.010974168777466 ; AI error: 594.8950422433426 BCD5 error: 175.9\n",
            "Found AL172017 at 2017-10-14 06:00:00\n",
            "\tIntensity Truth: 90.0, AI forecast: 64.50755804777145, BCD5 forecast: -8.0\n",
            "\tTrajectory Truth: 37.9, 19.8; AI forecast: 32.83545858860016, 30.931963387131685 ; AI error: 623.4693488148728 BCD5 error: 136.4\n",
            "Found AL172017 at 2017-10-14 12:00:00\n",
            "\tIntensity Truth: 85.0, AI forecast: 59.576750099658966, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 39.9, 17.0; AI forecast: 33.4382185459137, 29.226238524913782 ; AI error: 704.0947992139976 BCD5 error: 122.4\n",
            "Found AL172017 at 2017-10-14 18:00:00\n",
            "\tIntensity Truth: 80.0, AI forecast: 64.41961586475372, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 43.1, 14.3; AI forecast: 34.85799953937531, 27.34739602208137 ; AI error: 783.0686902497656 BCD5 error: 155.3\n",
            "Found AL172017 at 2017-10-15 00:00:00\n",
            "Found AL172017 at 2017-10-15 06:00:00\n",
            "Found AL172017 at 2017-10-15 12:00:00\n",
            "Found AL172017 at 2017-10-15 18:00:00\n",
            "Found AL192017 at 2017-11-07 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 41.93685047328472, BCD5 forecast: -3.0\n",
            "\tTrajectory Truth: 34.6, 48.7; AI forecast: 27.26739523410797, 50.92357246652245 ; AI error: 454.8613452442067 BCD5 error: 108.5\n",
            "\tIntensity Truth: 45.0, AI forecast: 44.28510246798396, BCD5 forecast: 3.0\n",
            "\tTrajectory Truth: 41.8, 48.8; AI forecast: 26.606824123859404, 50.493859122693536 ; AI error: 916.0208203857492 BCD5 error: 416.2\n",
            "Found AL192017 at 2017-11-07 06:00:00\n",
            "\tIntensity Truth: 50.0, AI forecast: 44.685759735293686, BCD5 forecast: 0.0\n",
            "\tTrajectory Truth: 36.4, 48.7; AI forecast: 27.623425328731535, 50.51461894437671 ; AI error: 534.9525713145767 BCD5 error: 133.5\n",
            "Found AL192017 at 2017-11-07 12:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 45.86566789075732, BCD5 forecast: 1.0\n",
            "\tTrajectory Truth: 38.3, 48.8; AI forecast: 28.634031903743743, 50.4190679743886 ; AI error: 585.9566105725199 BCD5 error: 178.4\n",
            "Found AL192017 at 2017-11-07 18:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 47.27873831987381, BCD5 forecast: -4.0\n",
            "\tTrajectory Truth: 40.1, 49.0; AI forecast: 30.011866259574887, 49.88089050799608 ; AI error: 607.2318330731792 BCD5 error: 198.7\n",
            "Found AL192017 at 2017-11-08 00:00:00\n",
            "\tIntensity Truth: 45.0, AI forecast: 53.05693559348583, BCD5 forecast: 4.0\n",
            "\tTrajectory Truth: 41.8, 48.8; AI forecast: 31.91267011165619, 48.97341637909412 ; AI error: 593.6980881607955 BCD5 error: 181.9\n",
            "Found AL192017 at 2017-11-08 06:00:00\n",
            "Found AL192017 at 2017-11-08 12:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btLAyw8CTA7B",
        "colab_type": "code",
        "outputId": "513c1f3a-be50-4b83-850d-3afce16497df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(ai_wind_errors).describe()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>997.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>18.843852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>16.534018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.021094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.778555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14.188020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>27.415935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>93.422785</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  997.000000\n",
              "mean    18.843852\n",
              "std     16.534018\n",
              "min      0.021094\n",
              "25%      5.778555\n",
              "50%     14.188020\n",
              "75%     27.415935\n",
              "max     93.422785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRxsLuy4TA7D",
        "colab_type": "code",
        "outputId": "7e5c0a17-2495-4ece-955f-7b4ba6d61eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(bcd5_wind_errors).describe()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>997.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>17.656971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>15.179471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>26.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>79.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  997.000000\n",
              "mean    17.656971\n",
              "std     15.179471\n",
              "min      0.000000\n",
              "25%      6.000000\n",
              "50%     13.000000\n",
              "75%     26.000000\n",
              "max     79.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aam7310tTA7F",
        "colab_type": "code",
        "outputId": "f92340bb-dbed-4d3a-a9b9-b7b8ae4ddf07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(ai_track_errors).describe()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>997.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>525.544207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>456.171791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.366501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>213.536192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>377.680055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>661.558680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2482.168650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   997.000000\n",
              "mean    525.544207\n",
              "std     456.171791\n",
              "min       4.366501\n",
              "25%     213.536192\n",
              "50%     377.680055\n",
              "75%     661.558680\n",
              "max    2482.168650"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbC4Y2CfTA7G",
        "colab_type": "code",
        "outputId": "484e2950-b8c3-47e4-9852-afc116d6f96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.DataFrame(bcd5_track_errors).describe()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>997.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>249.861785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>225.954794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>72.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>176.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>366.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1164.700000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count   997.000000\n",
              "mean    249.861785\n",
              "std     225.954794\n",
              "min       5.800000\n",
              "25%      72.800000\n",
              "50%     176.900000\n",
              "75%     366.100000\n",
              "max    1164.700000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWcOFR7fx7CO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "697ff592-c0f0-4065-d499-dfdc11ec7206"
      },
      "source": [
        "tracks['inputs'][:1]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 1.18253968, -0.23901582,  0.28571429,  0.        ,\n",
              "         -0.55      , -2.        , -1.25      ,  1.        ,\n",
              "         -5.        , -0.0625    ,  0.4       ],\n",
              "        [ 1.12698413, -0.26713533,  0.28571429,  0.        ,\n",
              "         -0.7       , -2.        , -1.25      ,  1.        ,\n",
              "         -5.        , -0.0625    ,  0.8       ],\n",
              "        [ 1.07142857, -0.29876977,  0.14285714, -1.        ,\n",
              "         -0.65      , -2.        , -1.33333333,  1.        ,\n",
              "         -5.        ,  0.        , -0.4       ],\n",
              "        [ 1.03174603, -0.32688928,  0.        , -1.        ,\n",
              "         -0.6       , -1.6       , -1.25      ,  1.        ,\n",
              "         -5.        ,  0.        ,  0.        ],\n",
              "        [ 0.99206349, -0.35852373,  0.        ,  0.        ,\n",
              "         -0.55      , -1.6       , -1.33333333,  1.        ,\n",
              "         -5.        ,  0.        ,  0.4       ]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SmjvdTI14mr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8cd6c926-02c5-4d25-e6ea-4afda9530383"
      },
      "source": [
        "[output[2] for output in \n",
        " scaler.inverse_transform(\n",
        "     [[0,0,winds[0],0,0,0,0,0,0,0,0] for winds in model_wind.predict(tracks['inputs'][:1])[0]]\n",
        " )]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[48.76174069941044,\n",
              " 52.29152776300907,\n",
              " 53.06020461022854,\n",
              " 53.759809136390686,\n",
              " 54.67978909611702]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvAJObXp66cR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hurricane_ai(input):\n",
        "  '''\n",
        "  input = {\n",
        "    -120 : timestep,\n",
        "    -96 : timestep,\n",
        "    -72 : timestep,\n",
        "    -48 : timestep,\n",
        "    -24 : timestep,\n",
        "    0 : timestep\n",
        "  }\n",
        "  output = {\n",
        "    24 : prediction,\n",
        "    48 : prediction,\n",
        "    72 : prediction,\n",
        "    96 : prediction,\n",
        "    120 : prediction\n",
        "  }\n",
        "  timestep = {\n",
        "      'lat' : float,\n",
        "      'long' : float,\n",
        "      'max-wind' : float,\n",
        "      'min_pressure' : float,\n",
        "      'entry-time' : datetime\n",
        "  }\n",
        "  prediction = {\n",
        "    'lat' : float,\n",
        "    'long' : float,\n",
        "    'max-winds' : float\n",
        "  }\n",
        "  '''\n",
        "  # Take entries and transform them into our data model\n",
        "  extract = []\n",
        "  temp = None\n",
        "  for index, value in enumerate([-120, -96, -72, -48, -24, 0]):\n",
        "    if not index :\n",
        "      temp = input[value]\n",
        "      continue\n",
        "    else:\n",
        "      extract.append(list(feature_extraction(input[value], temp).values()))\n",
        "      temp = input[value]\n",
        "  \n",
        "  state = np.expand_dims(scaler.transform(extract), axis = 0)\n",
        "  print('extract: {}, state: {}'.format(extract, state))\n",
        "  # Finally, use our hurricane ai to predict storm state\n",
        "  lat = [output[0] for output in scaler.inverse_transform(\n",
        "      [[lat[0],0,0,0,0,0,0,0,0,0,0] for lat in model_lat.predict(state)[0]])]\n",
        "  long = [output[1] for output in scaler.inverse_transform(\n",
        "      [[0,long[0],0,0,0,0,0,0,0,0,0] for long in model_long.predict(state)[0]])]\n",
        "  wind = [output[2] for output in scaler.inverse_transform(\n",
        "      [[0,0,wind[0],0,0,0,0,0,0,0,0] for wind in model_wind.predict(state)[0]])]\n",
        "   \n",
        "  output = dict()\n",
        "  for index, value in enumerate([24, 48, 72, 96, 120]) :\n",
        "    output[value] = {\n",
        "        'lat' : lat[index],\n",
        "        'long' : long[index],\n",
        "        'max_wind' : wind[index]\n",
        "    }\n",
        "  \n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjOHJAOlM2bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dateutil.parser import parse\n",
        "input = {\n",
        "  0 : {\n",
        "      'entry_time' : parse('Fri Aug 30 2019 1100 PM'),\n",
        "      'lat' : 25.5,\n",
        "      'long' : 71.4,\n",
        "      'max_wind' : 140 / 1.51 , # mph to knots\n",
        "      'min_pressure' : 948.0\n",
        "    }\n",
        "  -24 : {\n",
        "      'entry_time' : parse('Thu Aug 29 2019 1100 PM'),\n",
        "      'lat' : 23.3,\n",
        "      'long' : 68.4,\n",
        "      'max_wind' : 105 / 1.51 , # mph to knots\n",
        "      'min_pressure' : 977.0\n",
        "    },\n",
        "  -48 : {\n",
        "      'entry_time' : parse('Wed Aug 28 2019 1100 PM'),\n",
        "      'lat' : 19.7,\n",
        "      'long' : 66.0,\n",
        "      'max_wind' : 85 / 1.51 , # mph to knots\n",
        "      'min_pressure' : 986.0\n",
        "    },\n",
        "  -72 : {\n",
        "      'entry_time' : parse('Tue Aug 27 2019 1100 PM'),\n",
        "      'lat' : 16.0,\n",
        "      'long' : 63.0,\n",
        "      'max_wind' : 50 / 1.51 , # mph to knots\n",
        "      'min_pressure' : 1006.0\n",
        "    },\n",
        "  -96 : {\n",
        "      'entry_time' : parse('Mon Aug 26 2019 1100 PM'),\n",
        "      'lat' : 13.2,\n",
        "      'long' : 59.7,\n",
        "      'max_wind' : 50 / 1.51 , # mph to knots\n",
        "      'min_pressure' : 1003.0\n",
        "    },\n",
        "  -120 : {\n",
        "      'entry_time' : parse('Sun Aug 25 2019 1100 PM'),\n",
        "      'lat' : 11.7,\n",
        "      'long' : 55.3,\n",
        "      'max_wind' : 50 / 1.51 , # mph to knots\n",
        "      'min_pressure' : 1003.0\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_evczPmdLabu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "3e78a284-5d6d-4c3e-ae8a-eececea3d9c5"
      },
      "source": [
        "hurricane_ai(input)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "extract: [[13.2, 59.7, 92.71523178807946, 0.0, 1003.0, 0.0625, 0.18333333333333357, 2019, 8, 26, 23], [16.0, 63.0, 92.71523178807946, 0.0, 1006.0, 0.1166666666666667, 0.13749999999999987, 2019, 8, 27, 23], [19.7, 66.0, 92.71523178807946, 0.0, 986.0, 0.15416666666666665, 0.125, 2019, 8, 28, 23], [23.3, 68.4, 92.71523178807946, 0.0, 977.0, 0.15000000000000005, 0.10000000000000024, 2019, 8, 29, 23], [25.5, 71.4, 92.71523178807946, 0.0, 948.0, 0.09166666666666663, 0.125, 2019, 8, 30, 23]], state: [[[-0.55555556  0.11950791  1.36329234  0.          0.15\n",
            "    0.15        0.33333333  1.125      -1.          0.5625\n",
            "    1.13333333]\n",
            "  [-0.33333333  0.23550088  1.36329234  0.          0.3\n",
            "    0.8         0.10416667  1.125      -1.          0.625\n",
            "    1.13333333]\n",
            "  [-0.03968254  0.34094903  1.36329234  0.         -0.7\n",
            "    1.25        0.04166667  1.125      -1.          0.6875\n",
            "    1.13333333]\n",
            "  [ 0.24603175  0.42530756  1.36329234  0.         -1.15\n",
            "    1.2        -0.08333333  1.125      -1.          0.75\n",
            "    1.13333333]\n",
            "  [ 0.42063492  0.53075571  1.36329234  0.         -2.6\n",
            "    0.5         0.04166667  1.125      -1.          0.8125\n",
            "    1.13333333]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{24: {'lat': 23.09066185057163,\n",
              "  'long': 67.37367187142372,\n",
              "  'max_wind': 80.4758495092392},\n",
              " 48: {'lat': 26.951516091823578,\n",
              "  'long': 70.66133508086205,\n",
              "  'max_wind': 82.59643375873566},\n",
              " 72: {'lat': 32.081086015701295,\n",
              "  'long': 69.74207293093204,\n",
              "  'max_wind': 78.64478081464767},\n",
              " 96: {'lat': 37.890209794044495,\n",
              "  'long': 64.54580676853656,\n",
              "  'max_wind': 72.31878906488419},\n",
              " 120: {'lat': 43.72626209259033,\n",
              "  'long': 56.22684582218062,\n",
              "  'max_wind': 67.97349810600281}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u58TQ3E0fQ5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "60c70ddd-a17e-44f6-e55e-29454661cd98"
      },
      "source": [
        "!git status"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X8b7S3umtz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}