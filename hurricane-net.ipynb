{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hurricane-net\n",
    "Hammad Usmani\n",
    "### A machine learning algorithm to forecast the intensity and trajectory of Atlantic tropical storms\n",
    "[https://github.com/hammad93/hurricane-net](https://github.com/hammad93/hurricane-net)\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "2. [Problem](#Problem)\n",
    "3. [Datasets](#Datasets)\n",
    "4. [Workflow Diagram](#Workflow)\n",
    "5. [Data Extraction](#Extract)\n",
    "6. [Data Transformation](#Transform)\n",
    "7. [Data Loading](#Load)\n",
    "8. [Feature Engineering](#FeatureEngineering)\n",
    "9. [Model Architecture](#ModelArchitecture)\n",
    "11. [Model Selection](#ModelSelection)\n",
    "12. [Paramater Optimization](#Optimization)\n",
    "13. [Model Evaluation & Benchmarks](#Benchmarks)\n",
    "14. [Visualizations](#Visualizations)\n",
    "\n",
    "![Hurricane Maria 2017](img/hurricane-maria.png \"Hurricane Maria. Source: NOAA\")\n",
    "\n",
    "## Background<a id=\"Background\"></a>\n",
    "\n",
    "The National Hurricane Center (NHC) and National Oceanic and Atmospheric Administration (NOAA) provide predictions for storms trajectories, intensity, and size. They create these predictions based on models that can be classified into 3 groups: dynamical, statistical, and ensemble [1]. The most accurate models are based on computational fluid dynamics and achieve more precision than their statistical and ensemble counterparts [1][4]. The current statistical models (OCD5) are based on multiple regression methods that can explain a significant amount of variance [1]. In this project, we research and implement the domain of machine learning and deep learning into predictive hurricane models for both trajectory and intensity and evaluate them against the NHC standards. \n",
    "Previous research into machine learning to forecast tropical Atlantic storms include a sparse recurrent neural network (Kordmahalleh, Sefidmazgi, & Homaifar, 2016) and an artificial neural network (Jung & Das, 2013); both achieved favorable results. The hurricane models created can be utilized to develop more precise emergency planning. There is a necessity for more accurate and timely models that can help reduce the amount of loss caused by hurricanes. \n",
    "\n",
    "## Problem<a id=\"Problem\"></a>\n",
    "\n",
    "The NOAA and NHC have several different classifications for Atlantic hurricane models that describe feature prediction and model architecture. The 3 main classifications for hurricane model architecture include dynamical, statistical, and ensemble. Classifications also include relative compute time required to create an output grouped as either early or late and forecast parameters such as trajectory, intensity, and wind radii. The most accurate models are late models that take upwards of 6 hours to produce an output whereas models that can produce an output in seconds to minutes are called early. Early models tend to be statistical which include the baseline model for trajectory named CLIPER5 Climatology and Persistence (CLP5) utilizing multivariate regression. The performance for these methods can be augmented by incorporating more advanced statistical methods from deep learning such as recurrent neural networks. Kordmahalleh et al., 2016 created a sparse recurrent neural network augmented by a genetic algorithm but there are factors requiring improvement. The training set utilized an older version of the NHC Hurricane Database format known as HURDAT while a new format has been released called HURDAT2 with additional information on wind radii. Kordmahalleh et al., 2016 also utilized benchmarks different from the standard applied within the NHC. Other than improving their methodology, we can expand the scope by creating separate models for both intensity and trajectory. These models can be used to predict the trajectory and intensity for future Atlantic storms.\n",
    "\n",
    "## Datasets<a id=\"Datasets\"></a>\n",
    "\n",
    "The following datasets and inputs including their sources will be used to create machine learning models:\n",
    "- NHC Hurricane Database (HURDAT2)\n",
    "    - http://www.nhc.noaa.gov/data/#hurdat\n",
    "    - https://www.kaggle.com/noaa/hurricane-database\n",
    "- NHC Forecast Error Database\n",
    "    - http://www.nhc.noaa.gov/verification/verify7.shtml\n",
    "- NHC GIS\n",
    "    - http://www.nhc.noaa.gov/gis/\n",
    "    \n",
    "The NHC HURDAT2 database contains the tracking information for Atlantic tropical and subtropical cyclones which includes hurricanes and tropical storms from 1851 to 2016. The most updated version of the dataset is included on the noaa.gov site and includes 2 additional years of cyclone data compared to the data set available on Kaggle and is potentially more descriptive. To match the inputs of the baseline model used by the NHC, we are calculating the forward motion of the storm by applying a vector based on previous and current geographical location.\n",
    "\n",
    "*Table 1. This table contains the tentative features as input to the model*\n",
    "\n",
    "| **Name**         | **Data Type** | **Description**                                                     |\n",
    "|------------------|---------------|---------------------------------------------------------------------|\n",
    "| Time             | Date Time     | The date and time of the measurement.                               |\n",
    "| Latitude         | Float         | The geographical latitude of the storm eye to 1 decimal precision.  |\n",
    "| Longitude        | Float         | The geographical longitude of the storm eye to 1 decimal precision. |\n",
    "| Maximum Winds    | Integer       | The maximum sustained winds within the storm.                       |\n",
    "| Minimum Pressure | Integer       | The minimum barometric pressure within the storm.                   |\n",
    "| Forward Motion   | String        | Calculated vector of motion based on location in time series.       |\n",
    "\n",
    "The Forecast Error Database contains information on the accuracy of predicted models from the NHC. The two model forecast errors available are labeled OFCL and BCD5. The OFCL is the official NHC forecast and the BCD5 is the real track available. This data set can be used to benchmark and evaluate the deep learning model. \n",
    "The NOAA and NHC also hosts a geographical information system (GIS) that contains raw and processed data on hurricanes. The server hosting the GIS is publicly accessible and can be used to evaluate our model by comparing the 2017 Atlantic tropical season. The preliminary best tracks can be found here before they are finalized and available in the HURDAT2 data set. With the GIS, we can construct a final evaluation data set.\n",
    "\n",
    "*Diagram 1. This graphic describes the workflow for the deep learning models*.<a id=\"Workflow\"></a>\n",
    "![Data Pipeline](img/Deep Learning Workflow.png \"hurricane-net Data Pipeline\")\n",
    "\n",
    "## Extract Data<a id=\"Extract\"></a>\n",
    "\n",
    "*The following code uses the hurdat2 and models modules created to provide a class interface for the HURDAT2 and error forecast database located in the data and models folder. *\n",
    "\n",
    "We will begin our steps to perform extraction, transformation, and loading of our data for analysis or broadly known as ETL. Although we're dividing these steps into disctinct procedures, they are often more fluid and often have overlaps. The extraction phase consists of collecting and parsing the HURDAT2 and error forecast databases for analysis and benchmarking. The HURDAT2 database is our core foundation for creating the deep learning model. We store the database in its raw .txt format but it can be directly linked to the database hosted by the NHC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_id</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>entry_time</th>\n",
       "      <th>entry_id</th>\n",
       "      <th>entry_status</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>max_wind</th>\n",
       "      <th>min_pressure</th>\n",
       "      <th>34kt_ne</th>\n",
       "      <th>...</th>\n",
       "      <th>34kt_sw</th>\n",
       "      <th>34kt_nw</th>\n",
       "      <th>50kt_ne</th>\n",
       "      <th>50kt_se</th>\n",
       "      <th>50kt_sw</th>\n",
       "      <th>50kt_nw</th>\n",
       "      <th>64kt_ne</th>\n",
       "      <th>64kt_se</th>\n",
       "      <th>64kt_sw</th>\n",
       "      <th>64kt_nw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44063</th>\n",
       "      <td>AL122005</td>\n",
       "      <td>KATRINA</td>\n",
       "      <td>2005-08-23 18:00:00</td>\n",
       "      <td></td>\n",
       "      <td>TD</td>\n",
       "      <td>23.1N</td>\n",
       "      <td>75.1W</td>\n",
       "      <td>30</td>\n",
       "      <td>1008</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44064</th>\n",
       "      <td>AL122005</td>\n",
       "      <td>KATRINA</td>\n",
       "      <td>2005-08-24 00:00:00</td>\n",
       "      <td></td>\n",
       "      <td>TD</td>\n",
       "      <td>23.4N</td>\n",
       "      <td>75.7W</td>\n",
       "      <td>30</td>\n",
       "      <td>1007</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44065</th>\n",
       "      <td>AL122005</td>\n",
       "      <td>KATRINA</td>\n",
       "      <td>2005-08-24 06:00:00</td>\n",
       "      <td></td>\n",
       "      <td>TD</td>\n",
       "      <td>23.8N</td>\n",
       "      <td>76.2W</td>\n",
       "      <td>30</td>\n",
       "      <td>1007</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44066</th>\n",
       "      <td>AL122005</td>\n",
       "      <td>KATRINA</td>\n",
       "      <td>2005-08-24 12:00:00</td>\n",
       "      <td></td>\n",
       "      <td>TS</td>\n",
       "      <td>24.5N</td>\n",
       "      <td>76.5W</td>\n",
       "      <td>35</td>\n",
       "      <td>1006</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44067</th>\n",
       "      <td>AL122005</td>\n",
       "      <td>KATRINA</td>\n",
       "      <td>2005-08-24 18:00:00</td>\n",
       "      <td></td>\n",
       "      <td>TS</td>\n",
       "      <td>25.4N</td>\n",
       "      <td>76.9W</td>\n",
       "      <td>40</td>\n",
       "      <td>1003</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       storm_id storm_name          entry_time entry_id entry_status    lat  \\\n",
       "44063  AL122005    KATRINA 2005-08-23 18:00:00                    TD  23.1N   \n",
       "44064  AL122005    KATRINA 2005-08-24 00:00:00                    TD  23.4N   \n",
       "44065  AL122005    KATRINA 2005-08-24 06:00:00                    TD  23.8N   \n",
       "44066  AL122005    KATRINA 2005-08-24 12:00:00                    TS  24.5N   \n",
       "44067  AL122005    KATRINA 2005-08-24 18:00:00                    TS  25.4N   \n",
       "\n",
       "        long max_wind min_pressure 34kt_ne   ...   34kt_sw 34kt_nw 50kt_ne  \\\n",
       "44063  75.1W       30         1008       0   ...         0       0       0   \n",
       "44064  75.7W       30         1007       0   ...         0       0       0   \n",
       "44065  76.2W       30         1007       0   ...         0       0       0   \n",
       "44066  76.5W       35         1006      60   ...         0       0       0   \n",
       "44067  76.9W       40         1003      60   ...         0       0       0   \n",
       "\n",
       "      50kt_se 50kt_sw 50kt_nw 64kt_ne 64kt_se 64kt_sw 64kt_nw  \n",
       "44063       0       0       0       0       0       0       0  \n",
       "44064       0       0       0       0       0       0       0  \n",
       "44065       0       0       0       0       0       0       0  \n",
       "44066       0       0       0       0       0       0       0  \n",
       "44067       0       0       0       0       0       0       0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import various libraries throughout the software\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import datetime\n",
    "import dateutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Import from hurdat2 class in data folder and models class from hurricane-models folder\n",
    "from data.hurdat2 import hurdat2\n",
    "from errors.models import models\n",
    "\n",
    "# Initialize Dataframe for hurricanes and error database\n",
    "dataset = hurdat2(\"data/hurdat2.txt\")\n",
    "errors = models(\"errors/1970-present_OFCL_v_BCD5_ind_ATL_TI_errors_noTDs.txt\")\n",
    "\n",
    "# Show the first 5 records from Hurricane Katrina 2005 (AL122005)\n",
    "dataset.hurricanes.query('storm_id == \"AL122005\"').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{       'intensity_forecast': {       datetime.datetime(2005, 8, 28, 18, 0): 0.0,\n",
      "                                      datetime.datetime(2005, 8, 29, 6, 0): 20.9,\n",
      "                                      datetime.datetime(2005, 8, 29, 18, 0): 93.6,\n",
      "                                      datetime.datetime(2005, 8, 30, 6, 0): 170.2,\n",
      "                                      datetime.datetime(2005, 8, 30, 18, 0): None,\n",
      "                                      datetime.datetime(2005, 8, 31, 18, 0): None,\n",
      "                                      datetime.datetime(2005, 9, 1, 18, 0): None,\n",
      "                                      datetime.datetime(2005, 9, 2, 18, 0): None,\n",
      "                                      datetime.datetime(2005, 9, 3, 18, 0): None,\n",
      "                                      datetime.datetime(2005, 9, 4, 18, 0): None},\n",
      "        'lat': 26.3,\n",
      "        'long': 88.6,\n",
      "        'sample_sizes': {       'F012': 0.33,\n",
      "                                'F024': 0.33,\n",
      "                                'F036': 0.33,\n",
      "                                'F048': 0.0,\n",
      "                                'F072': 0.0,\n",
      "                                'F096': 0.0,\n",
      "                                'F120': 0.0,\n",
      "                                'F144': 0.0,\n",
      "                                'F168': 0.0},\n",
      "        'track_forecast': {       datetime.datetime(2005, 8, 28, 18, 0): 0.0,\n",
      "                                  datetime.datetime(2005, 8, 29, 6, 0): 28.0,\n",
      "                                  datetime.datetime(2005, 8, 29, 18, 0): 32.0,\n",
      "                                  datetime.datetime(2005, 8, 30, 6, 0): 17.0,\n",
      "                                  datetime.datetime(2005, 8, 30, 18, 0): None,\n",
      "                                  datetime.datetime(2005, 8, 31, 18, 0): None,\n",
      "                                  datetime.datetime(2005, 9, 1, 18, 0): None,\n",
      "                                  datetime.datetime(2005, 9, 2, 18, 0): None,\n",
      "                                  datetime.datetime(2005, 9, 3, 18, 0): None,\n",
      "                                  datetime.datetime(2005, 9, 4, 18, 0): None},\n",
      "        'wind_speed': 150}\n"
     ]
    }
   ],
   "source": [
    "# Show the first 3 OFCL hurricane model errors for Hurricane Katrina 2005 on 28-08-2005/18:00:00\n",
    "pprint(errors.models['OFCL'].storm['AL122005'][datetime.datetime(2005, 8, 28, 18, 0)], indent = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data<a id=\"Transform\"></a>\n",
    "\n",
    "The following code will tranform the hurricane best path data into objects that can be better manipulated for processing. to match between datasets, we will also create a `storm_id` dictionary to store storm names matched with ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming HURDAT2 into objects . . .\n",
      "Transforming 49691/49691 entries from HURDAT2\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create hurricane class\n",
    "class hurricane(object) : \n",
    "    def __init__(self, name, id) :\n",
    "        # Set instance variables\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.entries = dict()\n",
    "        self.models = dict()\n",
    "        \n",
    "        return\n",
    "    # Add hurricane track entry based on standard HURDAT2 format\n",
    "    def add_entry(self, array) :\n",
    "        entry = {\n",
    "            array[0] : { # dateteime of entry\n",
    "                'entry_time' : array[0], \n",
    "                'entry_id' : array[1],\n",
    "                'entry_status' : array[2],\n",
    "                'lat' : float(array[3][:-1]), # Convert to number from format '#.#N'\n",
    "                'long' : float(array[4][:-1]), # Convert to number from format '#.#W'\n",
    "                'max_wind' : float(array[5]),\n",
    "                'min_pressure' : None if array[6] is None else float(array[6]), # Early records are -999 or None\n",
    "                'wind_radii' :  array[7:], # Array based on HURDAT2 format\n",
    "            }\n",
    "        }\n",
    "        self.entries.update(entry)\n",
    "        \n",
    "        return\n",
    "    # Add hurricane model errors\n",
    "    def add_model(self, name, model) :\n",
    "        self.models[name] = model\n",
    "        \n",
    "        return\n",
    "# Storm ID Key for matching between datasets\n",
    "storm_id = dict()\n",
    "\n",
    "# Parse in hurricanes\n",
    "hurricanes = dict()\n",
    "print(\"Transforming HURDAT2 into objects . . .\")\n",
    "for index, entry in dataset.hurricanes.iterrows() :\n",
    "    print(\"Transforming {}/{} entries from HURDAT2\".format(index + 1, len(dataset.hurricanes)), end = \"\\r\")\n",
    "    # New hurricane\n",
    "    if entry['storm_id'] not in hurricanes :\n",
    "        hurricanes[entry['storm_id']] = hurricane(entry['storm_name'], entry['storm_id'])\n",
    "        storm_id[entry['storm_id']] = entry['storm_name']\n",
    "    # Add entry to hurricane\n",
    "    hurricanes[entry['storm_id']].add_entry(entry[2:])\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data<a id=\"Load\"></a>\n",
    "\n",
    "The following will finalize our preliminary data preparation by loading some of the errors into each hurricane object. Note that models start from the year 1970 and any hurricane before that has no previous model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all available model errors\n",
    "models = errors.models.keys()\n",
    "# Load model errors into hurricanes\n",
    "for id in storm_id :\n",
    "    for model in models :\n",
    "        # Skip if this hurricane does not have the model\n",
    "        if id not in errors.models[model].storm :\n",
    "            continue\n",
    "        hurricanes[id].add_model(model, errors.models[model].storm[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Engineering & Data Augmentation<a id=\"FeatureEngineering\"></a>\n",
    "\n",
    "The following section will extract the relevant features and engineer each data point so that we can fit it into the model. Because the type of inputs are important, the features will be transformed based on the model architecture. This will also include data augmentation methods. The higher level architecture will be a deep learning recurrent neural network with LSTM and time distributed layers.\n",
    "\n",
    "The current statistical baseline model using multivariate regression uses multiple predictors as input. According to Knaff 2013, the following predictors were calculated for their intensity model that were not included in the HURDAT2 database. These features can be calculated from the data loaded into our current object model.\n",
    "\n",
    "1. Date Information\n",
    "2. Zonal Speed Of The Storm (U) (kt)\n",
    "3. Meridional Speed Of The Storm (V) (kt)\n",
    "4. 12-h Change In Intensity (DVMX) (kt)\n",
    "\n",
    "The shape on the input to the LSTM will be in a 3D array with the format [samples, timestamps, features]. We will intitially begin with 1 time step and evaluate more can benefit our model. The output requires a 5 day forecast and observations without track data 5 days in the future will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineered 1830/1830 hurricanes for 5 timestep(s)\n",
      "Done feature engineering hurricanes.\n",
      "Scaling Data . . . (1 timestep for unqiue data)\n",
      "Feature engineered 1830/1830 hurricanes for 1 timestep(s)\n",
      "Done feature engineering hurricanes.\n",
      "Done scaling.\n"
     ]
    }
   ],
   "source": [
    "def feature_extraction(timestep, previous) :\n",
    "    '''\n",
    "    PURPOSE: Calculate the features for a machine learning model within the context of hurricane-net\n",
    "    METHOD: Use the predictors and the calculation methodology defined in Knaff 2013\n",
    "    INPUT:  timestep - current dictionary of features in the hurricane object format\n",
    "            previous - previous timestep dictionary of features in the hurricane object format\n",
    "    OUTPUT: Dictionary of features\n",
    "    '''\n",
    "    features = {\n",
    "        'lat' : timestep['lat'],\n",
    "        'long' : timestep['long'],\n",
    "        'max_wind' : timestep['max_wind'],\n",
    "        'delta_wind' : (timestep['max_wind'] - previous['max_wind']) / # Calculated from track (12h)\n",
    "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 43200),\n",
    "        'min_pressure' : timestep['min_pressure'], \n",
    "        'zonal_speed' : (timestep['lat'] - previous['lat'])/ # Calculated from track (per hour)\n",
    "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 3600),\n",
    "        'meridonal_speed' : (timestep['long'] - previous['long'])/# Calculated from track (per hour)\n",
    "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 3600),\n",
    "        'year' : timestep['entry_time'].year,\n",
    "        'month' : timestep['entry_time'].month,\n",
    "        'day' : timestep['entry_time'].day,\n",
    "        'hour' : timestep['entry_time'].hour,\n",
    "    }\n",
    "    return features\n",
    "    \n",
    "def storm_x_y(storm, timesteps = 1, lag = 24) :\n",
    "    '''\n",
    "    PURPOSE: Create independent and dependent samples for a machine learning model based on the timesteps\n",
    "    METHOD: Use the HURDAT2 database and a hurricane object as defined in hurricane-net for feature extraction\n",
    "    INPUT:  storm - hurricane object\n",
    "            timesteps - (default = 1) number of timesteps to calculate\n",
    "            include_none - (default = False) Boolean for including None in test data. Imputing function unavailable.\n",
    "            lag - (default = 24) lag in hours for the dependent variables up to 5 days\n",
    "    OUTPUT: Dictionary with independent (x) and dependent (y) values.\n",
    "    '''\n",
    "    x = []\n",
    "    # Create testing data structure with a dictionary\n",
    "    times = [time * lag for time in range(1, (120 // lag) + 1)] # Begin at lag hours with lag increments up to 120h inclusive\n",
    "    y = dict([(time,[]) for time in times])\n",
    "    \n",
    "    # Sort by entry time\n",
    "    entries = [entry[1] for entry in sorted(storm.entries.items())]\n",
    "    \n",
    "    for index in range(len(entries)) :\n",
    "        if index < timesteps : # Flag for insufficient initial time steps\n",
    "            continue\n",
    "\n",
    "        # If we're not including None values, check to see if there will be any\n",
    "        if None in [storm.entries.get(entries[index]['entry_time'] +\n",
    "                                         datetime.timedelta(hours = future)) for future in times] : break\n",
    "            \n",
    "        # Calculate time steps and their features for independent values\n",
    "        sample = []\n",
    "        for step in range(timesteps) :\n",
    "            # Training sample\n",
    "            timestep = entries[index - step]\n",
    "            previous = entries[index - step - 1]\n",
    "            sample.append([timestep['entry_time']] + [[feature_extraction(timestep, previous)]])\n",
    "        x.append(sample) # Add our constructed sample\n",
    "        \n",
    "        # Calculate time steps and their features for dependent values\n",
    "        for future in times :\n",
    "            timestep = storm.entries.get(entries[index]['entry_time'] + datetime.timedelta(hours = future))\n",
    "            previous = storm.entries.get(entries[index]['entry_time'] + datetime.timedelta(hours = future - lag))\n",
    "            \n",
    "            if timestep and previous: \n",
    "                y[future].append(feature_extraction(timestep, previous))\n",
    "            else :\n",
    "                y[future].append(None)\n",
    "    \n",
    "    # Return output, if there is no output, return None.\n",
    "    if len(x) is 0 :\n",
    "        return None\n",
    "    else:\n",
    "        return {'x': x, 'y': y}\n",
    "def shape(hurricanes, timesteps, remove_missing = True) :\n",
    "    '''\n",
    "    PURPOSE: Shape our data for input into machine learning models\n",
    "    METHOD: Use a numpy array to shape into (samples, timesteps, features)\n",
    "    INPUT:  hurricanes - dictionary of hurricane objects\n",
    "            timesteps - number of timesteps for the shape\n",
    "            remove_missing - boolean indicating whether the algorithm will disregard missing values\n",
    "    OUTPUT: numpy array of shape (samples, timesteps, 8) where 8 is the number of predictors in a hurricane object\n",
    "    '''\n",
    "    x = []\n",
    "    y = []\n",
    "    lag = 24 # lag time in hours\n",
    "    precision = np.float64 # defines the precision of our data type\n",
    "    times = [time * lag for time in range(1, (120 // lag) + 1)] # Begin at lag hours with lag increments up to 120h inclusive\n",
    "    count = 0\n",
    "    for hurricane in hurricanes.values() :\n",
    "        count += 1\n",
    "        result = storm_x_y(hurricane, timesteps, lag)\n",
    "        if result is None :\n",
    "            continue\n",
    "        # Extract only the values from the strom features using our specified precision\n",
    "        hurricane_x = np.array(\n",
    "            [[list(sample[1][0].values()) for sample in x] for x in result['x']],\n",
    "            dtype = precision)\n",
    "        hurricane_y = np.array(\n",
    "            [[list(result['y'][time][index].values()) for time in times] for index in range(len(result['y'][lag]))],\n",
    "            dtype = precision)\n",
    "        # Disregard if algorithm requires no missing values\n",
    "        if remove_missing :\n",
    "            if (len(np.where(np.isnan(hurricane_x))[0]) > 0) or (len(np.where(np.isnan(hurricane_y))[0]) > 0) :\n",
    "                continue\n",
    "        # Add to our results\n",
    "        x.extend(hurricane_x)\n",
    "        y.extend(hurricane_y)\n",
    "        print(\"Feature engineered {}/{} hurricanes for {} timestep(s)\".format(count, len(hurricanes), timesteps), end = \"\\r\")\n",
    "    print(\"\\nDone feature engineering hurricanes.\")\n",
    "    \n",
    "    return {'x': np.array(x), 'y': np.array(y)}\n",
    "def scaler(processed_data, hurricanes) :\n",
    "    '''\n",
    "    PURPOSE: Scale our data using the RobustScaler method from the sklearn library\n",
    "    METHOD: Generate data using 1 timesteps and then remove the NaN or None types to use the scaler methods\n",
    "    INPUT:  hurricanes - dictionary of hurricane objects\n",
    "            processed_data - dictionary of x and y values of data produced by shape() function with no missing values\n",
    "    OUTPUT: 1) Scaled processed_data using RobustScaler\n",
    "            2) RobustScaler object fit with appropriate data\n",
    "    '''\n",
    "    print(\"Scaling Data . . . (1 timestep for unqiue data)\")\n",
    "    # Create our scaler\n",
    "    unqiue_data = shape(hurricanes, timesteps = 1)\n",
    "    x = np.reshape(unqiue_data['x'], (unqiue_data['x'].shape[0], -1))\n",
    "    x = np.delete(x, np.where(np.isnan(x))[0], 0)\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(x)\n",
    "    \n",
    "    # Scale our data\n",
    "    for index in range(len(processed_data['x'])) :\n",
    "        # Scale our x\n",
    "        processed_data['x'][index] = scaler.transform(processed_data['x'][index])\n",
    "        # Scale our y\n",
    "        processed_data['y'][index] = scaler.transform(processed_data['y'][index])\n",
    "    print(\"Done scaling.\")\n",
    "    return processed_data, scaler\n",
    "# Finalize and scale procesed data into a dictionary\n",
    "preprocessed_data = shape(hurricanes, timesteps = 5)\n",
    "processed_data, scaler = scaler(preprocessed_data, hurricanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Architecture<a id=\"ModelArchitecture\"></a>\n",
    "\n",
    "Following feature engineering, we are now ready to input our data into a machine learning algorithm. The scope of this project will attempt a deep learning approach to forecasting Atlantic tropical cyclones. We will experiment with nunermous different architectures but we will focus around a Recurrent Neural Network utilizing LSTM cells.\n",
    "\n",
    "Notes:\n",
    "- We will use 500 epochs for wind intensity because the validation loss is not decresing\n",
    "- We will use 1,000 epochs for latitute and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_4 (Bidirection (None, 5, 2048)           8486912   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 5, 512)            5244928   \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 5, 1)              513       \n",
      "=================================================================\n",
      "Total params: 13,732,353\n",
      "Trainable params: 13,732,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3298 samples, validate on 825 samples\n",
      "Epoch 1/500\n",
      "3298/3298 [==============================] - 3s 1ms/step - loss: 0.7135 - val_loss: 0.6226\n",
      "Epoch 2/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.6116 - val_loss: 0.6060\n",
      "Epoch 3/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.5923 - val_loss: 0.5785\n",
      "Epoch 4/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.5617 - val_loss: 0.5499\n",
      "Epoch 5/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.5327 - val_loss: 0.5333\n",
      "Epoch 6/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.5208 - val_loss: 0.5194\n",
      "Epoch 7/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.5117 - val_loss: 0.5219\n",
      "Epoch 8/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.5180 - val_loss: 0.5178\n",
      "Epoch 9/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.5163 - val_loss: 0.5094\n",
      "Epoch 10/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.5053 - val_loss: 0.5101\n",
      "Epoch 11/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.5002 - val_loss: 0.5108\n",
      "Epoch 12/500\n",
      "3298/3298 [==============================] - 1s 303us/step - loss: 0.4983 - val_loss: 0.5100\n",
      "Epoch 13/500\n",
      "3298/3298 [==============================] - 1s 303us/step - loss: 0.4968 - val_loss: 0.5113\n",
      "Epoch 14/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4952 - val_loss: 0.5120\n",
      "Epoch 15/500\n",
      "3298/3298 [==============================] - 1s 303us/step - loss: 0.4972 - val_loss: 0.5087\n",
      "Epoch 16/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.4929 - val_loss: 0.5034\n",
      "Epoch 17/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.4882 - val_loss: 0.5001\n",
      "Epoch 18/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.4854 - val_loss: 0.4987\n",
      "Epoch 19/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4863 - val_loss: 0.4975\n",
      "Epoch 20/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.4850 - val_loss: 0.4960\n",
      "Epoch 21/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.4863 - val_loss: 0.4937\n",
      "Epoch 22/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4839 - val_loss: 0.4911\n",
      "Epoch 23/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4795 - val_loss: 0.4896\n",
      "Epoch 24/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4792 - val_loss: 0.4890\n",
      "Epoch 25/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4749 - val_loss: 0.4880\n",
      "Epoch 26/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4779 - val_loss: 0.4870\n",
      "Epoch 27/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4738 - val_loss: 0.4855\n",
      "Epoch 28/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4714 - val_loss: 0.4825\n",
      "Epoch 29/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.4673 - val_loss: 0.4802\n",
      "Epoch 30/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4655 - val_loss: 0.4779\n",
      "Epoch 31/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4662 - val_loss: 0.4744\n",
      "Epoch 32/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4623 - val_loss: 0.4707\n",
      "Epoch 33/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4579 - val_loss: 0.4676\n",
      "Epoch 34/500\n",
      "3298/3298 [==============================] - 1s 304us/step - loss: 0.4574 - val_loss: 0.4644\n",
      "Epoch 35/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4555 - val_loss: 0.4617\n",
      "Epoch 36/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4539 - val_loss: 0.4598\n",
      "Epoch 37/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4523 - val_loss: 0.4570\n",
      "Epoch 38/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4489 - val_loss: 0.4554\n",
      "Epoch 39/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4513 - val_loss: 0.4541\n",
      "Epoch 40/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4481 - val_loss: 0.4537\n",
      "Epoch 41/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4469 - val_loss: 0.4530\n",
      "Epoch 42/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4465 - val_loss: 0.4504\n",
      "Epoch 43/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4451 - val_loss: 0.4495\n",
      "Epoch 44/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4434 - val_loss: 0.4492\n",
      "Epoch 45/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4433 - val_loss: 0.4469\n",
      "Epoch 46/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4406 - val_loss: 0.4453\n",
      "Epoch 47/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4399 - val_loss: 0.4445\n",
      "Epoch 48/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4387 - val_loss: 0.4453\n",
      "Epoch 49/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4394 - val_loss: 0.4442\n",
      "Epoch 50/500\n",
      "3298/3298 [==============================] - 1s 305us/step - loss: 0.4382 - val_loss: 0.4422\n",
      "Epoch 51/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4369 - val_loss: 0.4428\n",
      "Epoch 52/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4351 - val_loss: 0.4399\n",
      "Epoch 53/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.4320 - val_loss: 0.4390\n",
      "Epoch 54/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4329 - val_loss: 0.4393\n",
      "Epoch 55/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4319 - val_loss: 0.4366\n",
      "Epoch 56/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4304 - val_loss: 0.4355\n",
      "Epoch 57/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4299 - val_loss: 0.4349\n",
      "Epoch 58/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.4298 - val_loss: 0.4338\n",
      "Epoch 59/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4253 - val_loss: 0.4326\n",
      "Epoch 60/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4252 - val_loss: 0.4325\n",
      "Epoch 61/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4265 - val_loss: 0.4312\n",
      "Epoch 62/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4236 - val_loss: 0.4294\n",
      "Epoch 63/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4243 - val_loss: 0.4289\n",
      "Epoch 64/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4232 - val_loss: 0.4275\n",
      "Epoch 65/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4193 - val_loss: 0.4276\n",
      "Epoch 66/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4212 - val_loss: 0.4284\n",
      "Epoch 67/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4201 - val_loss: 0.4267\n",
      "Epoch 68/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4179 - val_loss: 0.4281\n",
      "Epoch 69/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4182 - val_loss: 0.4245\n",
      "Epoch 70/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4169 - val_loss: 0.4246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/500\n",
      "3298/3298 [==============================] - 1s 306us/step - loss: 0.4152 - val_loss: 0.4236\n",
      "Epoch 72/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.4131 - val_loss: 0.4253\n",
      "Epoch 73/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.4147 - val_loss: 0.4269\n",
      "Epoch 74/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4179 - val_loss: 0.4233\n",
      "Epoch 75/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.4141 - val_loss: 0.4216\n",
      "Epoch 76/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.4126 - val_loss: 0.4188\n",
      "Epoch 77/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.4098 - val_loss: 0.4189\n",
      "Epoch 78/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4082 - val_loss: 0.4192\n",
      "Epoch 79/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4103 - val_loss: 0.4151\n",
      "Epoch 80/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4059 - val_loss: 0.4149\n",
      "Epoch 81/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4079 - val_loss: 0.4166\n",
      "Epoch 82/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4066 - val_loss: 0.4164\n",
      "Epoch 83/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.4068 - val_loss: 0.4139\n",
      "Epoch 84/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4031 - val_loss: 0.4122\n",
      "Epoch 85/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.4031 - val_loss: 0.4134\n",
      "Epoch 86/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.4037 - val_loss: 0.4119\n",
      "Epoch 87/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.4008 - val_loss: 0.4100\n",
      "Epoch 88/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3997 - val_loss: 0.4068\n",
      "Epoch 89/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3980 - val_loss: 0.4067\n",
      "Epoch 90/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.3972 - val_loss: 0.4061\n",
      "Epoch 91/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.3954 - val_loss: 0.4047\n",
      "Epoch 92/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3946 - val_loss: 0.4041\n",
      "Epoch 93/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3941 - val_loss: 0.4045\n",
      "Epoch 94/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3931 - val_loss: 0.4079\n",
      "Epoch 95/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3912 - val_loss: 0.4060\n",
      "Epoch 96/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3939 - val_loss: 0.4063\n",
      "Epoch 97/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3940 - val_loss: 0.4011\n",
      "Epoch 98/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3892 - val_loss: 0.3986\n",
      "Epoch 99/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3877 - val_loss: 0.3967\n",
      "Epoch 100/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3861 - val_loss: 0.3967\n",
      "Epoch 101/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3851 - val_loss: 0.3961\n",
      "Epoch 102/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3843 - val_loss: 0.3935\n",
      "Epoch 103/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3824 - val_loss: 0.3947\n",
      "Epoch 104/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3815 - val_loss: 0.3926\n",
      "Epoch 105/500\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.3789 - val_loss: 0.3913\n",
      "Epoch 106/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3764 - val_loss: 0.3898\n",
      "Epoch 107/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3764 - val_loss: 0.3944\n",
      "Epoch 108/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.3748 - val_loss: 0.3969\n",
      "Epoch 109/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3853 - val_loss: 0.4036\n",
      "Epoch 110/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3858 - val_loss: 0.3915\n",
      "Epoch 111/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3790 - val_loss: 0.3859\n",
      "Epoch 112/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3713 - val_loss: 0.3946\n",
      "Epoch 113/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3751 - val_loss: 0.3866\n",
      "Epoch 114/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3711 - val_loss: 0.3870\n",
      "Epoch 115/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3707 - val_loss: 0.3880\n",
      "Epoch 116/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3678 - val_loss: 0.3833\n",
      "Epoch 117/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3635 - val_loss: 0.3839\n",
      "Epoch 118/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3662 - val_loss: 0.3798\n",
      "Epoch 119/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3612 - val_loss: 0.3825\n",
      "Epoch 120/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3635 - val_loss: 0.3798\n",
      "Epoch 121/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3590 - val_loss: 0.3781\n",
      "Epoch 122/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3577 - val_loss: 0.3772\n",
      "Epoch 123/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3555 - val_loss: 0.3753\n",
      "Epoch 124/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3518 - val_loss: 0.3735\n",
      "Epoch 125/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3506 - val_loss: 0.3734\n",
      "Epoch 126/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3497 - val_loss: 0.3733\n",
      "Epoch 127/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3524 - val_loss: 0.3728\n",
      "Epoch 128/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3502 - val_loss: 0.3711\n",
      "Epoch 129/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3449 - val_loss: 0.3690\n",
      "Epoch 130/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3477 - val_loss: 0.3688\n",
      "Epoch 131/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3454 - val_loss: 0.3680\n",
      "Epoch 132/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3482 - val_loss: 0.3713\n",
      "Epoch 133/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3483 - val_loss: 0.3702\n",
      "Epoch 134/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3540 - val_loss: 0.3762\n",
      "Epoch 135/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.3521 - val_loss: 0.3654\n",
      "Epoch 136/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3434 - val_loss: 0.3584\n",
      "Epoch 137/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3373 - val_loss: 0.3647\n",
      "Epoch 138/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3412 - val_loss: 0.3611\n",
      "Epoch 139/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3387 - val_loss: 0.3537\n",
      "Epoch 140/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3305 - val_loss: 0.3599\n",
      "Epoch 141/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3389 - val_loss: 0.3505\n",
      "Epoch 142/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3267 - val_loss: 0.3536\n",
      "Epoch 143/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3262 - val_loss: 0.3511\n",
      "Epoch 144/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3263 - val_loss: 0.3454\n",
      "Epoch 145/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3234 - val_loss: 0.3482\n",
      "Epoch 146/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.3210 - val_loss: 0.3429\n",
      "Epoch 147/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3186 - val_loss: 0.3420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.3193 - val_loss: 0.3394\n",
      "Epoch 149/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3141 - val_loss: 0.3395\n",
      "Epoch 150/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3134 - val_loss: 0.3338\n",
      "Epoch 151/500\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.3122 - val_loss: 0.3345\n",
      "Epoch 152/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3082 - val_loss: 0.3365\n",
      "Epoch 153/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3067 - val_loss: 0.3341\n",
      "Epoch 154/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3161 - val_loss: 0.3469\n",
      "Epoch 155/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3168 - val_loss: 0.3324\n",
      "Epoch 156/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3102 - val_loss: 0.3305\n",
      "Epoch 157/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3050 - val_loss: 0.3363\n",
      "Epoch 158/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3034 - val_loss: 0.3238\n",
      "Epoch 159/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.3035 - val_loss: 0.3281\n",
      "Epoch 160/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3055 - val_loss: 0.3226\n",
      "Epoch 161/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2946 - val_loss: 0.3197\n",
      "Epoch 162/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2955 - val_loss: 0.3238\n",
      "Epoch 163/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2976 - val_loss: 0.3111\n",
      "Epoch 164/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2891 - val_loss: 0.3209\n",
      "Epoch 165/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2948 - val_loss: 0.3133\n",
      "Epoch 166/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2862 - val_loss: 0.3098\n",
      "Epoch 167/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2894 - val_loss: 0.3134\n",
      "Epoch 168/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2839 - val_loss: 0.3044\n",
      "Epoch 169/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2781 - val_loss: 0.3021\n",
      "Epoch 170/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2819 - val_loss: 0.3005\n",
      "Epoch 171/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2781 - val_loss: 0.3002\n",
      "Epoch 172/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2761 - val_loss: 0.3027\n",
      "Epoch 173/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2745 - val_loss: 0.2972\n",
      "Epoch 174/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2724 - val_loss: 0.3008\n",
      "Epoch 175/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2751 - val_loss: 0.2946\n",
      "Epoch 176/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2705 - val_loss: 0.2906\n",
      "Epoch 177/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2676 - val_loss: 0.2884\n",
      "Epoch 178/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2624 - val_loss: 0.2847\n",
      "Epoch 179/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2624 - val_loss: 0.2843\n",
      "Epoch 180/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2613 - val_loss: 0.2813\n",
      "Epoch 181/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2561 - val_loss: 0.2796\n",
      "Epoch 182/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2540 - val_loss: 0.2798\n",
      "Epoch 183/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2585 - val_loss: 0.2835\n",
      "Epoch 184/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2590 - val_loss: 0.2825\n",
      "Epoch 185/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2600 - val_loss: 0.2803\n",
      "Epoch 186/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2552 - val_loss: 0.2727\n",
      "Epoch 187/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2507 - val_loss: 0.2705\n",
      "Epoch 188/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2499 - val_loss: 0.2676\n",
      "Epoch 189/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2451 - val_loss: 0.2677\n",
      "Epoch 190/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2525 - val_loss: 0.2708\n",
      "Epoch 191/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2468 - val_loss: 0.2651\n",
      "Epoch 192/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2448 - val_loss: 0.2664\n",
      "Epoch 193/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2414 - val_loss: 0.2571\n",
      "Epoch 194/500\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.2343 - val_loss: 0.2593\n",
      "Epoch 195/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2402 - val_loss: 0.2585\n",
      "Epoch 196/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2363 - val_loss: 0.2542\n",
      "Epoch 197/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2360 - val_loss: 0.2560\n",
      "Epoch 198/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2345 - val_loss: 0.2497\n",
      "Epoch 199/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2335 - val_loss: 0.2501\n",
      "Epoch 200/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2276 - val_loss: 0.2553\n",
      "Epoch 201/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2360 - val_loss: 0.2499\n",
      "Epoch 202/500\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.2317 - val_loss: 0.2571\n",
      "Epoch 203/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2359 - val_loss: 0.2423\n",
      "Epoch 204/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2247 - val_loss: 0.2441\n",
      "Epoch 205/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2224 - val_loss: 0.2499\n",
      "Epoch 206/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2284 - val_loss: 0.2396\n",
      "Epoch 207/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2205 - val_loss: 0.2415\n",
      "Epoch 208/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2161 - val_loss: 0.2420\n",
      "Epoch 209/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2188 - val_loss: 0.2390\n",
      "Epoch 210/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2181 - val_loss: 0.2311\n",
      "Epoch 211/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2127 - val_loss: 0.2371\n",
      "Epoch 212/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2160 - val_loss: 0.2332\n",
      "Epoch 213/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2126 - val_loss: 0.2282\n",
      "Epoch 214/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2067 - val_loss: 0.2313\n",
      "Epoch 215/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2078 - val_loss: 0.2309\n",
      "Epoch 216/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2106 - val_loss: 0.2250\n",
      "Epoch 217/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2083 - val_loss: 0.2307\n",
      "Epoch 218/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2075 - val_loss: 0.2253\n",
      "Epoch 219/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2060 - val_loss: 0.2299\n",
      "Epoch 220/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2059 - val_loss: 0.2219\n",
      "Epoch 221/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2054 - val_loss: 0.2232\n",
      "Epoch 222/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1987 - val_loss: 0.2208\n",
      "Epoch 223/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2033 - val_loss: 0.2169\n",
      "Epoch 224/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1972 - val_loss: 0.2239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2000 - val_loss: 0.2206\n",
      "Epoch 226/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2027 - val_loss: 0.2219\n",
      "Epoch 227/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1973 - val_loss: 0.2166\n",
      "Epoch 228/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1992 - val_loss: 0.2176\n",
      "Epoch 229/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1975 - val_loss: 0.2132\n",
      "Epoch 230/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1939 - val_loss: 0.2111\n",
      "Epoch 231/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1901 - val_loss: 0.2157\n",
      "Epoch 232/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1917 - val_loss: 0.2089\n",
      "Epoch 233/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1898 - val_loss: 0.2120\n",
      "Epoch 234/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1918 - val_loss: 0.2083\n",
      "Epoch 235/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1859 - val_loss: 0.2086\n",
      "Epoch 236/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1858 - val_loss: 0.2064\n",
      "Epoch 237/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1827 - val_loss: 0.2044\n",
      "Epoch 238/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1821 - val_loss: 0.2068\n",
      "Epoch 239/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1832 - val_loss: 0.2026\n",
      "Epoch 240/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1837 - val_loss: 0.2085\n",
      "Epoch 241/500\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1859 - val_loss: 0.2062\n",
      "Epoch 242/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1817 - val_loss: 0.2095\n",
      "Epoch 243/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1833 - val_loss: 0.2020\n",
      "Epoch 244/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1832 - val_loss: 0.2048\n",
      "Epoch 245/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1758 - val_loss: 0.1997\n",
      "Epoch 246/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1786 - val_loss: 0.1989\n",
      "Epoch 247/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1801 - val_loss: 0.2018\n",
      "Epoch 248/500\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.1740 - val_loss: 0.1977\n",
      "Epoch 249/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1727 - val_loss: 0.2043\n",
      "Epoch 250/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1798 - val_loss: 0.1971\n",
      "Epoch 251/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1715 - val_loss: 0.1936\n",
      "Epoch 252/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1720 - val_loss: 0.1995\n",
      "Epoch 253/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1749 - val_loss: 0.1926\n",
      "Epoch 254/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1717 - val_loss: 0.1980\n",
      "Epoch 255/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1746 - val_loss: 0.1948\n",
      "Epoch 256/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1699 - val_loss: 0.1922\n",
      "Epoch 257/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1652 - val_loss: 0.1912\n",
      "Epoch 258/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1673 - val_loss: 0.1895\n",
      "Epoch 259/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1664 - val_loss: 0.1918\n",
      "Epoch 260/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1655 - val_loss: 0.1887\n",
      "Epoch 261/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1630 - val_loss: 0.1892\n",
      "Epoch 262/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1626 - val_loss: 0.1854\n",
      "Epoch 263/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1646 - val_loss: 0.1906\n",
      "Epoch 264/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1670 - val_loss: 0.1860\n",
      "Epoch 265/500\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.1597 - val_loss: 0.1834\n",
      "Epoch 266/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1630 - val_loss: 0.1903\n",
      "Epoch 267/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1627 - val_loss: 0.1859\n",
      "Epoch 268/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1591 - val_loss: 0.1861\n",
      "Epoch 269/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1604 - val_loss: 0.1849\n",
      "Epoch 270/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1592 - val_loss: 0.1875\n",
      "Epoch 271/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1578 - val_loss: 0.1831\n",
      "Epoch 272/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1557 - val_loss: 0.1880\n",
      "Epoch 273/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1608 - val_loss: 0.1813\n",
      "Epoch 274/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1598 - val_loss: 0.1830\n",
      "Epoch 275/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1570 - val_loss: 0.1813\n",
      "Epoch 276/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1553 - val_loss: 0.1826\n",
      "Epoch 277/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1516 - val_loss: 0.1813\n",
      "Epoch 278/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1550 - val_loss: 0.1802\n",
      "Epoch 279/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1505 - val_loss: 0.1812\n",
      "Epoch 280/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1523 - val_loss: 0.1787\n",
      "Epoch 281/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1504 - val_loss: 0.1785\n",
      "Epoch 282/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1543 - val_loss: 0.1778\n",
      "Epoch 283/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1460 - val_loss: 0.1808\n",
      "Epoch 284/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1501 - val_loss: 0.1784\n",
      "Epoch 285/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1513 - val_loss: 0.1781\n",
      "Epoch 286/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1489 - val_loss: 0.1756\n",
      "Epoch 287/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1472 - val_loss: 0.1761\n",
      "Epoch 288/500\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.1499 - val_loss: 0.1769\n",
      "Epoch 289/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1469 - val_loss: 0.1721\n",
      "Epoch 290/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1424 - val_loss: 0.1773\n",
      "Epoch 291/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1475 - val_loss: 0.1779\n",
      "Epoch 292/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1486 - val_loss: 0.1827\n",
      "Epoch 293/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1500 - val_loss: 0.1753\n",
      "Epoch 294/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1488 - val_loss: 0.1749\n",
      "Epoch 295/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1454 - val_loss: 0.1709\n",
      "Epoch 296/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1410 - val_loss: 0.1711\n",
      "Epoch 297/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1438 - val_loss: 0.1816\n",
      "Epoch 298/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1507 - val_loss: 0.1729\n",
      "Epoch 299/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1455 - val_loss: 0.1771\n",
      "Epoch 300/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1411 - val_loss: 0.1718\n",
      "Epoch 301/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1384 - val_loss: 0.1691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1411 - val_loss: 0.1788\n",
      "Epoch 303/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1475 - val_loss: 0.1706\n",
      "Epoch 304/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1454 - val_loss: 0.1694\n",
      "Epoch 305/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1370 - val_loss: 0.1716\n",
      "Epoch 306/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1364 - val_loss: 0.1696\n",
      "Epoch 307/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1417 - val_loss: 0.1760\n",
      "Epoch 308/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1480 - val_loss: 0.1669\n",
      "Epoch 309/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1380 - val_loss: 0.1675\n",
      "Epoch 310/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1376 - val_loss: 0.1682\n",
      "Epoch 311/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1368 - val_loss: 0.1671\n",
      "Epoch 312/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1421 - val_loss: 0.1728\n",
      "Epoch 313/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1399 - val_loss: 0.1667\n",
      "Epoch 314/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1372 - val_loss: 0.1658\n",
      "Epoch 315/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1322 - val_loss: 0.1663\n",
      "Epoch 316/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1365 - val_loss: 0.1639\n",
      "Epoch 317/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1330 - val_loss: 0.1666\n",
      "Epoch 318/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1331 - val_loss: 0.1630\n",
      "Epoch 319/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1323 - val_loss: 0.1621\n",
      "Epoch 320/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1314 - val_loss: 0.1640\n",
      "Epoch 321/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1311 - val_loss: 0.1622\n",
      "Epoch 322/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1292 - val_loss: 0.1652\n",
      "Epoch 323/500\n",
      "3298/3298 [==============================] - 1s 319us/step - loss: 0.1261 - val_loss: 0.1625\n",
      "Epoch 324/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1328 - val_loss: 0.1614\n",
      "Epoch 325/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1295 - val_loss: 0.1606\n",
      "Epoch 326/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1291 - val_loss: 0.1619\n",
      "Epoch 327/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1272 - val_loss: 0.1582\n",
      "Epoch 328/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1264 - val_loss: 0.1592\n",
      "Epoch 329/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1255 - val_loss: 0.1604\n",
      "Epoch 330/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1303 - val_loss: 0.1632\n",
      "Epoch 331/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1309 - val_loss: 0.1616\n",
      "Epoch 332/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1297 - val_loss: 0.1611\n",
      "Epoch 333/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1261 - val_loss: 0.1579\n",
      "Epoch 334/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1239 - val_loss: 0.1590\n",
      "Epoch 335/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1273 - val_loss: 0.1650\n",
      "Epoch 336/500\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.1332 - val_loss: 0.1619\n",
      "Epoch 337/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1319 - val_loss: 0.1616\n",
      "Epoch 338/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1290 - val_loss: 0.1605\n",
      "Epoch 339/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1262 - val_loss: 0.1602\n",
      "Epoch 340/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1252 - val_loss: 0.1636\n",
      "Epoch 341/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1260 - val_loss: 0.1599\n",
      "Epoch 342/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1276 - val_loss: 0.1614\n",
      "Epoch 343/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1242 - val_loss: 0.1589\n",
      "Epoch 344/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1231 - val_loss: 0.1563\n",
      "Epoch 345/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1202 - val_loss: 0.1643\n",
      "Epoch 346/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1277 - val_loss: 0.1591\n",
      "Epoch 347/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1248 - val_loss: 0.1567\n",
      "Epoch 348/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1202 - val_loss: 0.1596\n",
      "Epoch 349/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1311 - val_loss: 0.1560\n",
      "Epoch 350/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1202 - val_loss: 0.1579\n",
      "Epoch 351/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1226 - val_loss: 0.1555\n",
      "Epoch 352/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1200 - val_loss: 0.1570\n",
      "Epoch 353/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1222 - val_loss: 0.1554\n",
      "Epoch 354/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1190 - val_loss: 0.1556\n",
      "Epoch 355/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1178 - val_loss: 0.1566\n",
      "Epoch 356/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1197 - val_loss: 0.1544\n",
      "Epoch 357/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1174 - val_loss: 0.1520\n",
      "Epoch 358/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1187 - val_loss: 0.1561\n",
      "Epoch 359/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1170 - val_loss: 0.1545\n",
      "Epoch 360/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1195 - val_loss: 0.1513\n",
      "Epoch 361/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1156 - val_loss: 0.1544\n",
      "Epoch 362/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1154 - val_loss: 0.1528\n",
      "Epoch 363/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1152 - val_loss: 0.1513\n",
      "Epoch 364/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1162 - val_loss: 0.1531\n",
      "Epoch 365/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1168 - val_loss: 0.1545\n",
      "Epoch 366/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1176 - val_loss: 0.1518\n",
      "Epoch 367/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1110 - val_loss: 0.1511\n",
      "Epoch 368/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1133 - val_loss: 0.1500\n",
      "Epoch 369/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1128 - val_loss: 0.1501\n",
      "Epoch 370/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1123 - val_loss: 0.1508\n",
      "Epoch 371/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1121 - val_loss: 0.1522\n",
      "Epoch 372/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1134 - val_loss: 0.1518\n",
      "Epoch 373/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1139 - val_loss: 0.1530\n",
      "Epoch 374/500\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.1176 - val_loss: 0.1554\n",
      "Epoch 375/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1145 - val_loss: 0.1511\n",
      "Epoch 376/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1107 - val_loss: 0.1534\n",
      "Epoch 377/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1109 - val_loss: 0.1537\n",
      "Epoch 378/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1106 - val_loss: 0.1508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1134 - val_loss: 0.1498\n",
      "Epoch 380/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1089 - val_loss: 0.1503\n",
      "Epoch 381/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1078 - val_loss: 0.1511\n",
      "Epoch 382/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1101 - val_loss: 0.1487\n",
      "Epoch 383/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1088 - val_loss: 0.1472\n",
      "Epoch 384/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1122 - val_loss: 0.1515\n",
      "Epoch 385/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1111 - val_loss: 0.1491\n",
      "Epoch 386/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1108 - val_loss: 0.1490\n",
      "Epoch 387/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1092 - val_loss: 0.1484\n",
      "Epoch 388/500\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.1105 - val_loss: 0.1490\n",
      "Epoch 389/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1067 - val_loss: 0.1466\n",
      "Epoch 390/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1098 - val_loss: 0.1494\n",
      "Epoch 391/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1082 - val_loss: 0.1496\n",
      "Epoch 392/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1107 - val_loss: 0.1464\n",
      "Epoch 393/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1088 - val_loss: 0.1450\n",
      "Epoch 394/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1097 - val_loss: 0.1502\n",
      "Epoch 395/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1083 - val_loss: 0.1464\n",
      "Epoch 396/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1076 - val_loss: 0.1501\n",
      "Epoch 397/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1116 - val_loss: 0.1504\n",
      "Epoch 398/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1140 - val_loss: 0.1515\n",
      "Epoch 399/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1059 - val_loss: 0.1477\n",
      "Epoch 400/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1077 - val_loss: 0.1473\n",
      "Epoch 401/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1030 - val_loss: 0.1486\n",
      "Epoch 402/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1061 - val_loss: 0.1469\n",
      "Epoch 403/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1106 - val_loss: 0.1499\n",
      "Epoch 404/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1083 - val_loss: 0.1482\n",
      "Epoch 405/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1078 - val_loss: 0.1469\n",
      "Epoch 406/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1017 - val_loss: 0.1462\n",
      "Epoch 407/500\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1054 - val_loss: 0.1446\n",
      "Epoch 408/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1062 - val_loss: 0.1492\n",
      "Epoch 409/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1063 - val_loss: 0.1465\n",
      "Epoch 410/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1068 - val_loss: 0.1478\n",
      "Epoch 411/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1048 - val_loss: 0.1480\n",
      "Epoch 412/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1086 - val_loss: 0.1442\n",
      "Epoch 413/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1043 - val_loss: 0.1482\n",
      "Epoch 414/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1077 - val_loss: 0.1463\n",
      "Epoch 415/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1062 - val_loss: 0.1467\n",
      "Epoch 416/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1043 - val_loss: 0.1444\n",
      "Epoch 417/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1024 - val_loss: 0.1448\n",
      "Epoch 418/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1012 - val_loss: 0.1453\n",
      "Epoch 419/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1020 - val_loss: 0.1437\n",
      "Epoch 420/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1040 - val_loss: 0.1431\n",
      "Epoch 421/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1014 - val_loss: 0.1431\n",
      "Epoch 422/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1013 - val_loss: 0.1452\n",
      "Epoch 423/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1034 - val_loss: 0.1442\n",
      "Epoch 424/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0988 - val_loss: 0.1457\n",
      "Epoch 425/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1012 - val_loss: 0.1478\n",
      "Epoch 426/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1040 - val_loss: 0.1431\n",
      "Epoch 427/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1008 - val_loss: 0.1416\n",
      "Epoch 428/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0975 - val_loss: 0.1423\n",
      "Epoch 429/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0980 - val_loss: 0.1422\n",
      "Epoch 430/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0986 - val_loss: 0.1447\n",
      "Epoch 431/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1028 - val_loss: 0.1449\n",
      "Epoch 432/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1010 - val_loss: 0.1421\n",
      "Epoch 433/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0978 - val_loss: 0.1414\n",
      "Epoch 434/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0976 - val_loss: 0.1446\n",
      "Epoch 435/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0975 - val_loss: 0.1457\n",
      "Epoch 436/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0988 - val_loss: 0.1415\n",
      "Epoch 437/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0990 - val_loss: 0.1418\n",
      "Epoch 438/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0981 - val_loss: 0.1420\n",
      "Epoch 439/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0984 - val_loss: 0.1408\n",
      "Epoch 440/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0964 - val_loss: 0.1439\n",
      "Epoch 441/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1008 - val_loss: 0.1431\n",
      "Epoch 442/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0996 - val_loss: 0.1463\n",
      "Epoch 443/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1000 - val_loss: 0.1415\n",
      "Epoch 444/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1006 - val_loss: 0.1422\n",
      "Epoch 445/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0996 - val_loss: 0.1411\n",
      "Epoch 446/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0971 - val_loss: 0.1403\n",
      "Epoch 447/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0966 - val_loss: 0.1417\n",
      "Epoch 448/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0962 - val_loss: 0.1399\n",
      "Epoch 449/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0957 - val_loss: 0.1410\n",
      "Epoch 450/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0948 - val_loss: 0.1386\n",
      "Epoch 451/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0927 - val_loss: 0.1401\n",
      "Epoch 452/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0959 - val_loss: 0.1400\n",
      "Epoch 453/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0939 - val_loss: 0.1391\n",
      "Epoch 454/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0943 - val_loss: 0.1389\n",
      "Epoch 455/500\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0975 - val_loss: 0.1396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0961 - val_loss: 0.1398\n",
      "Epoch 457/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0929 - val_loss: 0.1389\n",
      "Epoch 458/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0948 - val_loss: 0.1399\n",
      "Epoch 459/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0971 - val_loss: 0.1407\n",
      "Epoch 460/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0968 - val_loss: 0.1409\n",
      "Epoch 461/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0949 - val_loss: 0.1394\n",
      "Epoch 462/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0953 - val_loss: 0.1388\n",
      "Epoch 463/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0973 - val_loss: 0.1406\n",
      "Epoch 464/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0943 - val_loss: 0.1414\n",
      "Epoch 465/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0939 - val_loss: 0.1381\n",
      "Epoch 466/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0960 - val_loss: 0.1381\n",
      "Epoch 467/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0945 - val_loss: 0.1409\n",
      "Epoch 468/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0939 - val_loss: 0.1381\n",
      "Epoch 469/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0910 - val_loss: 0.1380\n",
      "Epoch 470/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0933 - val_loss: 0.1373\n",
      "Epoch 471/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0946 - val_loss: 0.1383\n",
      "Epoch 472/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0916 - val_loss: 0.1371\n",
      "Epoch 473/500\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0911 - val_loss: 0.1363\n",
      "Epoch 474/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0911 - val_loss: 0.1368\n",
      "Epoch 475/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0956 - val_loss: 0.1384\n",
      "Epoch 476/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0965 - val_loss: 0.1358\n",
      "Epoch 477/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0926 - val_loss: 0.1365\n",
      "Epoch 478/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0907 - val_loss: 0.1350\n",
      "Epoch 479/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0896 - val_loss: 0.1385\n",
      "Epoch 480/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0893 - val_loss: 0.1402\n",
      "Epoch 481/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0908 - val_loss: 0.1364\n",
      "Epoch 482/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0891 - val_loss: 0.1398\n",
      "Epoch 483/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0937 - val_loss: 0.1392\n",
      "Epoch 484/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0946 - val_loss: 0.1381\n",
      "Epoch 485/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0908 - val_loss: 0.1378\n",
      "Epoch 486/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0908 - val_loss: 0.1366\n",
      "Epoch 487/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0886 - val_loss: 0.1365\n",
      "Epoch 488/500\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0901 - val_loss: 0.1394\n",
      "Epoch 489/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0895 - val_loss: 0.1357\n",
      "Epoch 490/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0866 - val_loss: 0.1371\n",
      "Epoch 491/500\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0897 - val_loss: 0.1382\n",
      "Epoch 492/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0901 - val_loss: 0.1378\n",
      "Epoch 493/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0878 - val_loss: 0.1360\n",
      "Epoch 494/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0891 - val_loss: 0.1366\n",
      "Epoch 495/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0877 - val_loss: 0.1378\n",
      "Epoch 496/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0872 - val_loss: 0.1355\n",
      "Epoch 497/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0919 - val_loss: 0.1379\n",
      "Epoch 498/500\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0888 - val_loss: 0.1366\n",
      "Epoch 499/500\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0896 - val_loss: 0.1374\n",
      "Epoch 500/500\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0894 - val_loss: 0.1351\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_5 (Bidirection (None, 5, 2048)           8486912   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 5, 512)            5244928   \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 5, 1)              513       \n",
      "=================================================================\n",
      "Total params: 13,732,353\n",
      "Trainable params: 13,732,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3298 samples, validate on 825 samples\n",
      "Epoch 1/1000\n",
      "3298/3298 [==============================] - 4s 1ms/step - loss: 0.7847 - val_loss: 0.5726\n",
      "Epoch 2/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.6091 - val_loss: 0.5024\n",
      "Epoch 3/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.5162 - val_loss: 0.4589\n",
      "Epoch 4/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.4723 - val_loss: 0.4123\n",
      "Epoch 5/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.4188 - val_loss: 0.3892\n",
      "Epoch 6/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3976 - val_loss: 0.3508\n",
      "Epoch 7/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3570 - val_loss: 0.3444\n",
      "Epoch 8/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.3485 - val_loss: 0.3489\n",
      "Epoch 9/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3543 - val_loss: 0.3343\n",
      "Epoch 10/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3387 - val_loss: 0.3286\n",
      "Epoch 11/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.3336 - val_loss: 0.3250\n",
      "Epoch 12/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.3284 - val_loss: 0.3182\n",
      "Epoch 13/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3189 - val_loss: 0.3155\n",
      "Epoch 14/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3146 - val_loss: 0.3108\n",
      "Epoch 15/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3078 - val_loss: 0.3016\n",
      "Epoch 16/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.3013 - val_loss: 0.2948\n",
      "Epoch 17/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2964 - val_loss: 0.2897\n",
      "Epoch 18/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2933 - val_loss: 0.2855\n",
      "Epoch 19/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2944 - val_loss: 0.2824\n",
      "Epoch 20/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2939 - val_loss: 0.2792\n",
      "Epoch 21/1000\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.2901 - val_loss: 0.2789\n",
      "Epoch 22/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2886 - val_loss: 0.2815\n",
      "Epoch 23/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2878 - val_loss: 0.2816\n",
      "Epoch 24/1000\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.2858 - val_loss: 0.2782\n",
      "Epoch 25/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2818 - val_loss: 0.2762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2797 - val_loss: 0.2747\n",
      "Epoch 27/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2795 - val_loss: 0.2720\n",
      "Epoch 28/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2733 - val_loss: 0.2707\n",
      "Epoch 29/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2722 - val_loss: 0.2703\n",
      "Epoch 30/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2717 - val_loss: 0.2694\n",
      "Epoch 31/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2715 - val_loss: 0.2692\n",
      "Epoch 32/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2720 - val_loss: 0.2685\n",
      "Epoch 33/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2724 - val_loss: 0.2668\n",
      "Epoch 34/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.2690 - val_loss: 0.2648\n",
      "Epoch 35/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2680 - val_loss: 0.2631\n",
      "Epoch 36/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2676 - val_loss: 0.2612\n",
      "Epoch 37/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2677 - val_loss: 0.2603\n",
      "Epoch 38/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2648 - val_loss: 0.2598\n",
      "Epoch 39/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2643 - val_loss: 0.2585\n",
      "Epoch 40/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.2636 - val_loss: 0.2568\n",
      "Epoch 41/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2612 - val_loss: 0.2552\n",
      "Epoch 42/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2627 - val_loss: 0.2541\n",
      "Epoch 43/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2597 - val_loss: 0.2530\n",
      "Epoch 44/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2580 - val_loss: 0.2521\n",
      "Epoch 45/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2594 - val_loss: 0.2521\n",
      "Epoch 46/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2604 - val_loss: 0.2535\n",
      "Epoch 47/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.2586 - val_loss: 0.2537\n",
      "Epoch 48/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2609 - val_loss: 0.2532\n",
      "Epoch 49/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2567 - val_loss: 0.2516\n",
      "Epoch 50/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2574 - val_loss: 0.2505\n",
      "Epoch 51/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2567 - val_loss: 0.2509\n",
      "Epoch 52/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2583 - val_loss: 0.2521\n",
      "Epoch 53/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2548 - val_loss: 0.2516\n",
      "Epoch 54/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2560 - val_loss: 0.2501\n",
      "Epoch 55/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2553 - val_loss: 0.2493\n",
      "Epoch 56/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2536 - val_loss: 0.2493\n",
      "Epoch 57/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2547 - val_loss: 0.2496\n",
      "Epoch 58/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2544 - val_loss: 0.2491\n",
      "Epoch 59/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2536 - val_loss: 0.2488\n",
      "Epoch 60/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2517 - val_loss: 0.2481\n",
      "Epoch 61/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2535 - val_loss: 0.2479\n",
      "Epoch 62/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2505 - val_loss: 0.2493\n",
      "Epoch 63/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2522 - val_loss: 0.2486\n",
      "Epoch 64/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2507 - val_loss: 0.2471\n",
      "Epoch 65/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2505 - val_loss: 0.2469\n",
      "Epoch 66/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2497 - val_loss: 0.2471\n",
      "Epoch 67/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2494 - val_loss: 0.2477\n",
      "Epoch 68/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2485 - val_loss: 0.2464\n",
      "Epoch 69/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2484 - val_loss: 0.2461\n",
      "Epoch 70/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.2472 - val_loss: 0.2457\n",
      "Epoch 71/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2501 - val_loss: 0.2461\n",
      "Epoch 72/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2491 - val_loss: 0.2456\n",
      "Epoch 73/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2492 - val_loss: 0.2448\n",
      "Epoch 74/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2484 - val_loss: 0.2444\n",
      "Epoch 75/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2479 - val_loss: 0.2453\n",
      "Epoch 76/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2458 - val_loss: 0.2434\n",
      "Epoch 77/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2441 - val_loss: 0.2433\n",
      "Epoch 78/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2466 - val_loss: 0.2440\n",
      "Epoch 79/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.2463 - val_loss: 0.2429\n",
      "Epoch 80/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2459 - val_loss: 0.2420\n",
      "Epoch 81/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2442 - val_loss: 0.2417\n",
      "Epoch 82/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2447 - val_loss: 0.2417\n",
      "Epoch 83/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2434 - val_loss: 0.2423\n",
      "Epoch 84/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2416 - val_loss: 0.2415\n",
      "Epoch 85/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2427 - val_loss: 0.2402\n",
      "Epoch 86/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2439 - val_loss: 0.2403\n",
      "Epoch 87/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2428 - val_loss: 0.2413\n",
      "Epoch 88/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2461 - val_loss: 0.2409\n",
      "Epoch 89/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2419 - val_loss: 0.2396\n",
      "Epoch 90/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2432 - val_loss: 0.2399\n",
      "Epoch 91/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2405 - val_loss: 0.2395\n",
      "Epoch 92/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2424 - val_loss: 0.2382\n",
      "Epoch 93/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2390 - val_loss: 0.2385\n",
      "Epoch 94/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2433 - val_loss: 0.2401\n",
      "Epoch 95/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2406 - val_loss: 0.2379\n",
      "Epoch 96/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2403 - val_loss: 0.2372\n",
      "Epoch 97/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2379 - val_loss: 0.2370\n",
      "Epoch 98/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2394 - val_loss: 0.2373\n",
      "Epoch 99/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2386 - val_loss: 0.2353\n",
      "Epoch 100/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2368 - val_loss: 0.2352\n",
      "Epoch 101/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2372 - val_loss: 0.2371\n",
      "Epoch 102/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2352 - val_loss: 0.2357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2387 - val_loss: 0.2340\n",
      "Epoch 104/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2361 - val_loss: 0.2352\n",
      "Epoch 105/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2366 - val_loss: 0.2335\n",
      "Epoch 106/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2331 - val_loss: 0.2322\n",
      "Epoch 107/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2348 - val_loss: 0.2332\n",
      "Epoch 108/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2365 - val_loss: 0.2328\n",
      "Epoch 109/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.2352 - val_loss: 0.2338\n",
      "Epoch 110/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2341 - val_loss: 0.2330\n",
      "Epoch 111/1000\n",
      "3298/3298 [==============================] - 1s 307us/step - loss: 0.2363 - val_loss: 0.2338\n",
      "Epoch 112/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2330 - val_loss: 0.2317\n",
      "Epoch 113/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2329 - val_loss: 0.2313\n",
      "Epoch 114/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2331 - val_loss: 0.2304\n",
      "Epoch 115/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2309 - val_loss: 0.2287\n",
      "Epoch 116/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2295 - val_loss: 0.2302\n",
      "Epoch 117/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2315 - val_loss: 0.2298\n",
      "Epoch 118/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.2320 - val_loss: 0.2299\n",
      "Epoch 119/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2298 - val_loss: 0.2275\n",
      "Epoch 120/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2303 - val_loss: 0.2290\n",
      "Epoch 121/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2287 - val_loss: 0.2288\n",
      "Epoch 122/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2283 - val_loss: 0.2281\n",
      "Epoch 123/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2306 - val_loss: 0.2283\n",
      "Epoch 124/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2282 - val_loss: 0.2270\n",
      "Epoch 125/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2290 - val_loss: 0.2239\n",
      "Epoch 126/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2272 - val_loss: 0.2255\n",
      "Epoch 127/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2277 - val_loss: 0.2295\n",
      "Epoch 128/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2286 - val_loss: 0.2235\n",
      "Epoch 129/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2239 - val_loss: 0.2237\n",
      "Epoch 130/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2251 - val_loss: 0.2246\n",
      "Epoch 131/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2240 - val_loss: 0.2272\n",
      "Epoch 132/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2269 - val_loss: 0.2272\n",
      "Epoch 133/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2311 - val_loss: 0.2250\n",
      "Epoch 134/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2254 - val_loss: 0.2226\n",
      "Epoch 135/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.2210 - val_loss: 0.2194\n",
      "Epoch 136/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.2185 - val_loss: 0.2204\n",
      "Epoch 137/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2232 - val_loss: 0.2200\n",
      "Epoch 138/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2206 - val_loss: 0.2223\n",
      "Epoch 139/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2203 - val_loss: 0.2177\n",
      "Epoch 140/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2168 - val_loss: 0.2184\n",
      "Epoch 141/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2188 - val_loss: 0.2175\n",
      "Epoch 142/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2202 - val_loss: 0.2207\n",
      "Epoch 143/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2197 - val_loss: 0.2209\n",
      "Epoch 144/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2207 - val_loss: 0.2187\n",
      "Epoch 145/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2219 - val_loss: 0.2169\n",
      "Epoch 146/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2188 - val_loss: 0.2154\n",
      "Epoch 147/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2143 - val_loss: 0.2163\n",
      "Epoch 148/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2144 - val_loss: 0.2164\n",
      "Epoch 149/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2168 - val_loss: 0.2179\n",
      "Epoch 150/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.2166 - val_loss: 0.2153\n",
      "Epoch 151/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2149 - val_loss: 0.2132\n",
      "Epoch 152/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2156 - val_loss: 0.2140\n",
      "Epoch 153/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2105 - val_loss: 0.2142\n",
      "Epoch 154/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.2105 - val_loss: 0.2114\n",
      "Epoch 155/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2127 - val_loss: 0.2110\n",
      "Epoch 156/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2106 - val_loss: 0.2118\n",
      "Epoch 157/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2100 - val_loss: 0.2123\n",
      "Epoch 158/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2107 - val_loss: 0.2098\n",
      "Epoch 159/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2085 - val_loss: 0.2070\n",
      "Epoch 160/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2072 - val_loss: 0.2084\n",
      "Epoch 161/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2094 - val_loss: 0.2076\n",
      "Epoch 162/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2048 - val_loss: 0.2066\n",
      "Epoch 163/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2061 - val_loss: 0.2078\n",
      "Epoch 164/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2042 - val_loss: 0.2053\n",
      "Epoch 165/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2073 - val_loss: 0.2057\n",
      "Epoch 166/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2057 - val_loss: 0.2066\n",
      "Epoch 167/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.2027 - val_loss: 0.2121\n",
      "Epoch 168/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2080 - val_loss: 0.2104\n",
      "Epoch 169/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2093 - val_loss: 0.2138\n",
      "Epoch 170/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2143 - val_loss: 0.2103\n",
      "Epoch 171/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2096 - val_loss: 0.2051\n",
      "Epoch 172/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2065 - val_loss: 0.2066\n",
      "Epoch 173/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2037 - val_loss: 0.2042\n",
      "Epoch 174/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2036 - val_loss: 0.2061\n",
      "Epoch 175/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.2044 - val_loss: 0.2052\n",
      "Epoch 176/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1982 - val_loss: 0.1992\n",
      "Epoch 177/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2001 - val_loss: 0.2011\n",
      "Epoch 178/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1999 - val_loss: 0.2053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1991 - val_loss: 0.2001\n",
      "Epoch 180/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1996 - val_loss: 0.1987\n",
      "Epoch 181/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1958 - val_loss: 0.2006\n",
      "Epoch 182/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1964 - val_loss: 0.2015\n",
      "Epoch 183/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1994 - val_loss: 0.1990\n",
      "Epoch 184/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1953 - val_loss: 0.1962\n",
      "Epoch 185/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1937 - val_loss: 0.1986\n",
      "Epoch 186/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1973 - val_loss: 0.2029\n",
      "Epoch 187/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1979 - val_loss: 0.1954\n",
      "Epoch 188/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1931 - val_loss: 0.1955\n",
      "Epoch 189/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1905 - val_loss: 0.1985\n",
      "Epoch 190/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1915 - val_loss: 0.1919\n",
      "Epoch 191/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1898 - val_loss: 0.1933\n",
      "Epoch 192/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1876 - val_loss: 0.1974\n",
      "Epoch 193/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1902 - val_loss: 0.1931\n",
      "Epoch 194/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1902 - val_loss: 0.1929\n",
      "Epoch 195/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1888 - val_loss: 0.1948\n",
      "Epoch 196/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1875 - val_loss: 0.1906\n",
      "Epoch 197/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1890 - val_loss: 0.1933\n",
      "Epoch 198/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1907 - val_loss: 0.1913\n",
      "Epoch 199/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1861 - val_loss: 0.1893\n",
      "Epoch 200/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1863 - val_loss: 0.1896\n",
      "Epoch 201/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1841 - val_loss: 0.1911\n",
      "Epoch 202/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1848 - val_loss: 0.1882\n",
      "Epoch 203/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1825 - val_loss: 0.1907\n",
      "Epoch 204/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1843 - val_loss: 0.1910\n",
      "Epoch 205/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1867 - val_loss: 0.1890\n",
      "Epoch 206/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1849 - val_loss: 0.1917\n",
      "Epoch 207/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1843 - val_loss: 0.1831\n",
      "Epoch 208/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.1772 - val_loss: 0.1834\n",
      "Epoch 209/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1798 - val_loss: 0.1884\n",
      "Epoch 210/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1841 - val_loss: 0.1869\n",
      "Epoch 211/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1798 - val_loss: 0.1811\n",
      "Epoch 212/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1773 - val_loss: 0.1840\n",
      "Epoch 213/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1784 - val_loss: 0.1928\n",
      "Epoch 214/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1820 - val_loss: 0.1817\n",
      "Epoch 215/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1775 - val_loss: 0.1812\n",
      "Epoch 216/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1785 - val_loss: 0.1812\n",
      "Epoch 217/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1727 - val_loss: 0.1822\n",
      "Epoch 218/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1756 - val_loss: 0.1806\n",
      "Epoch 219/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1729 - val_loss: 0.1765\n",
      "Epoch 220/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1728 - val_loss: 0.1785\n",
      "Epoch 221/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1734 - val_loss: 0.1806\n",
      "Epoch 222/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1706 - val_loss: 0.1781\n",
      "Epoch 223/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1744 - val_loss: 0.1789\n",
      "Epoch 224/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1743 - val_loss: 0.1878\n",
      "Epoch 225/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1768 - val_loss: 0.1786\n",
      "Epoch 226/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1721 - val_loss: 0.1735\n",
      "Epoch 227/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1674 - val_loss: 0.1779\n",
      "Epoch 228/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1683 - val_loss: 0.1779\n",
      "Epoch 229/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1717 - val_loss: 0.1771\n",
      "Epoch 230/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1729 - val_loss: 0.1780\n",
      "Epoch 231/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1711 - val_loss: 0.1791\n",
      "Epoch 232/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1662 - val_loss: 0.1695\n",
      "Epoch 233/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1633 - val_loss: 0.1708\n",
      "Epoch 234/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1624 - val_loss: 0.1730\n",
      "Epoch 235/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1633 - val_loss: 0.1707\n",
      "Epoch 236/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1643 - val_loss: 0.1730\n",
      "Epoch 237/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1637 - val_loss: 0.1700\n",
      "Epoch 238/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1644 - val_loss: 0.1692\n",
      "Epoch 239/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1588 - val_loss: 0.1677\n",
      "Epoch 240/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1591 - val_loss: 0.1675\n",
      "Epoch 241/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1600 - val_loss: 0.1692\n",
      "Epoch 242/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1590 - val_loss: 0.1646\n",
      "Epoch 243/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1576 - val_loss: 0.1644\n",
      "Epoch 244/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1569 - val_loss: 0.1698\n",
      "Epoch 245/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1593 - val_loss: 0.1644\n",
      "Epoch 246/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1571 - val_loss: 0.1651\n",
      "Epoch 247/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1534 - val_loss: 0.1664\n",
      "Epoch 248/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1572 - val_loss: 0.1634\n",
      "Epoch 249/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1584 - val_loss: 0.1652\n",
      "Epoch 250/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.1541 - val_loss: 0.1636\n",
      "Epoch 251/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1543 - val_loss: 0.1589\n",
      "Epoch 252/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1525 - val_loss: 0.1630\n",
      "Epoch 253/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1513 - val_loss: 0.1616\n",
      "Epoch 254/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1548 - val_loss: 0.1652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1558 - val_loss: 0.1653\n",
      "Epoch 256/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1577 - val_loss: 0.1626\n",
      "Epoch 257/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1547 - val_loss: 0.1605\n",
      "Epoch 258/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1489 - val_loss: 0.1554\n",
      "Epoch 259/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1450 - val_loss: 0.1536\n",
      "Epoch 260/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1471 - val_loss: 0.1623\n",
      "Epoch 261/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1530 - val_loss: 0.1613\n",
      "Epoch 262/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.1547 - val_loss: 0.1546\n",
      "Epoch 263/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1439 - val_loss: 0.1540\n",
      "Epoch 264/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1444 - val_loss: 0.1518\n",
      "Epoch 265/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1441 - val_loss: 0.1508\n",
      "Epoch 266/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1429 - val_loss: 0.1548\n",
      "Epoch 267/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1472 - val_loss: 0.1479\n",
      "Epoch 268/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1394 - val_loss: 0.1466\n",
      "Epoch 269/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.1427 - val_loss: 0.1477\n",
      "Epoch 270/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1385 - val_loss: 0.1472\n",
      "Epoch 271/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1422 - val_loss: 0.1480\n",
      "Epoch 272/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1417 - val_loss: 0.1516\n",
      "Epoch 273/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1413 - val_loss: 0.1441\n",
      "Epoch 274/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1382 - val_loss: 0.1511\n",
      "Epoch 275/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1410 - val_loss: 0.1460\n",
      "Epoch 276/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1365 - val_loss: 0.1443\n",
      "Epoch 277/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1392 - val_loss: 0.1495\n",
      "Epoch 278/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1358 - val_loss: 0.1440\n",
      "Epoch 279/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1377 - val_loss: 0.1418\n",
      "Epoch 280/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1360 - val_loss: 0.1459\n",
      "Epoch 281/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1338 - val_loss: 0.1400\n",
      "Epoch 282/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1353 - val_loss: 0.1408\n",
      "Epoch 283/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1341 - val_loss: 0.1397\n",
      "Epoch 284/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1316 - val_loss: 0.1378\n",
      "Epoch 285/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1317 - val_loss: 0.1392\n",
      "Epoch 286/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1313 - val_loss: 0.1407\n",
      "Epoch 287/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1325 - val_loss: 0.1353\n",
      "Epoch 288/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1336 - val_loss: 0.1387\n",
      "Epoch 289/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1316 - val_loss: 0.1371\n",
      "Epoch 290/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1321 - val_loss: 0.1367\n",
      "Epoch 291/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1298 - val_loss: 0.1360\n",
      "Epoch 292/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1274 - val_loss: 0.1363\n",
      "Epoch 293/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1274 - val_loss: 0.1324\n",
      "Epoch 294/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1293 - val_loss: 0.1428\n",
      "Epoch 295/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.1336 - val_loss: 0.1383\n",
      "Epoch 296/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1280 - val_loss: 0.1307\n",
      "Epoch 297/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1268 - val_loss: 0.1316\n",
      "Epoch 298/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1279 - val_loss: 0.1294\n",
      "Epoch 299/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1229 - val_loss: 0.1299\n",
      "Epoch 300/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1257 - val_loss: 0.1291\n",
      "Epoch 301/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1246 - val_loss: 0.1249\n",
      "Epoch 302/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1204 - val_loss: 0.1277\n",
      "Epoch 303/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1232 - val_loss: 0.1283\n",
      "Epoch 304/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1223 - val_loss: 0.1288\n",
      "Epoch 305/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1227 - val_loss: 0.1232\n",
      "Epoch 306/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1222 - val_loss: 0.1258\n",
      "Epoch 307/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1226 - val_loss: 0.1283\n",
      "Epoch 308/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1223 - val_loss: 0.1283\n",
      "Epoch 309/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1273 - val_loss: 0.1303\n",
      "Epoch 310/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1252 - val_loss: 0.1301\n",
      "Epoch 311/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1286 - val_loss: 0.1197\n",
      "Epoch 312/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1176 - val_loss: 0.1204\n",
      "Epoch 313/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1185 - val_loss: 0.1206\n",
      "Epoch 314/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1193 - val_loss: 0.1205\n",
      "Epoch 315/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1142 - val_loss: 0.1182\n",
      "Epoch 316/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1168 - val_loss: 0.1166\n",
      "Epoch 317/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1138 - val_loss: 0.1204\n",
      "Epoch 318/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1165 - val_loss: 0.1220\n",
      "Epoch 319/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1185 - val_loss: 0.1227\n",
      "Epoch 320/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1189 - val_loss: 0.1222\n",
      "Epoch 321/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1187 - val_loss: 0.1161\n",
      "Epoch 322/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1142 - val_loss: 0.1159\n",
      "Epoch 323/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1132 - val_loss: 0.1138\n",
      "Epoch 324/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1143 - val_loss: 0.1168\n",
      "Epoch 325/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1149 - val_loss: 0.1217\n",
      "Epoch 326/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1218 - val_loss: 0.1192\n",
      "Epoch 327/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1159 - val_loss: 0.1110\n",
      "Epoch 328/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.1138 - val_loss: 0.1098\n",
      "Epoch 329/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1107 - val_loss: 0.1144\n",
      "Epoch 330/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1125 - val_loss: 0.1147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1118 - val_loss: 0.1163\n",
      "Epoch 332/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1165 - val_loss: 0.1122\n",
      "Epoch 333/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1123 - val_loss: 0.1123\n",
      "Epoch 334/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1086 - val_loss: 0.1098\n",
      "Epoch 335/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1090 - val_loss: 0.1113\n",
      "Epoch 336/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1117 - val_loss: 0.1157\n",
      "Epoch 337/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.1117 - val_loss: 0.1079\n",
      "Epoch 338/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1056 - val_loss: 0.1059\n",
      "Epoch 339/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1050 - val_loss: 0.1090\n",
      "Epoch 340/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1055 - val_loss: 0.1057\n",
      "Epoch 341/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1050 - val_loss: 0.1069\n",
      "Epoch 342/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1025 - val_loss: 0.1046\n",
      "Epoch 343/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1033 - val_loss: 0.1023\n",
      "Epoch 344/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1021 - val_loss: 0.1071\n",
      "Epoch 345/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1034 - val_loss: 0.1059\n",
      "Epoch 346/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1049 - val_loss: 0.1038\n",
      "Epoch 347/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1041 - val_loss: 0.1107\n",
      "Epoch 348/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1103 - val_loss: 0.1067\n",
      "Epoch 349/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1082 - val_loss: 0.1022\n",
      "Epoch 350/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1011 - val_loss: 0.1051\n",
      "Epoch 351/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1029 - val_loss: 0.1006\n",
      "Epoch 352/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1013 - val_loss: 0.1027\n",
      "Epoch 353/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1007 - val_loss: 0.1092\n",
      "Epoch 354/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1062 - val_loss: 0.1021\n",
      "Epoch 355/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1036 - val_loss: 0.1021\n",
      "Epoch 356/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1019 - val_loss: 0.1050\n",
      "Epoch 357/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.1020 - val_loss: 0.1025\n",
      "Epoch 358/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1027 - val_loss: 0.1064\n",
      "Epoch 359/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1051 - val_loss: 0.1055\n",
      "Epoch 360/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1041 - val_loss: 0.0995\n",
      "Epoch 361/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0968 - val_loss: 0.1045\n",
      "Epoch 362/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1071 - val_loss: 0.1056\n",
      "Epoch 363/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1050 - val_loss: 0.1068\n",
      "Epoch 364/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1045 - val_loss: 0.0971\n",
      "Epoch 365/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0965 - val_loss: 0.1015\n",
      "Epoch 366/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1007 - val_loss: 0.1046\n",
      "Epoch 367/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1026 - val_loss: 0.0985\n",
      "Epoch 368/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0967 - val_loss: 0.0984\n",
      "Epoch 369/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0995 - val_loss: 0.1059\n",
      "Epoch 370/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1004 - val_loss: 0.1007\n",
      "Epoch 371/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1004 - val_loss: 0.0968\n",
      "Epoch 372/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0991 - val_loss: 0.0958\n",
      "Epoch 373/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0929 - val_loss: 0.0967\n",
      "Epoch 374/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0919 - val_loss: 0.0948\n",
      "Epoch 375/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0978 - val_loss: 0.0962\n",
      "Epoch 376/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0957 - val_loss: 0.0950\n",
      "Epoch 377/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0915 - val_loss: 0.0944\n",
      "Epoch 378/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0925 - val_loss: 0.0950\n",
      "Epoch 379/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0943 - val_loss: 0.0967\n",
      "Epoch 380/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0947 - val_loss: 0.0962\n",
      "Epoch 381/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0943 - val_loss: 0.0928\n",
      "Epoch 382/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0913 - val_loss: 0.0928\n",
      "Epoch 383/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0933 - val_loss: 0.0919\n",
      "Epoch 384/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0920 - val_loss: 0.0914\n",
      "Epoch 385/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0902 - val_loss: 0.0948\n",
      "Epoch 386/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0939 - val_loss: 0.0917\n",
      "Epoch 387/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0896 - val_loss: 0.0910\n",
      "Epoch 388/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0889 - val_loss: 0.0922\n",
      "Epoch 389/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0918 - val_loss: 0.0910\n",
      "Epoch 390/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0896 - val_loss: 0.0934\n",
      "Epoch 391/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0891 - val_loss: 0.0902\n",
      "Epoch 392/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0893 - val_loss: 0.0899\n",
      "Epoch 393/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0896 - val_loss: 0.0953\n",
      "Epoch 394/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0909 - val_loss: 0.0903\n",
      "Epoch 395/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0879 - val_loss: 0.0898\n",
      "Epoch 396/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0897 - val_loss: 0.0877\n",
      "Epoch 397/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0879 - val_loss: 0.0886\n",
      "Epoch 398/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0862 - val_loss: 0.0894\n",
      "Epoch 399/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0880 - val_loss: 0.0866\n",
      "Epoch 400/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0869 - val_loss: 0.0923\n",
      "Epoch 401/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0895 - val_loss: 0.0923\n",
      "Epoch 402/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0932 - val_loss: 0.0929\n",
      "Epoch 403/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0942 - val_loss: 0.0909\n",
      "Epoch 404/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0898 - val_loss: 0.0869\n",
      "Epoch 405/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0894 - val_loss: 0.0916\n",
      "Epoch 406/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0901 - val_loss: 0.0900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0890 - val_loss: 0.0946\n",
      "Epoch 408/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0918 - val_loss: 0.0957\n",
      "Epoch 409/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0906 - val_loss: 0.0906\n",
      "Epoch 410/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0877 - val_loss: 0.0886\n",
      "Epoch 411/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0854 - val_loss: 0.0872\n",
      "Epoch 412/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0838 - val_loss: 0.0890\n",
      "Epoch 413/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0873 - val_loss: 0.0916\n",
      "Epoch 414/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0881 - val_loss: 0.0885\n",
      "Epoch 415/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0865 - val_loss: 0.0893\n",
      "Epoch 416/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0875 - val_loss: 0.0909\n",
      "Epoch 417/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0883 - val_loss: 0.0850\n",
      "Epoch 418/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0858 - val_loss: 0.0870\n",
      "Epoch 419/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0863 - val_loss: 0.0843\n",
      "Epoch 420/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0828 - val_loss: 0.0846\n",
      "Epoch 421/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0838 - val_loss: 0.0884\n",
      "Epoch 422/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0854 - val_loss: 0.0863\n",
      "Epoch 423/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0853 - val_loss: 0.0851\n",
      "Epoch 424/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0832 - val_loss: 0.0829\n",
      "Epoch 425/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0807 - val_loss: 0.0871\n",
      "Epoch 426/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0835 - val_loss: 0.0823\n",
      "Epoch 427/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0769 - val_loss: 0.0824\n",
      "Epoch 428/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0812 - val_loss: 0.0889\n",
      "Epoch 429/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0832 - val_loss: 0.0815\n",
      "Epoch 430/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0783 - val_loss: 0.0840\n",
      "Epoch 431/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0820 - val_loss: 0.0841\n",
      "Epoch 432/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0829 - val_loss: 0.0827\n",
      "Epoch 433/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0803 - val_loss: 0.0856\n",
      "Epoch 434/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0806 - val_loss: 0.0826\n",
      "Epoch 435/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0831 - val_loss: 0.0813\n",
      "Epoch 436/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0798 - val_loss: 0.0822\n",
      "Epoch 437/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0796 - val_loss: 0.0835\n",
      "Epoch 438/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0786 - val_loss: 0.0838\n",
      "Epoch 439/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0800 - val_loss: 0.0876\n",
      "Epoch 440/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0865 - val_loss: 0.0879\n",
      "Epoch 441/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0878 - val_loss: 0.0841\n",
      "Epoch 442/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 443/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0803 - val_loss: 0.0806\n",
      "Epoch 444/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0775 - val_loss: 0.0802\n",
      "Epoch 445/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0775 - val_loss: 0.0802\n",
      "Epoch 446/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0766 - val_loss: 0.0792\n",
      "Epoch 447/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0783 - val_loss: 0.0825\n",
      "Epoch 448/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0784 - val_loss: 0.0836\n",
      "Epoch 449/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0810 - val_loss: 0.0831\n",
      "Epoch 450/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0805 - val_loss: 0.0836\n",
      "Epoch 451/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0775 - val_loss: 0.0788\n",
      "Epoch 452/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0766 - val_loss: 0.0806\n",
      "Epoch 453/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0781 - val_loss: 0.0799\n",
      "Epoch 454/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0768 - val_loss: 0.0847\n",
      "Epoch 455/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0779 - val_loss: 0.0797\n",
      "Epoch 456/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0767 - val_loss: 0.0812\n",
      "Epoch 457/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0767 - val_loss: 0.0825\n",
      "Epoch 458/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0785 - val_loss: 0.0784\n",
      "Epoch 459/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0789 - val_loss: 0.0804\n",
      "Epoch 460/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0780 - val_loss: 0.0804\n",
      "Epoch 461/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0742 - val_loss: 0.0761\n",
      "Epoch 462/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0745 - val_loss: 0.0788\n",
      "Epoch 463/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0794 - val_loss: 0.0794\n",
      "Epoch 464/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0767 - val_loss: 0.0779\n",
      "Epoch 465/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0770 - val_loss: 0.0813\n",
      "Epoch 466/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0799 - val_loss: 0.0787\n",
      "Epoch 467/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0765 - val_loss: 0.0767\n",
      "Epoch 468/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0734 - val_loss: 0.0800\n",
      "Epoch 469/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0737 - val_loss: 0.0769\n",
      "Epoch 470/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0740 - val_loss: 0.0807\n",
      "Epoch 471/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0751 - val_loss: 0.0765\n",
      "Epoch 472/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0763 - val_loss: 0.0798\n",
      "Epoch 473/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0773 - val_loss: 0.0808\n",
      "Epoch 474/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0754 - val_loss: 0.0766\n",
      "Epoch 475/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0725 - val_loss: 0.0798\n",
      "Epoch 476/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0768 - val_loss: 0.0798\n",
      "Epoch 477/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0746 - val_loss: 0.0756\n",
      "Epoch 478/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0708 - val_loss: 0.0766\n",
      "Epoch 479/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0747 - val_loss: 0.0784\n",
      "Epoch 480/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0772 - val_loss: 0.0799\n",
      "Epoch 481/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0745 - val_loss: 0.0750\n",
      "Epoch 482/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0736 - val_loss: 0.0780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 483/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0729 - val_loss: 0.0763\n",
      "Epoch 484/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0723 - val_loss: 0.0777\n",
      "Epoch 485/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0769 - val_loss: 0.0775\n",
      "Epoch 486/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0754 - val_loss: 0.0775\n",
      "Epoch 487/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0755 - val_loss: 0.0751\n",
      "Epoch 488/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0717 - val_loss: 0.0782\n",
      "Epoch 489/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0744 - val_loss: 0.0757\n",
      "Epoch 490/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0719 - val_loss: 0.0770\n",
      "Epoch 491/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0774 - val_loss: 0.0793\n",
      "Epoch 492/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0729 - val_loss: 0.0748\n",
      "Epoch 493/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0718 - val_loss: 0.0754\n",
      "Epoch 494/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0713 - val_loss: 0.0749\n",
      "Epoch 495/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0729 - val_loss: 0.0747\n",
      "Epoch 496/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0732 - val_loss: 0.0800\n",
      "Epoch 497/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0756 - val_loss: 0.0744\n",
      "Epoch 498/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0712 - val_loss: 0.0756\n",
      "Epoch 499/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0716 - val_loss: 0.0767\n",
      "Epoch 500/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0716 - val_loss: 0.0739\n",
      "Epoch 501/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0712 - val_loss: 0.0766\n",
      "Epoch 502/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0708 - val_loss: 0.0734\n",
      "Epoch 503/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0717 - val_loss: 0.0733\n",
      "Epoch 504/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0693 - val_loss: 0.0754\n",
      "Epoch 505/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0718 - val_loss: 0.0726\n",
      "Epoch 506/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0701 - val_loss: 0.0717\n",
      "Epoch 507/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0672 - val_loss: 0.0740\n",
      "Epoch 508/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0686 - val_loss: 0.0722\n",
      "Epoch 509/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0697 - val_loss: 0.0737\n",
      "Epoch 510/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0723 - val_loss: 0.0733\n",
      "Epoch 511/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0698 - val_loss: 0.0744\n",
      "Epoch 512/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0684 - val_loss: 0.0721\n",
      "Epoch 513/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0692 - val_loss: 0.0721\n",
      "Epoch 514/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0707 - val_loss: 0.0777\n",
      "Epoch 515/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0721 - val_loss: 0.0730\n",
      "Epoch 516/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0710 - val_loss: 0.0711\n",
      "Epoch 517/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0688 - val_loss: 0.0720\n",
      "Epoch 518/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0666 - val_loss: 0.0723\n",
      "Epoch 519/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0708 - val_loss: 0.0764\n",
      "Epoch 520/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0712 - val_loss: 0.0713\n",
      "Epoch 521/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0693 - val_loss: 0.0734\n",
      "Epoch 522/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0688 - val_loss: 0.0722\n",
      "Epoch 523/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0665 - val_loss: 0.0702\n",
      "Epoch 524/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0678 - val_loss: 0.0718\n",
      "Epoch 525/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0686 - val_loss: 0.0696\n",
      "Epoch 526/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0654 - val_loss: 0.0711\n",
      "Epoch 527/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0650 - val_loss: 0.0723\n",
      "Epoch 528/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0680 - val_loss: 0.0706\n",
      "Epoch 529/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0670 - val_loss: 0.0716\n",
      "Epoch 530/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0657 - val_loss: 0.0700\n",
      "Epoch 531/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0664 - val_loss: 0.0692\n",
      "Epoch 532/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0653 - val_loss: 0.0699\n",
      "Epoch 533/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0645 - val_loss: 0.0700\n",
      "Epoch 534/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0659 - val_loss: 0.0701\n",
      "Epoch 535/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0677 - val_loss: 0.0720\n",
      "Epoch 536/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0689 - val_loss: 0.0734\n",
      "Epoch 537/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0683 - val_loss: 0.0722\n",
      "Epoch 538/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0689 - val_loss: 0.0705\n",
      "Epoch 539/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0675 - val_loss: 0.0706\n",
      "Epoch 540/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0668 - val_loss: 0.0720\n",
      "Epoch 541/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0673 - val_loss: 0.0702\n",
      "Epoch 542/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0677 - val_loss: 0.0709\n",
      "Epoch 543/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0651 - val_loss: 0.0723\n",
      "Epoch 544/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0640 - val_loss: 0.0705\n",
      "Epoch 545/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0671 - val_loss: 0.0705\n",
      "Epoch 546/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0662 - val_loss: 0.0727\n",
      "Epoch 547/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0663 - val_loss: 0.0704\n",
      "Epoch 548/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0668 - val_loss: 0.0742\n",
      "Epoch 549/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0700 - val_loss: 0.0706\n",
      "Epoch 550/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0679 - val_loss: 0.0702\n",
      "Epoch 551/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0656 - val_loss: 0.0688\n",
      "Epoch 552/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0668 - val_loss: 0.0683\n",
      "Epoch 553/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0649 - val_loss: 0.0708\n",
      "Epoch 554/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0649 - val_loss: 0.0696\n",
      "Epoch 555/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0668 - val_loss: 0.0733\n",
      "Epoch 556/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0688 - val_loss: 0.0693\n",
      "Epoch 557/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0649 - val_loss: 0.0685\n",
      "Epoch 558/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0662 - val_loss: 0.0687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 559/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0624 - val_loss: 0.0671\n",
      "Epoch 560/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0641 - val_loss: 0.0674\n",
      "Epoch 561/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0618 - val_loss: 0.0697\n",
      "Epoch 562/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0629 - val_loss: 0.0689\n",
      "Epoch 563/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0648 - val_loss: 0.0717\n",
      "Epoch 564/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0661 - val_loss: 0.0671\n",
      "Epoch 565/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0631 - val_loss: 0.0703\n",
      "Epoch 566/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0632 - val_loss: 0.0685\n",
      "Epoch 567/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0636 - val_loss: 0.0661\n",
      "Epoch 568/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0628 - val_loss: 0.0702\n",
      "Epoch 569/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0629 - val_loss: 0.0675\n",
      "Epoch 570/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0613 - val_loss: 0.0694\n",
      "Epoch 571/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0629 - val_loss: 0.0668\n",
      "Epoch 572/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0610 - val_loss: 0.0678\n",
      "Epoch 573/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0602 - val_loss: 0.0674\n",
      "Epoch 574/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0635 - val_loss: 0.0675\n",
      "Epoch 575/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0634 - val_loss: 0.0687\n",
      "Epoch 576/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0613 - val_loss: 0.0676\n",
      "Epoch 577/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0617 - val_loss: 0.0667\n",
      "Epoch 578/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0633 - val_loss: 0.0691\n",
      "Epoch 579/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0613 - val_loss: 0.0695\n",
      "Epoch 580/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0631 - val_loss: 0.0706\n",
      "Epoch 581/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0652 - val_loss: 0.0671\n",
      "Epoch 582/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0619 - val_loss: 0.0677\n",
      "Epoch 583/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0623 - val_loss: 0.0691\n",
      "Epoch 584/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0583 - val_loss: 0.0671\n",
      "Epoch 585/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0628 - val_loss: 0.0686\n",
      "Epoch 586/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0620 - val_loss: 0.0683\n",
      "Epoch 587/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0620 - val_loss: 0.0677\n",
      "Epoch 588/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0625 - val_loss: 0.0699\n",
      "Epoch 589/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0634 - val_loss: 0.0688\n",
      "Epoch 590/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0648 - val_loss: 0.0683\n",
      "Epoch 591/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0620 - val_loss: 0.0644\n",
      "Epoch 592/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0593 - val_loss: 0.0665\n",
      "Epoch 593/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0610 - val_loss: 0.0689\n",
      "Epoch 594/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0627 - val_loss: 0.0678\n",
      "Epoch 595/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0650 - val_loss: 0.0683\n",
      "Epoch 596/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0625 - val_loss: 0.0662\n",
      "Epoch 597/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0603 - val_loss: 0.0632\n",
      "Epoch 598/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0595 - val_loss: 0.0663\n",
      "Epoch 599/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0598 - val_loss: 0.0659\n",
      "Epoch 600/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0622 - val_loss: 0.0683\n",
      "Epoch 601/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0618 - val_loss: 0.0656\n",
      "Epoch 602/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0605 - val_loss: 0.0663\n",
      "Epoch 603/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0612 - val_loss: 0.0680\n",
      "Epoch 604/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0596 - val_loss: 0.0651\n",
      "Epoch 605/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0623 - val_loss: 0.0677\n",
      "Epoch 606/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0620 - val_loss: 0.0635\n",
      "Epoch 607/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0588 - val_loss: 0.0650\n",
      "Epoch 608/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0594 - val_loss: 0.0651\n",
      "Epoch 609/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0587 - val_loss: 0.0639\n",
      "Epoch 610/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0603 - val_loss: 0.0691\n",
      "Epoch 611/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0597 - val_loss: 0.0644\n",
      "Epoch 612/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0576 - val_loss: 0.0646\n",
      "Epoch 613/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0590 - val_loss: 0.0641\n",
      "Epoch 614/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0595 - val_loss: 0.0635\n",
      "Epoch 615/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0577 - val_loss: 0.0661\n",
      "Epoch 616/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0604 - val_loss: 0.0649\n",
      "Epoch 617/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0592 - val_loss: 0.0661\n",
      "Epoch 618/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0617 - val_loss: 0.0663\n",
      "Epoch 619/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0609 - val_loss: 0.0641\n",
      "Epoch 620/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0570 - val_loss: 0.0644\n",
      "Epoch 621/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0577 - val_loss: 0.0655\n",
      "Epoch 622/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0578 - val_loss: 0.0650\n",
      "Epoch 623/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0572 - val_loss: 0.0627\n",
      "Epoch 624/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0579 - val_loss: 0.0666\n",
      "Epoch 625/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0588 - val_loss: 0.0628\n",
      "Epoch 626/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0588 - val_loss: 0.0642\n",
      "Epoch 627/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0593 - val_loss: 0.0672\n",
      "Epoch 628/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0597 - val_loss: 0.0633\n",
      "Epoch 629/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0578 - val_loss: 0.0664\n",
      "Epoch 630/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0582 - val_loss: 0.0637\n",
      "Epoch 631/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0585 - val_loss: 0.0620\n",
      "Epoch 632/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0570 - val_loss: 0.0638\n",
      "Epoch 633/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0579 - val_loss: 0.0625\n",
      "Epoch 634/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0577 - val_loss: 0.0659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 635/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0567 - val_loss: 0.0629\n",
      "Epoch 636/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0557 - val_loss: 0.0651\n",
      "Epoch 637/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0585 - val_loss: 0.0673\n",
      "Epoch 638/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0598 - val_loss: 0.0686\n",
      "Epoch 639/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0642 - val_loss: 0.0689\n",
      "Epoch 640/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0630 - val_loss: 0.0645\n",
      "Epoch 641/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0582 - val_loss: 0.0653\n",
      "Epoch 642/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0579 - val_loss: 0.0638\n",
      "Epoch 643/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0560 - val_loss: 0.0635\n",
      "Epoch 644/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0588 - val_loss: 0.0649\n",
      "Epoch 645/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0560 - val_loss: 0.0631\n",
      "Epoch 646/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0566 - val_loss: 0.0655\n",
      "Epoch 647/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0605 - val_loss: 0.0636\n",
      "Epoch 648/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0560 - val_loss: 0.0638\n",
      "Epoch 649/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0560 - val_loss: 0.0626\n",
      "Epoch 650/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0573 - val_loss: 0.0645\n",
      "Epoch 651/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0555 - val_loss: 0.0617\n",
      "Epoch 652/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0575 - val_loss: 0.0619\n",
      "Epoch 653/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0565 - val_loss: 0.0642\n",
      "Epoch 654/1000\n",
      "3298/3298 [==============================] - 1s 319us/step - loss: 0.0566 - val_loss: 0.0615\n",
      "Epoch 655/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0567 - val_loss: 0.0654\n",
      "Epoch 656/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0563 - val_loss: 0.0642\n",
      "Epoch 657/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0586 - val_loss: 0.0640\n",
      "Epoch 658/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0554 - val_loss: 0.0612\n",
      "Epoch 659/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0559 - val_loss: 0.0658\n",
      "Epoch 660/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0573 - val_loss: 0.0630\n",
      "Epoch 661/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0561 - val_loss: 0.0629\n",
      "Epoch 662/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0544 - val_loss: 0.0631\n",
      "Epoch 663/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0570 - val_loss: 0.0635\n",
      "Epoch 664/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0561 - val_loss: 0.0647\n",
      "Epoch 665/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0570 - val_loss: 0.0643\n",
      "Epoch 666/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0593 - val_loss: 0.0647\n",
      "Epoch 667/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0549 - val_loss: 0.0610\n",
      "Epoch 668/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0527 - val_loss: 0.0613\n",
      "Epoch 669/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0560 - val_loss: 0.0643\n",
      "Epoch 670/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0570 - val_loss: 0.0617\n",
      "Epoch 671/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0553 - val_loss: 0.0650\n",
      "Epoch 672/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0556 - val_loss: 0.0641\n",
      "Epoch 673/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0569 - val_loss: 0.0651\n",
      "Epoch 674/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0596 - val_loss: 0.0683\n",
      "Epoch 675/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0604 - val_loss: 0.0642\n",
      "Epoch 676/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0585 - val_loss: 0.0610\n",
      "Epoch 677/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0550 - val_loss: 0.0637\n",
      "Epoch 678/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0542 - val_loss: 0.0609\n",
      "Epoch 679/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0564 - val_loss: 0.0637\n",
      "Epoch 680/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0565 - val_loss: 0.0657\n",
      "Epoch 681/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0590 - val_loss: 0.0627\n",
      "Epoch 682/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0568 - val_loss: 0.0634\n",
      "Epoch 683/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0558 - val_loss: 0.0606\n",
      "Epoch 684/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0554 - val_loss: 0.0631\n",
      "Epoch 685/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0558 - val_loss: 0.0652\n",
      "Epoch 686/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0582 - val_loss: 0.0656\n",
      "Epoch 687/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0584 - val_loss: 0.0635\n",
      "Epoch 688/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0553 - val_loss: 0.0612\n",
      "Epoch 689/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0525 - val_loss: 0.0641\n",
      "Epoch 690/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0545 - val_loss: 0.0699\n",
      "Epoch 691/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0644 - val_loss: 0.0661\n",
      "Epoch 692/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0595 - val_loss: 0.0625\n",
      "Epoch 693/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0546 - val_loss: 0.0636\n",
      "Epoch 694/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0548 - val_loss: 0.0678\n",
      "Epoch 695/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0610 - val_loss: 0.0667\n",
      "Epoch 696/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0590 - val_loss: 0.0618\n",
      "Epoch 697/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0536 - val_loss: 0.0635\n",
      "Epoch 698/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0559 - val_loss: 0.0617\n",
      "Epoch 699/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0532 - val_loss: 0.0655\n",
      "Epoch 700/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0585 - val_loss: 0.0649\n",
      "Epoch 701/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0563 - val_loss: 0.0600\n",
      "Epoch 702/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0549 - val_loss: 0.0612\n",
      "Epoch 703/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0512 - val_loss: 0.0608\n",
      "Epoch 704/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0515 - val_loss: 0.0617\n",
      "Epoch 705/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0546 - val_loss: 0.0631\n",
      "Epoch 706/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0554 - val_loss: 0.0615\n",
      "Epoch 707/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0521 - val_loss: 0.0599\n",
      "Epoch 708/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0545 - val_loss: 0.0624\n",
      "Epoch 709/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0506 - val_loss: 0.0630\n",
      "Epoch 710/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0548 - val_loss: 0.0617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 711/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0543 - val_loss: 0.0624\n",
      "Epoch 712/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0537 - val_loss: 0.0615\n",
      "Epoch 713/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0520 - val_loss: 0.0614\n",
      "Epoch 714/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0540 - val_loss: 0.0595\n",
      "Epoch 715/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0525 - val_loss: 0.0602\n",
      "Epoch 716/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0505 - val_loss: 0.0617\n",
      "Epoch 717/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0516 - val_loss: 0.0592\n",
      "Epoch 718/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0519 - val_loss: 0.0608\n",
      "Epoch 719/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0512 - val_loss: 0.0596\n",
      "Epoch 720/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0525 - val_loss: 0.0590\n",
      "Epoch 721/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.0524 - val_loss: 0.0623\n",
      "Epoch 722/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0507 - val_loss: 0.0594\n",
      "Epoch 723/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0518 - val_loss: 0.0599\n",
      "Epoch 724/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0521 - val_loss: 0.0577\n",
      "Epoch 725/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0507 - val_loss: 0.0576\n",
      "Epoch 726/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0506 - val_loss: 0.0622\n",
      "Epoch 727/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0518 - val_loss: 0.0600\n",
      "Epoch 728/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0520 - val_loss: 0.0600\n",
      "Epoch 729/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0506 - val_loss: 0.0593\n",
      "Epoch 730/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0495 - val_loss: 0.0585\n",
      "Epoch 731/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0493 - val_loss: 0.0580\n",
      "Epoch 732/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0504 - val_loss: 0.0596\n",
      "Epoch 733/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0499 - val_loss: 0.0584\n",
      "Epoch 734/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0502 - val_loss: 0.0593\n",
      "Epoch 735/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0492 - val_loss: 0.0591\n",
      "Epoch 736/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0532 - val_loss: 0.0620\n",
      "Epoch 737/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0540 - val_loss: 0.0640\n",
      "Epoch 738/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0552 - val_loss: 0.0608\n",
      "Epoch 739/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0531 - val_loss: 0.0581\n",
      "Epoch 740/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0502 - val_loss: 0.0586\n",
      "Epoch 741/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0499 - val_loss: 0.0604\n",
      "Epoch 742/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0508 - val_loss: 0.0638\n",
      "Epoch 743/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0549 - val_loss: 0.0625\n",
      "Epoch 744/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0541 - val_loss: 0.0595\n",
      "Epoch 745/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0522 - val_loss: 0.0585\n",
      "Epoch 746/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0500 - val_loss: 0.0590\n",
      "Epoch 747/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0490 - val_loss: 0.0588\n",
      "Epoch 748/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0505 - val_loss: 0.0614\n",
      "Epoch 749/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.0517 - val_loss: 0.0608\n",
      "Epoch 750/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0519 - val_loss: 0.0599\n",
      "Epoch 751/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0521 - val_loss: 0.0600\n",
      "Epoch 752/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0520 - val_loss: 0.0598\n",
      "Epoch 753/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0496 - val_loss: 0.0576\n",
      "Epoch 754/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0492 - val_loss: 0.0578\n",
      "Epoch 755/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0477 - val_loss: 0.0581\n",
      "Epoch 756/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0504 - val_loss: 0.0610\n",
      "Epoch 757/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0508 - val_loss: 0.0565\n",
      "Epoch 758/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0508 - val_loss: 0.0593\n",
      "Epoch 759/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0490 - val_loss: 0.0575\n",
      "Epoch 760/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0515 - val_loss: 0.0600\n",
      "Epoch 761/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0508 - val_loss: 0.0594\n",
      "Epoch 762/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0487 - val_loss: 0.0584\n",
      "Epoch 763/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0498 - val_loss: 0.0613\n",
      "Epoch 764/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0505 - val_loss: 0.0570\n",
      "Epoch 765/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0502 - val_loss: 0.0590\n",
      "Epoch 766/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0480 - val_loss: 0.0588\n",
      "Epoch 767/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0500 - val_loss: 0.0603\n",
      "Epoch 768/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0524 - val_loss: 0.0626\n",
      "Epoch 769/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0517 - val_loss: 0.0580\n",
      "Epoch 770/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0510 - val_loss: 0.0578\n",
      "Epoch 771/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0481 - val_loss: 0.0608\n",
      "Epoch 772/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0488 - val_loss: 0.0585\n",
      "Epoch 773/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0508 - val_loss: 0.0594\n",
      "Epoch 774/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0502 - val_loss: 0.0598\n",
      "Epoch 775/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0502 - val_loss: 0.0567\n",
      "Epoch 776/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0484 - val_loss: 0.0607\n",
      "Epoch 777/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0500 - val_loss: 0.0585\n",
      "Epoch 778/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0488 - val_loss: 0.0599\n",
      "Epoch 779/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0497 - val_loss: 0.0618\n",
      "Epoch 780/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0536 - val_loss: 0.0599\n",
      "Epoch 781/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0522 - val_loss: 0.0574\n",
      "Epoch 782/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0481 - val_loss: 0.0572\n",
      "Epoch 783/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0489 - val_loss: 0.0558\n",
      "Epoch 784/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0499 - val_loss: 0.0623\n",
      "Epoch 785/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0530 - val_loss: 0.0585\n",
      "Epoch 786/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0504 - val_loss: 0.0574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 787/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0498 - val_loss: 0.0635\n",
      "Epoch 788/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0499 - val_loss: 0.0570\n",
      "Epoch 789/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0500 - val_loss: 0.0578\n",
      "Epoch 790/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0489 - val_loss: 0.0587\n",
      "Epoch 791/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0468 - val_loss: 0.0583\n",
      "Epoch 792/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0499 - val_loss: 0.0615\n",
      "Epoch 793/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0499 - val_loss: 0.0575\n",
      "Epoch 794/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0495 - val_loss: 0.0578\n",
      "Epoch 795/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0480 - val_loss: 0.0600\n",
      "Epoch 796/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0490 - val_loss: 0.0582\n",
      "Epoch 797/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0514 - val_loss: 0.0605\n",
      "Epoch 798/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0498 - val_loss: 0.0604\n",
      "Epoch 799/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0498 - val_loss: 0.0556\n",
      "Epoch 800/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0503 - val_loss: 0.0593\n",
      "Epoch 801/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0480 - val_loss: 0.0584\n",
      "Epoch 802/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0480 - val_loss: 0.0561\n",
      "Epoch 803/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0479 - val_loss: 0.0618\n",
      "Epoch 804/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0499 - val_loss: 0.0553\n",
      "Epoch 805/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0474 - val_loss: 0.0578\n",
      "Epoch 806/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0468 - val_loss: 0.0600\n",
      "Epoch 807/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0489 - val_loss: 0.0572\n",
      "Epoch 808/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0502 - val_loss: 0.0599\n",
      "Epoch 809/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0476 - val_loss: 0.0577\n",
      "Epoch 810/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0476 - val_loss: 0.0555\n",
      "Epoch 811/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0464 - val_loss: 0.0601\n",
      "Epoch 812/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0482 - val_loss: 0.0565\n",
      "Epoch 813/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0485 - val_loss: 0.0580\n",
      "Epoch 814/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0461 - val_loss: 0.0591\n",
      "Epoch 815/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0477 - val_loss: 0.0555\n",
      "Epoch 816/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0481 - val_loss: 0.0609\n",
      "Epoch 817/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0472 - val_loss: 0.0584\n",
      "Epoch 818/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0477 - val_loss: 0.0557\n",
      "Epoch 819/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0468 - val_loss: 0.0604\n",
      "Epoch 820/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0483 - val_loss: 0.0577\n",
      "Epoch 821/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0483 - val_loss: 0.0589\n",
      "Epoch 822/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0476 - val_loss: 0.0602\n",
      "Epoch 823/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0495 - val_loss: 0.0561\n",
      "Epoch 824/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0460 - val_loss: 0.0572\n",
      "Epoch 825/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0463 - val_loss: 0.0563\n",
      "Epoch 826/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.0458 - val_loss: 0.0584\n",
      "Epoch 827/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0491 - val_loss: 0.0619\n",
      "Epoch 828/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0493 - val_loss: 0.0586\n",
      "Epoch 829/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0482 - val_loss: 0.0570\n",
      "Epoch 830/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0445 - val_loss: 0.0578\n",
      "Epoch 831/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0459 - val_loss: 0.0573\n",
      "Epoch 832/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0478 - val_loss: 0.0608\n",
      "Epoch 833/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0501 - val_loss: 0.0614\n",
      "Epoch 834/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0504 - val_loss: 0.0576\n",
      "Epoch 835/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0475 - val_loss: 0.0574\n",
      "Epoch 836/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0466 - val_loss: 0.0580\n",
      "Epoch 837/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0494 - val_loss: 0.0587\n",
      "Epoch 838/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0513 - val_loss: 0.0612\n",
      "Epoch 839/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0503 - val_loss: 0.0571\n",
      "Epoch 840/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0465 - val_loss: 0.0563\n",
      "Epoch 841/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0460 - val_loss: 0.0600\n",
      "Epoch 842/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0489 - val_loss: 0.0623\n",
      "Epoch 843/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0507 - val_loss: 0.0583\n",
      "Epoch 844/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0502 - val_loss: 0.0593\n",
      "Epoch 845/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0463 - val_loss: 0.0573\n",
      "Epoch 846/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0501 - val_loss: 0.0601\n",
      "Epoch 847/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0498 - val_loss: 0.0600\n",
      "Epoch 848/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0478 - val_loss: 0.0566\n",
      "Epoch 849/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0478 - val_loss: 0.0586\n",
      "Epoch 850/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0459 - val_loss: 0.0586\n",
      "Epoch 851/1000\n",
      "3298/3298 [==============================] - 1s 320us/step - loss: 0.0466 - val_loss: 0.0562\n",
      "Epoch 852/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0469 - val_loss: 0.0596\n",
      "Epoch 853/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0465 - val_loss: 0.0571\n",
      "Epoch 854/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0453 - val_loss: 0.0572\n",
      "Epoch 855/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0485 - val_loss: 0.0604\n",
      "Epoch 856/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0473 - val_loss: 0.0564\n",
      "Epoch 857/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0465 - val_loss: 0.0566\n",
      "Epoch 858/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0457 - val_loss: 0.0576\n",
      "Epoch 859/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0467 - val_loss: 0.0551\n",
      "Epoch 860/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0454 - val_loss: 0.0583\n",
      "Epoch 861/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0469 - val_loss: 0.0560\n",
      "Epoch 862/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0433 - val_loss: 0.0549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 863/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0464 - val_loss: 0.0574\n",
      "Epoch 864/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0442 - val_loss: 0.0570\n",
      "Epoch 865/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0428 - val_loss: 0.0564\n",
      "Epoch 866/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0450 - val_loss: 0.0565\n",
      "Epoch 867/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0466 - val_loss: 0.0574\n",
      "Epoch 868/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0455 - val_loss: 0.0568\n",
      "Epoch 869/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0452 - val_loss: 0.0563\n",
      "Epoch 870/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0452 - val_loss: 0.0562\n",
      "Epoch 871/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0439 - val_loss: 0.0567\n",
      "Epoch 872/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0429 - val_loss: 0.0550\n",
      "Epoch 873/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0434 - val_loss: 0.0558\n",
      "Epoch 874/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0446 - val_loss: 0.0550\n",
      "Epoch 875/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0431 - val_loss: 0.0553\n",
      "Epoch 876/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0455 - val_loss: 0.0561\n",
      "Epoch 877/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0451 - val_loss: 0.0548\n",
      "Epoch 878/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0436 - val_loss: 0.0562\n",
      "Epoch 879/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0454 - val_loss: 0.0579\n",
      "Epoch 880/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0455 - val_loss: 0.0555\n",
      "Epoch 881/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0435 - val_loss: 0.0564\n",
      "Epoch 882/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0438 - val_loss: 0.0564\n",
      "Epoch 883/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0445 - val_loss: 0.0560\n",
      "Epoch 884/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0430 - val_loss: 0.0537\n",
      "Epoch 885/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0444 - val_loss: 0.0571\n",
      "Epoch 886/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0434 - val_loss: 0.0550\n",
      "Epoch 887/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0429 - val_loss: 0.0545\n",
      "Epoch 888/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0428 - val_loss: 0.0560\n",
      "Epoch 889/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0454 - val_loss: 0.0541\n",
      "Epoch 890/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0441 - val_loss: 0.0549\n",
      "Epoch 891/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0435 - val_loss: 0.0549\n",
      "Epoch 892/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0418 - val_loss: 0.0549\n",
      "Epoch 893/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0429 - val_loss: 0.0573\n",
      "Epoch 894/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0442 - val_loss: 0.0549\n",
      "Epoch 895/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0435 - val_loss: 0.0558\n",
      "Epoch 896/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0446 - val_loss: 0.0557\n",
      "Epoch 897/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0449 - val_loss: 0.0540\n",
      "Epoch 898/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0437 - val_loss: 0.0554\n",
      "Epoch 899/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0441 - val_loss: 0.0542\n",
      "Epoch 900/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0443 - val_loss: 0.0569\n",
      "Epoch 901/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0445 - val_loss: 0.0532\n",
      "Epoch 902/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0408 - val_loss: 0.0532\n",
      "Epoch 903/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0419 - val_loss: 0.0554\n",
      "Epoch 904/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.0434 - val_loss: 0.0540\n",
      "Epoch 905/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0424 - val_loss: 0.0558\n",
      "Epoch 906/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0418 - val_loss: 0.0546\n",
      "Epoch 907/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0443 - val_loss: 0.0548\n",
      "Epoch 908/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0423 - val_loss: 0.0576\n",
      "Epoch 909/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0452 - val_loss: 0.0568\n",
      "Epoch 910/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0444 - val_loss: 0.0546\n",
      "Epoch 911/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0447 - val_loss: 0.0567\n",
      "Epoch 912/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0438 - val_loss: 0.0541\n",
      "Epoch 913/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0432 - val_loss: 0.0564\n",
      "Epoch 914/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0438 - val_loss: 0.0555\n",
      "Epoch 915/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0433 - val_loss: 0.0572\n",
      "Epoch 916/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0452 - val_loss: 0.0586\n",
      "Epoch 917/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0451 - val_loss: 0.0553\n",
      "Epoch 918/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0444 - val_loss: 0.0553\n",
      "Epoch 919/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0424 - val_loss: 0.0556\n",
      "Epoch 920/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0426 - val_loss: 0.0545\n",
      "Epoch 921/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0424 - val_loss: 0.0572\n",
      "Epoch 922/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0439 - val_loss: 0.0554\n",
      "Epoch 923/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0427 - val_loss: 0.0581\n",
      "Epoch 924/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0465 - val_loss: 0.0565\n",
      "Epoch 925/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0464 - val_loss: 0.0558\n",
      "Epoch 926/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0448 - val_loss: 0.0530\n",
      "Epoch 927/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0424 - val_loss: 0.0566\n",
      "Epoch 928/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0438 - val_loss: 0.0532\n",
      "Epoch 929/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0432 - val_loss: 0.0562\n",
      "Epoch 930/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0410 - val_loss: 0.0579\n",
      "Epoch 931/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0426 - val_loss: 0.0545\n",
      "Epoch 932/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0448 - val_loss: 0.0573\n",
      "Epoch 933/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0418 - val_loss: 0.0541\n",
      "Epoch 934/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0429 - val_loss: 0.0547\n",
      "Epoch 935/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0421 - val_loss: 0.0560\n",
      "Epoch 936/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0422 - val_loss: 0.0540\n",
      "Epoch 937/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0429 - val_loss: 0.0559\n",
      "Epoch 938/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0429 - val_loss: 0.0541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 939/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0415 - val_loss: 0.0556\n",
      "Epoch 940/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0411 - val_loss: 0.0544\n",
      "Epoch 941/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0425 - val_loss: 0.0536\n",
      "Epoch 942/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0420 - val_loss: 0.0576\n",
      "Epoch 943/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0439 - val_loss: 0.0571\n",
      "Epoch 944/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0447 - val_loss: 0.0544\n",
      "Epoch 945/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0422 - val_loss: 0.0556\n",
      "Epoch 946/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0413 - val_loss: 0.0558\n",
      "Epoch 947/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0413 - val_loss: 0.0560\n",
      "Epoch 948/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0434 - val_loss: 0.0538\n",
      "Epoch 949/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0429 - val_loss: 0.0575\n",
      "Epoch 950/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0437 - val_loss: 0.0562\n",
      "Epoch 951/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0432 - val_loss: 0.0538\n",
      "Epoch 952/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0424 - val_loss: 0.0562\n",
      "Epoch 953/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0414 - val_loss: 0.0542\n",
      "Epoch 954/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0426 - val_loss: 0.0524\n",
      "Epoch 955/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0413 - val_loss: 0.0578\n",
      "Epoch 956/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0441 - val_loss: 0.0552\n",
      "Epoch 957/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0434 - val_loss: 0.0565\n",
      "Epoch 958/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0425 - val_loss: 0.0552\n",
      "Epoch 959/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0411 - val_loss: 0.0533\n",
      "Epoch 960/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0416 - val_loss: 0.0578\n",
      "Epoch 961/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0443 - val_loss: 0.0575\n",
      "Epoch 962/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0453 - val_loss: 0.0569\n",
      "Epoch 963/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0451 - val_loss: 0.0589\n",
      "Epoch 964/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0446 - val_loss: 0.0533\n",
      "Epoch 965/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0415 - val_loss: 0.0536\n",
      "Epoch 966/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0405 - val_loss: 0.0572\n",
      "Epoch 967/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0417 - val_loss: 0.0528\n",
      "Epoch 968/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0435 - val_loss: 0.0561\n",
      "Epoch 969/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0425 - val_loss: 0.0556\n",
      "Epoch 970/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0408 - val_loss: 0.0540\n",
      "Epoch 971/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0412 - val_loss: 0.0579\n",
      "Epoch 972/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0435 - val_loss: 0.0542\n",
      "Epoch 973/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0409 - val_loss: 0.0541\n",
      "Epoch 974/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0400 - val_loss: 0.0570\n",
      "Epoch 975/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0430 - val_loss: 0.0547\n",
      "Epoch 976/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0430 - val_loss: 0.0548\n",
      "Epoch 977/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0413 - val_loss: 0.0572\n",
      "Epoch 978/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0416 - val_loss: 0.0533\n",
      "Epoch 979/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0419 - val_loss: 0.0559\n",
      "Epoch 980/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0405 - val_loss: 0.0543\n",
      "Epoch 981/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0410 - val_loss: 0.0546\n",
      "Epoch 982/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0424 - val_loss: 0.0568\n",
      "Epoch 983/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0419 - val_loss: 0.0545\n",
      "Epoch 984/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0425 - val_loss: 0.0559\n",
      "Epoch 985/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0416 - val_loss: 0.0535\n",
      "Epoch 986/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0400 - val_loss: 0.0543\n",
      "Epoch 987/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0397 - val_loss: 0.0550\n",
      "Epoch 988/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0401 - val_loss: 0.0554\n",
      "Epoch 989/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0406 - val_loss: 0.0582\n",
      "Epoch 990/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0427 - val_loss: 0.0551\n",
      "Epoch 991/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0414 - val_loss: 0.0554\n",
      "Epoch 992/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0404 - val_loss: 0.0546\n",
      "Epoch 993/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0398 - val_loss: 0.0519\n",
      "Epoch 994/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0405 - val_loss: 0.0550\n",
      "Epoch 995/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0410 - val_loss: 0.0532\n",
      "Epoch 996/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0403 - val_loss: 0.0545\n",
      "Epoch 997/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0403 - val_loss: 0.0522\n",
      "Epoch 998/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0397 - val_loss: 0.0541\n",
      "Epoch 999/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0386 - val_loss: 0.0558\n",
      "Epoch 1000/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0395 - val_loss: 0.0537\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 5, 2048)           8486912   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 5, 512)            5244928   \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 5, 1)              513       \n",
      "=================================================================\n",
      "Total params: 13,732,353\n",
      "Trainable params: 13,732,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3298 samples, validate on 825 samples\n",
      "Epoch 1/1000\n",
      "3298/3298 [==============================] - 4s 1ms/step - loss: 0.5226 - val_loss: 0.4181\n",
      "Epoch 2/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.4145 - val_loss: 0.3234\n",
      "Epoch 3/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3221 - val_loss: 0.3009\n",
      "Epoch 4/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.3204 - val_loss: 0.3084\n",
      "Epoch 5/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.3299 - val_loss: 0.2483\n",
      "Epoch 6/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2703 - val_loss: 0.2460\n",
      "Epoch 7/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2616 - val_loss: 0.2557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2641 - val_loss: 0.2495\n",
      "Epoch 9/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2597 - val_loss: 0.2294\n",
      "Epoch 10/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2444 - val_loss: 0.2113\n",
      "Epoch 11/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2346 - val_loss: 0.2051\n",
      "Epoch 12/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2297 - val_loss: 0.2088\n",
      "Epoch 13/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2321 - val_loss: 0.2127\n",
      "Epoch 14/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2312 - val_loss: 0.2084\n",
      "Epoch 15/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2200 - val_loss: 0.2008\n",
      "Epoch 16/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2148 - val_loss: 0.1950\n",
      "Epoch 17/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.2136 - val_loss: 0.1909\n",
      "Epoch 18/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2109 - val_loss: 0.1916\n",
      "Epoch 19/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2136 - val_loss: 0.1874\n",
      "Epoch 20/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.2089 - val_loss: 0.1845\n",
      "Epoch 21/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2038 - val_loss: 0.1836\n",
      "Epoch 22/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2009 - val_loss: 0.1865\n",
      "Epoch 23/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.2008 - val_loss: 0.1874\n",
      "Epoch 24/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.2035 - val_loss: 0.1875\n",
      "Epoch 25/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2037 - val_loss: 0.1841\n",
      "Epoch 26/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.2008 - val_loss: 0.1819\n",
      "Epoch 27/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1987 - val_loss: 0.1809\n",
      "Epoch 28/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1997 - val_loss: 0.1825\n",
      "Epoch 29/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1992 - val_loss: 0.1824\n",
      "Epoch 30/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1965 - val_loss: 0.1803\n",
      "Epoch 31/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1972 - val_loss: 0.1786\n",
      "Epoch 32/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1962 - val_loss: 0.1776\n",
      "Epoch 33/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1946 - val_loss: 0.1774\n",
      "Epoch 34/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1955 - val_loss: 0.1783\n",
      "Epoch 35/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1940 - val_loss: 0.1785\n",
      "Epoch 36/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1965 - val_loss: 0.1774\n",
      "Epoch 37/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1970 - val_loss: 0.1757\n",
      "Epoch 38/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1920 - val_loss: 0.1750\n",
      "Epoch 39/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1928 - val_loss: 0.1749\n",
      "Epoch 40/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1939 - val_loss: 0.1743\n",
      "Epoch 41/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1932 - val_loss: 0.1739\n",
      "Epoch 42/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1888 - val_loss: 0.1733\n",
      "Epoch 43/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1904 - val_loss: 0.1735\n",
      "Epoch 44/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1929 - val_loss: 0.1737\n",
      "Epoch 45/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1897 - val_loss: 0.1733\n",
      "Epoch 46/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1900 - val_loss: 0.1728\n",
      "Epoch 47/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1881 - val_loss: 0.1726\n",
      "Epoch 48/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1887 - val_loss: 0.1724\n",
      "Epoch 49/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1863 - val_loss: 0.1706\n",
      "Epoch 50/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1918 - val_loss: 0.1696\n",
      "Epoch 51/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1876 - val_loss: 0.1697\n",
      "Epoch 52/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1894 - val_loss: 0.1703\n",
      "Epoch 53/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.1881 - val_loss: 0.1703\n",
      "Epoch 54/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1894 - val_loss: 0.1696\n",
      "Epoch 55/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1887 - val_loss: 0.1682\n",
      "Epoch 56/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1845 - val_loss: 0.1687\n",
      "Epoch 57/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1862 - val_loss: 0.1691\n",
      "Epoch 58/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1852 - val_loss: 0.1690\n",
      "Epoch 59/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1858 - val_loss: 0.1686\n",
      "Epoch 60/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1879 - val_loss: 0.1679\n",
      "Epoch 61/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1868 - val_loss: 0.1670\n",
      "Epoch 62/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1853 - val_loss: 0.1669\n",
      "Epoch 63/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1865 - val_loss: 0.1678\n",
      "Epoch 64/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1861 - val_loss: 0.1676\n",
      "Epoch 65/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1856 - val_loss: 0.1661\n",
      "Epoch 66/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1847 - val_loss: 0.1664\n",
      "Epoch 67/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1850 - val_loss: 0.1672\n",
      "Epoch 68/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1862 - val_loss: 0.1662\n",
      "Epoch 69/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1820 - val_loss: 0.1653\n",
      "Epoch 70/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1845 - val_loss: 0.1660\n",
      "Epoch 71/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1812 - val_loss: 0.1661\n",
      "Epoch 72/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1822 - val_loss: 0.1654\n",
      "Epoch 73/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1822 - val_loss: 0.1649\n",
      "Epoch 74/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1813 - val_loss: 0.1655\n",
      "Epoch 75/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1817 - val_loss: 0.1664\n",
      "Epoch 76/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1812 - val_loss: 0.1648\n",
      "Epoch 77/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1817 - val_loss: 0.1645\n",
      "Epoch 78/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1786 - val_loss: 0.1652\n",
      "Epoch 79/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1805 - val_loss: 0.1636\n",
      "Epoch 80/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1806 - val_loss: 0.1635\n",
      "Epoch 81/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1795 - val_loss: 0.1646\n",
      "Epoch 82/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1788 - val_loss: 0.1637\n",
      "Epoch 83/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1803 - val_loss: 0.1626\n",
      "Epoch 84/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1800 - val_loss: 0.1619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1796 - val_loss: 0.1627\n",
      "Epoch 86/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1773 - val_loss: 0.1629\n",
      "Epoch 87/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1806 - val_loss: 0.1636\n",
      "Epoch 88/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1777 - val_loss: 0.1615\n",
      "Epoch 89/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1782 - val_loss: 0.1606\n",
      "Epoch 90/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1747 - val_loss: 0.1612\n",
      "Epoch 91/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1759 - val_loss: 0.1617\n",
      "Epoch 92/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1771 - val_loss: 0.1600\n",
      "Epoch 93/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1769 - val_loss: 0.1614\n",
      "Epoch 94/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1778 - val_loss: 0.1615\n",
      "Epoch 95/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1753 - val_loss: 0.1611\n",
      "Epoch 96/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1781 - val_loss: 0.1608\n",
      "Epoch 97/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1768 - val_loss: 0.1629\n",
      "Epoch 98/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1767 - val_loss: 0.1601\n",
      "Epoch 99/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1755 - val_loss: 0.1583\n",
      "Epoch 100/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1737 - val_loss: 0.1599\n",
      "Epoch 101/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1730 - val_loss: 0.1599\n",
      "Epoch 102/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1783 - val_loss: 0.1584\n",
      "Epoch 103/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1733 - val_loss: 0.1593\n",
      "Epoch 104/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1750 - val_loss: 0.1593\n",
      "Epoch 105/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1744 - val_loss: 0.1587\n",
      "Epoch 106/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1727 - val_loss: 0.1591\n",
      "Epoch 107/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1728 - val_loss: 0.1584\n",
      "Epoch 108/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1721 - val_loss: 0.1650\n",
      "Epoch 109/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1748 - val_loss: 0.1598\n",
      "Epoch 110/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1736 - val_loss: 0.1545\n",
      "Epoch 111/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1697 - val_loss: 0.1574\n",
      "Epoch 112/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1713 - val_loss: 0.1617\n",
      "Epoch 113/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1725 - val_loss: 0.1569\n",
      "Epoch 114/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1715 - val_loss: 0.1537\n",
      "Epoch 115/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1710 - val_loss: 0.1578\n",
      "Epoch 116/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1717 - val_loss: 0.1569\n",
      "Epoch 117/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1668 - val_loss: 0.1534\n",
      "Epoch 118/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1681 - val_loss: 0.1546\n",
      "Epoch 119/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1698 - val_loss: 0.1572\n",
      "Epoch 120/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1686 - val_loss: 0.1577\n",
      "Epoch 121/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1699 - val_loss: 0.1535\n",
      "Epoch 122/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1687 - val_loss: 0.1554\n",
      "Epoch 123/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1686 - val_loss: 0.1546\n",
      "Epoch 124/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1687 - val_loss: 0.1532\n",
      "Epoch 125/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1658 - val_loss: 0.1515\n",
      "Epoch 126/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1637 - val_loss: 0.1549\n",
      "Epoch 127/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1684 - val_loss: 0.1562\n",
      "Epoch 128/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1672 - val_loss: 0.1544\n",
      "Epoch 129/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1672 - val_loss: 0.1522\n",
      "Epoch 130/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.1625 - val_loss: 0.1533\n",
      "Epoch 131/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1647 - val_loss: 0.1499\n",
      "Epoch 132/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1628 - val_loss: 0.1535\n",
      "Epoch 133/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1680 - val_loss: 0.1544\n",
      "Epoch 134/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1631 - val_loss: 0.1509\n",
      "Epoch 135/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1626 - val_loss: 0.1527\n",
      "Epoch 136/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1619 - val_loss: 0.1501\n",
      "Epoch 137/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1593 - val_loss: 0.1501\n",
      "Epoch 138/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1598 - val_loss: 0.1517\n",
      "Epoch 139/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1619 - val_loss: 0.1510\n",
      "Epoch 140/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1600 - val_loss: 0.1496\n",
      "Epoch 141/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1597 - val_loss: 0.1497\n",
      "Epoch 142/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1616 - val_loss: 0.1495\n",
      "Epoch 143/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1614 - val_loss: 0.1512\n",
      "Epoch 144/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1585 - val_loss: 0.1474\n",
      "Epoch 145/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1604 - val_loss: 0.1507\n",
      "Epoch 146/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.1609 - val_loss: 0.1511\n",
      "Epoch 147/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1590 - val_loss: 0.1505\n",
      "Epoch 148/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1580 - val_loss: 0.1459\n",
      "Epoch 149/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1558 - val_loss: 0.1507\n",
      "Epoch 150/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1590 - val_loss: 0.1504\n",
      "Epoch 151/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1536 - val_loss: 0.1444\n",
      "Epoch 152/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1556 - val_loss: 0.1480\n",
      "Epoch 153/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1561 - val_loss: 0.1476\n",
      "Epoch 154/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1520 - val_loss: 0.1444\n",
      "Epoch 155/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1548 - val_loss: 0.1460\n",
      "Epoch 156/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1514 - val_loss: 0.1479\n",
      "Epoch 157/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1519 - val_loss: 0.1427\n",
      "Epoch 158/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1534 - val_loss: 0.1460\n",
      "Epoch 159/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1513 - val_loss: 0.1489\n",
      "Epoch 160/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1550 - val_loss: 0.1478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1509 - val_loss: 0.1426\n",
      "Epoch 162/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1554 - val_loss: 0.1486\n",
      "Epoch 163/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1525 - val_loss: 0.1454\n",
      "Epoch 164/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1520 - val_loss: 0.1423\n",
      "Epoch 165/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.1498 - val_loss: 0.1438\n",
      "Epoch 166/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1481 - val_loss: 0.1435\n",
      "Epoch 167/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1497 - val_loss: 0.1492\n",
      "Epoch 168/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1525 - val_loss: 0.1406\n",
      "Epoch 169/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.1498 - val_loss: 0.1466\n",
      "Epoch 170/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1483 - val_loss: 0.1412\n",
      "Epoch 171/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1479 - val_loss: 0.1395\n",
      "Epoch 172/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1476 - val_loss: 0.1409\n",
      "Epoch 173/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1458 - val_loss: 0.1395\n",
      "Epoch 174/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1493 - val_loss: 0.1402\n",
      "Epoch 175/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1451 - val_loss: 0.1384\n",
      "Epoch 176/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1437 - val_loss: 0.1390\n",
      "Epoch 177/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1450 - val_loss: 0.1395\n",
      "Epoch 178/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1455 - val_loss: 0.1364\n",
      "Epoch 179/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1448 - val_loss: 0.1422\n",
      "Epoch 180/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.1445 - val_loss: 0.1367\n",
      "Epoch 181/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1452 - val_loss: 0.1389\n",
      "Epoch 182/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1446 - val_loss: 0.1383\n",
      "Epoch 183/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1419 - val_loss: 0.1424\n",
      "Epoch 184/1000\n",
      "3298/3298 [==============================] - 1s 319us/step - loss: 0.1453 - val_loss: 0.1355\n",
      "Epoch 185/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1427 - val_loss: 0.1340\n",
      "Epoch 186/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1415 - val_loss: 0.1359\n",
      "Epoch 187/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1397 - val_loss: 0.1346\n",
      "Epoch 188/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1393 - val_loss: 0.1312\n",
      "Epoch 189/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1389 - val_loss: 0.1338\n",
      "Epoch 190/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1414 - val_loss: 0.1326\n",
      "Epoch 191/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1376 - val_loss: 0.1291\n",
      "Epoch 192/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1396 - val_loss: 0.1363\n",
      "Epoch 193/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1403 - val_loss: 0.1357\n",
      "Epoch 194/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1411 - val_loss: 0.1388\n",
      "Epoch 195/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1401 - val_loss: 0.1298\n",
      "Epoch 196/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1361 - val_loss: 0.1329\n",
      "Epoch 197/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1365 - val_loss: 0.1351\n",
      "Epoch 198/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1383 - val_loss: 0.1311\n",
      "Epoch 199/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1406 - val_loss: 0.1328\n",
      "Epoch 200/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1362 - val_loss: 0.1296\n",
      "Epoch 201/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1354 - val_loss: 0.1266\n",
      "Epoch 202/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1349 - val_loss: 0.1342\n",
      "Epoch 203/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1348 - val_loss: 0.1288\n",
      "Epoch 204/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1362 - val_loss: 0.1271\n",
      "Epoch 205/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1330 - val_loss: 0.1275\n",
      "Epoch 206/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.1308 - val_loss: 0.1276\n",
      "Epoch 207/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1313 - val_loss: 0.1291\n",
      "Epoch 208/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1362 - val_loss: 0.1269\n",
      "Epoch 209/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.1303 - val_loss: 0.1276\n",
      "Epoch 210/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1309 - val_loss: 0.1281\n",
      "Epoch 211/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1344 - val_loss: 0.1219\n",
      "Epoch 212/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1318 - val_loss: 0.1251\n",
      "Epoch 213/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1291 - val_loss: 0.1225\n",
      "Epoch 214/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1289 - val_loss: 0.1207\n",
      "Epoch 215/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1261 - val_loss: 0.1319\n",
      "Epoch 216/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1333 - val_loss: 0.1253\n",
      "Epoch 217/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1308 - val_loss: 0.1190\n",
      "Epoch 218/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1266 - val_loss: 0.1229\n",
      "Epoch 219/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1290 - val_loss: 0.1247\n",
      "Epoch 220/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1318 - val_loss: 0.1252\n",
      "Epoch 221/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1299 - val_loss: 0.1233\n",
      "Epoch 222/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1261 - val_loss: 0.1178\n",
      "Epoch 223/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1272 - val_loss: 0.1195\n",
      "Epoch 224/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1266 - val_loss: 0.1197\n",
      "Epoch 225/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1282 - val_loss: 0.1176\n",
      "Epoch 226/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1253 - val_loss: 0.1196\n",
      "Epoch 227/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1228 - val_loss: 0.1160\n",
      "Epoch 228/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1219 - val_loss: 0.1148\n",
      "Epoch 229/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.1227 - val_loss: 0.1177\n",
      "Epoch 230/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1224 - val_loss: 0.1149\n",
      "Epoch 231/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1208 - val_loss: 0.1130\n",
      "Epoch 232/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1204 - val_loss: 0.1149\n",
      "Epoch 233/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1213 - val_loss: 0.1129\n",
      "Epoch 234/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1200 - val_loss: 0.1117\n",
      "Epoch 235/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1214 - val_loss: 0.1125\n",
      "Epoch 236/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1212 - val_loss: 0.1138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1182 - val_loss: 0.1111\n",
      "Epoch 238/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1199 - val_loss: 0.1121\n",
      "Epoch 239/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1186 - val_loss: 0.1100\n",
      "Epoch 240/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1179 - val_loss: 0.1097\n",
      "Epoch 241/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1162 - val_loss: 0.1136\n",
      "Epoch 242/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1183 - val_loss: 0.1116\n",
      "Epoch 243/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1181 - val_loss: 0.1113\n",
      "Epoch 244/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1199 - val_loss: 0.1123\n",
      "Epoch 245/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1176 - val_loss: 0.1084\n",
      "Epoch 246/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1170 - val_loss: 0.1083\n",
      "Epoch 247/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.1134 - val_loss: 0.1095\n",
      "Epoch 248/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1144 - val_loss: 0.1062\n",
      "Epoch 249/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1146 - val_loss: 0.1034\n",
      "Epoch 250/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1107 - val_loss: 0.1087\n",
      "Epoch 251/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1127 - val_loss: 0.1056\n",
      "Epoch 252/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1157 - val_loss: 0.1048\n",
      "Epoch 253/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1119 - val_loss: 0.1076\n",
      "Epoch 254/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1129 - val_loss: 0.1056\n",
      "Epoch 255/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1161 - val_loss: 0.1024\n",
      "Epoch 256/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1109 - val_loss: 0.1100\n",
      "Epoch 257/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1118 - val_loss: 0.1019\n",
      "Epoch 258/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1120 - val_loss: 0.1053\n",
      "Epoch 259/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1087 - val_loss: 0.1032\n",
      "Epoch 260/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1108 - val_loss: 0.1104\n",
      "Epoch 261/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1141 - val_loss: 0.1051\n",
      "Epoch 262/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1127 - val_loss: 0.1101\n",
      "Epoch 263/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1166 - val_loss: 0.1058\n",
      "Epoch 264/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1121 - val_loss: 0.0976\n",
      "Epoch 265/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1077 - val_loss: 0.1061\n",
      "Epoch 266/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1094 - val_loss: 0.1050\n",
      "Epoch 267/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1084 - val_loss: 0.1002\n",
      "Epoch 268/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1080 - val_loss: 0.1043\n",
      "Epoch 269/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1083 - val_loss: 0.1023\n",
      "Epoch 270/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1131 - val_loss: 0.0991\n",
      "Epoch 271/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1072 - val_loss: 0.0980\n",
      "Epoch 272/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1038 - val_loss: 0.1006\n",
      "Epoch 273/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1081 - val_loss: 0.1015\n",
      "Epoch 274/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1058 - val_loss: 0.0960\n",
      "Epoch 275/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1031 - val_loss: 0.0980\n",
      "Epoch 276/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1044 - val_loss: 0.0961\n",
      "Epoch 277/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1040 - val_loss: 0.0951\n",
      "Epoch 278/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1005 - val_loss: 0.0977\n",
      "Epoch 279/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1008 - val_loss: 0.0975\n",
      "Epoch 280/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1014 - val_loss: 0.0977\n",
      "Epoch 281/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.1088 - val_loss: 0.1056\n",
      "Epoch 282/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1098 - val_loss: 0.1068\n",
      "Epoch 283/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1087 - val_loss: 0.0978\n",
      "Epoch 284/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1078 - val_loss: 0.0991\n",
      "Epoch 285/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1023 - val_loss: 0.0950\n",
      "Epoch 286/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1022 - val_loss: 0.0993\n",
      "Epoch 287/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1018 - val_loss: 0.0963\n",
      "Epoch 288/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1005 - val_loss: 0.0950\n",
      "Epoch 289/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1024 - val_loss: 0.0968\n",
      "Epoch 290/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1018 - val_loss: 0.0931\n",
      "Epoch 291/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0984 - val_loss: 0.0946\n",
      "Epoch 292/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 293/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.1050 - val_loss: 0.0940\n",
      "Epoch 294/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.1021 - val_loss: 0.0905\n",
      "Epoch 295/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0975 - val_loss: 0.0958\n",
      "Epoch 296/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0987 - val_loss: 0.0953\n",
      "Epoch 297/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.1021 - val_loss: 0.0913\n",
      "Epoch 298/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.1010 - val_loss: 0.0965\n",
      "Epoch 299/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0999 - val_loss: 0.0905\n",
      "Epoch 300/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0979 - val_loss: 0.0934\n",
      "Epoch 301/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0983 - val_loss: 0.0993\n",
      "Epoch 302/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0997 - val_loss: 0.0883\n",
      "Epoch 303/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0933 - val_loss: 0.0913\n",
      "Epoch 304/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0950 - val_loss: 0.0900\n",
      "Epoch 305/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0957 - val_loss: 0.0932\n",
      "Epoch 306/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0992 - val_loss: 0.0943\n",
      "Epoch 307/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0960 - val_loss: 0.0865\n",
      "Epoch 308/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0953 - val_loss: 0.0888\n",
      "Epoch 309/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0928 - val_loss: 0.0948\n",
      "Epoch 310/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0965 - val_loss: 0.0893\n",
      "Epoch 311/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0948 - val_loss: 0.0876\n",
      "Epoch 312/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0883 - val_loss: 0.0884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0910 - val_loss: 0.0889\n",
      "Epoch 314/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0924 - val_loss: 0.0898\n",
      "Epoch 315/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0944 - val_loss: 0.0864\n",
      "Epoch 316/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0895 - val_loss: 0.0841\n",
      "Epoch 317/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0904 - val_loss: 0.0875\n",
      "Epoch 318/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0915 - val_loss: 0.0887\n",
      "Epoch 319/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0932 - val_loss: 0.0857\n",
      "Epoch 320/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0898 - val_loss: 0.0849\n",
      "Epoch 321/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0873 - val_loss: 0.0837\n",
      "Epoch 322/1000\n",
      "3298/3298 [==============================] - 1s 320us/step - loss: 0.0873 - val_loss: 0.0893\n",
      "Epoch 323/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0891 - val_loss: 0.0878\n",
      "Epoch 324/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0934 - val_loss: 0.0809\n",
      "Epoch 325/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0895 - val_loss: 0.0890\n",
      "Epoch 326/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0899 - val_loss: 0.0829\n",
      "Epoch 327/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0927 - val_loss: 0.0863\n",
      "Epoch 328/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0882 - val_loss: 0.0841\n",
      "Epoch 329/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0860 - val_loss: 0.0828\n",
      "Epoch 330/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0873 - val_loss: 0.0877\n",
      "Epoch 331/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0901 - val_loss: 0.0810\n",
      "Epoch 332/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0861 - val_loss: 0.0835\n",
      "Epoch 333/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0871 - val_loss: 0.0839\n",
      "Epoch 334/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0866 - val_loss: 0.0808\n",
      "Epoch 335/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0874 - val_loss: 0.0865\n",
      "Epoch 336/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0855 - val_loss: 0.0812\n",
      "Epoch 337/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0851 - val_loss: 0.0794\n",
      "Epoch 338/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0877 - val_loss: 0.0910\n",
      "Epoch 339/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0916 - val_loss: 0.0770\n",
      "Epoch 340/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0830 - val_loss: 0.0827\n",
      "Epoch 341/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0865 - val_loss: 0.0899\n",
      "Epoch 342/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0845 - val_loss: 0.0791\n",
      "Epoch 343/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0830 - val_loss: 0.0778\n",
      "Epoch 344/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0845 - val_loss: 0.0906\n",
      "Epoch 345/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0917 - val_loss: 0.0791\n",
      "Epoch 346/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0849 - val_loss: 0.0788\n",
      "Epoch 347/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0852 - val_loss: 0.0876\n",
      "Epoch 348/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0879 - val_loss: 0.0775\n",
      "Epoch 349/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0810 - val_loss: 0.0773\n",
      "Epoch 350/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0849 - val_loss: 0.0856\n",
      "Epoch 351/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0857 - val_loss: 0.0794\n",
      "Epoch 352/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0822 - val_loss: 0.0766\n",
      "Epoch 353/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0827 - val_loss: 0.0851\n",
      "Epoch 354/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0821 - val_loss: 0.0780\n",
      "Epoch 355/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0824 - val_loss: 0.0772\n",
      "Epoch 356/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0836 - val_loss: 0.0814\n",
      "Epoch 357/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0810 - val_loss: 0.0791\n",
      "Epoch 358/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0799 - val_loss: 0.0743\n",
      "Epoch 359/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0801 - val_loss: 0.0821\n",
      "Epoch 360/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0815 - val_loss: 0.0783\n",
      "Epoch 361/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0802 - val_loss: 0.0758\n",
      "Epoch 362/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0791 - val_loss: 0.0797\n",
      "Epoch 363/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0808 - val_loss: 0.0783\n",
      "Epoch 364/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0804 - val_loss: 0.0777\n",
      "Epoch 365/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0813 - val_loss: 0.0782\n",
      "Epoch 366/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0801 - val_loss: 0.0780\n",
      "Epoch 367/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0791 - val_loss: 0.0770\n",
      "Epoch 368/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0815 - val_loss: 0.0749\n",
      "Epoch 369/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0754 - val_loss: 0.0768\n",
      "Epoch 370/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0804 - val_loss: 0.0802\n",
      "Epoch 371/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0823 - val_loss: 0.0812\n",
      "Epoch 372/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0805 - val_loss: 0.0785\n",
      "Epoch 373/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0799 - val_loss: 0.0738\n",
      "Epoch 374/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0786 - val_loss: 0.0759\n",
      "Epoch 375/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0754 - val_loss: 0.0768\n",
      "Epoch 376/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0804 - val_loss: 0.0790\n",
      "Epoch 377/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0803 - val_loss: 0.0775\n",
      "Epoch 378/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0782 - val_loss: 0.0724\n",
      "Epoch 379/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0764 - val_loss: 0.0717\n",
      "Epoch 380/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0747 - val_loss: 0.0763\n",
      "Epoch 381/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0769 - val_loss: 0.0732\n",
      "Epoch 382/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0739 - val_loss: 0.0721\n",
      "Epoch 383/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0769 - val_loss: 0.0761\n",
      "Epoch 384/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0751 - val_loss: 0.0740\n",
      "Epoch 385/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0753 - val_loss: 0.0742\n",
      "Epoch 386/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0769 - val_loss: 0.0779\n",
      "Epoch 387/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0787 - val_loss: 0.0717\n",
      "Epoch 388/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0750 - val_loss: 0.0697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0733 - val_loss: 0.0753\n",
      "Epoch 390/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0747 - val_loss: 0.0695\n",
      "Epoch 391/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0736 - val_loss: 0.0747\n",
      "Epoch 392/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0714 - val_loss: 0.0733\n",
      "Epoch 393/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0756 - val_loss: 0.0732\n",
      "Epoch 394/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0783 - val_loss: 0.0763\n",
      "Epoch 395/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0742 - val_loss: 0.0713\n",
      "Epoch 396/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0719 - val_loss: 0.0746\n",
      "Epoch 397/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0742 - val_loss: 0.0761\n",
      "Epoch 398/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0769 - val_loss: 0.0777\n",
      "Epoch 399/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0778 - val_loss: 0.0703\n",
      "Epoch 400/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0726 - val_loss: 0.0739\n",
      "Epoch 401/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0727 - val_loss: 0.0718\n",
      "Epoch 402/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0762 - val_loss: 0.0708\n",
      "Epoch 403/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0744 - val_loss: 0.0751\n",
      "Epoch 404/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0758 - val_loss: 0.0673\n",
      "Epoch 405/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0731 - val_loss: 0.0716\n",
      "Epoch 406/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0725 - val_loss: 0.0695\n",
      "Epoch 407/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0687 - val_loss: 0.0685\n",
      "Epoch 408/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0721 - val_loss: 0.0756\n",
      "Epoch 409/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0741 - val_loss: 0.0736\n",
      "Epoch 410/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0738 - val_loss: 0.0677\n",
      "Epoch 411/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0720 - val_loss: 0.0734\n",
      "Epoch 412/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0704 - val_loss: 0.0694\n",
      "Epoch 413/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0708 - val_loss: 0.0682\n",
      "Epoch 414/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0682 - val_loss: 0.0702\n",
      "Epoch 415/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0714 - val_loss: 0.0692\n",
      "Epoch 416/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0709 - val_loss: 0.0683\n",
      "Epoch 417/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0720 - val_loss: 0.0700\n",
      "Epoch 418/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0688 - val_loss: 0.0681\n",
      "Epoch 419/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0707 - val_loss: 0.0731\n",
      "Epoch 420/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0728 - val_loss: 0.0736\n",
      "Epoch 421/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0721 - val_loss: 0.0676\n",
      "Epoch 422/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0725 - val_loss: 0.0717\n",
      "Epoch 423/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0713 - val_loss: 0.0661\n",
      "Epoch 424/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0689 - val_loss: 0.0666\n",
      "Epoch 425/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0684 - val_loss: 0.0777\n",
      "Epoch 426/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0728 - val_loss: 0.0681\n",
      "Epoch 427/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0680 - val_loss: 0.0645\n",
      "Epoch 428/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0697 - val_loss: 0.0716\n",
      "Epoch 429/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0695 - val_loss: 0.0640\n",
      "Epoch 430/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0674 - val_loss: 0.0639\n",
      "Epoch 431/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0688 - val_loss: 0.0740\n",
      "Epoch 432/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0698 - val_loss: 0.0642\n",
      "Epoch 433/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0649 - val_loss: 0.0646\n",
      "Epoch 434/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0681 - val_loss: 0.0725\n",
      "Epoch 435/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0693 - val_loss: 0.0629\n",
      "Epoch 436/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0661 - val_loss: 0.0632\n",
      "Epoch 437/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0657 - val_loss: 0.0697\n",
      "Epoch 438/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0668 - val_loss: 0.0619\n",
      "Epoch 439/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0642 - val_loss: 0.0623\n",
      "Epoch 440/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0623 - val_loss: 0.0707\n",
      "Epoch 441/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0673 - val_loss: 0.0620\n",
      "Epoch 442/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0663 - val_loss: 0.0644\n",
      "Epoch 443/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0665 - val_loss: 0.0682\n",
      "Epoch 444/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0657 - val_loss: 0.0636\n",
      "Epoch 445/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0657 - val_loss: 0.0663\n",
      "Epoch 446/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0657 - val_loss: 0.0628\n",
      "Epoch 447/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0643 - val_loss: 0.0645\n",
      "Epoch 448/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0634 - val_loss: 0.0641\n",
      "Epoch 449/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0644 - val_loss: 0.0642\n",
      "Epoch 450/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0651 - val_loss: 0.0687\n",
      "Epoch 451/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0658 - val_loss: 0.0679\n",
      "Epoch 452/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0664 - val_loss: 0.0673\n",
      "Epoch 453/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0686 - val_loss: 0.0662\n",
      "Epoch 454/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0645 - val_loss: 0.0642\n",
      "Epoch 455/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0653 - val_loss: 0.0634\n",
      "Epoch 456/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0628 - val_loss: 0.0622\n",
      "Epoch 457/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0619 - val_loss: 0.0638\n",
      "Epoch 458/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0650 - val_loss: 0.0628\n",
      "Epoch 459/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0629 - val_loss: 0.0622\n",
      "Epoch 460/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0600 - val_loss: 0.0631\n",
      "Epoch 461/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0630 - val_loss: 0.0639\n",
      "Epoch 462/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0628 - val_loss: 0.0622\n",
      "Epoch 463/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0637 - val_loss: 0.0652\n",
      "Epoch 464/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0642 - val_loss: 0.0618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0616 - val_loss: 0.0639\n",
      "Epoch 466/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0650 - val_loss: 0.0629\n",
      "Epoch 467/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0629 - val_loss: 0.0677\n",
      "Epoch 468/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0680 - val_loss: 0.0632\n",
      "Epoch 469/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0653 - val_loss: 0.0671\n",
      "Epoch 470/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0656 - val_loss: 0.0634\n",
      "Epoch 471/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0641 - val_loss: 0.0679\n",
      "Epoch 472/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0641 - val_loss: 0.0647\n",
      "Epoch 473/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0654 - val_loss: 0.0629\n",
      "Epoch 474/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0639 - val_loss: 0.0638\n",
      "Epoch 475/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0618 - val_loss: 0.0616\n",
      "Epoch 476/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0618 - val_loss: 0.0609\n",
      "Epoch 477/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0598 - val_loss: 0.0606\n",
      "Epoch 478/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0610 - val_loss: 0.0600\n",
      "Epoch 479/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0602 - val_loss: 0.0600\n",
      "Epoch 480/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0602 - val_loss: 0.0622\n",
      "Epoch 481/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0584 - val_loss: 0.0592\n",
      "Epoch 482/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0594 - val_loss: 0.0620\n",
      "Epoch 483/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0595 - val_loss: 0.0582\n",
      "Epoch 484/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0595 - val_loss: 0.0627\n",
      "Epoch 485/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 486/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0591 - val_loss: 0.0584\n",
      "Epoch 487/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0592 - val_loss: 0.0635\n",
      "Epoch 488/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0592 - val_loss: 0.0585\n",
      "Epoch 489/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0590 - val_loss: 0.0642\n",
      "Epoch 490/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0625 - val_loss: 0.0656\n",
      "Epoch 491/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0660 - val_loss: 0.0693\n",
      "Epoch 492/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0683 - val_loss: 0.0636\n",
      "Epoch 493/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0629 - val_loss: 0.0633\n",
      "Epoch 494/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0592 - val_loss: 0.0578\n",
      "Epoch 495/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0607 - val_loss: 0.0635\n",
      "Epoch 496/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0604 - val_loss: 0.0591\n",
      "Epoch 497/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0607 - val_loss: 0.0592\n",
      "Epoch 498/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0601 - val_loss: 0.0647\n",
      "Epoch 499/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0614 - val_loss: 0.0585\n",
      "Epoch 500/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0605 - val_loss: 0.0594\n",
      "Epoch 501/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0573 - val_loss: 0.0602\n",
      "Epoch 502/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0565 - val_loss: 0.0574\n",
      "Epoch 503/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0590 - val_loss: 0.0605\n",
      "Epoch 504/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0569 - val_loss: 0.0584\n",
      "Epoch 505/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0565 - val_loss: 0.0583\n",
      "Epoch 506/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0569 - val_loss: 0.0593\n",
      "Epoch 507/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0570 - val_loss: 0.0607\n",
      "Epoch 508/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0575 - val_loss: 0.0604\n",
      "Epoch 509/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0567 - val_loss: 0.0578\n",
      "Epoch 510/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0563 - val_loss: 0.0575\n",
      "Epoch 511/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0569 - val_loss: 0.0634\n",
      "Epoch 512/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0594 - val_loss: 0.0595\n",
      "Epoch 513/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0603 - val_loss: 0.0572\n",
      "Epoch 514/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0568 - val_loss: 0.0599\n",
      "Epoch 515/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0565 - val_loss: 0.0557\n",
      "Epoch 516/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0582 - val_loss: 0.0629\n",
      "Epoch 517/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0578 - val_loss: 0.0557\n",
      "Epoch 518/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0546 - val_loss: 0.0561\n",
      "Epoch 519/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0551 - val_loss: 0.0612\n",
      "Epoch 520/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0574 - val_loss: 0.0558\n",
      "Epoch 521/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.0574 - val_loss: 0.0623\n",
      "Epoch 522/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.0568 - val_loss: 0.0555\n",
      "Epoch 523/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0555 - val_loss: 0.0560\n",
      "Epoch 524/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0572 - val_loss: 0.0649\n",
      "Epoch 525/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0574 - val_loss: 0.0555\n",
      "Epoch 526/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0587 - val_loss: 0.0573\n",
      "Epoch 527/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0554 - val_loss: 0.0608\n",
      "Epoch 528/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0561 - val_loss: 0.0556\n",
      "Epoch 529/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0572 - val_loss: 0.0612\n",
      "Epoch 530/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0569 - val_loss: 0.0583\n",
      "Epoch 531/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0571 - val_loss: 0.0569\n",
      "Epoch 532/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 533/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0551 - val_loss: 0.0570\n",
      "Epoch 534/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0563 - val_loss: 0.0614\n",
      "Epoch 535/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0603 - val_loss: 0.0576\n",
      "Epoch 536/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0557 - val_loss: 0.0593\n",
      "Epoch 537/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0558 - val_loss: 0.0585\n",
      "Epoch 538/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0580 - val_loss: 0.0600\n",
      "Epoch 539/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0564 - val_loss: 0.0579\n",
      "Epoch 540/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0563 - val_loss: 0.0601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0574 - val_loss: 0.0547\n",
      "Epoch 542/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0543 - val_loss: 0.0601\n",
      "Epoch 543/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0548 - val_loss: 0.0543\n",
      "Epoch 544/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0531 - val_loss: 0.0543\n",
      "Epoch 545/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0549 - val_loss: 0.0577\n",
      "Epoch 546/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0527 - val_loss: 0.0533\n",
      "Epoch 547/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0538 - val_loss: 0.0578\n",
      "Epoch 548/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0533 - val_loss: 0.0551\n",
      "Epoch 549/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0538 - val_loss: 0.0568\n",
      "Epoch 550/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0545 - val_loss: 0.0576\n",
      "Epoch 551/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0548 - val_loss: 0.0568\n",
      "Epoch 552/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0546 - val_loss: 0.0567\n",
      "Epoch 553/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0535 - val_loss: 0.0565\n",
      "Epoch 554/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0548 - val_loss: 0.0549\n",
      "Epoch 555/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0545 - val_loss: 0.0598\n",
      "Epoch 556/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0555 - val_loss: 0.0540\n",
      "Epoch 557/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0537 - val_loss: 0.0559\n",
      "Epoch 558/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0532 - val_loss: 0.0539\n",
      "Epoch 559/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0515 - val_loss: 0.0564\n",
      "Epoch 560/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0521 - val_loss: 0.0558\n",
      "Epoch 561/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0556 - val_loss: 0.0568\n",
      "Epoch 562/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0574 - val_loss: 0.0597\n",
      "Epoch 563/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0548 - val_loss: 0.0519\n",
      "Epoch 564/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0529 - val_loss: 0.0566\n",
      "Epoch 565/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0514 - val_loss: 0.0539\n",
      "Epoch 566/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0509 - val_loss: 0.0535\n",
      "Epoch 567/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0530 - val_loss: 0.0580\n",
      "Epoch 568/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0534 - val_loss: 0.0516\n",
      "Epoch 569/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0518 - val_loss: 0.0561\n",
      "Epoch 570/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0509 - val_loss: 0.0529\n",
      "Epoch 571/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0491 - val_loss: 0.0518\n",
      "Epoch 572/1000\n",
      "3298/3298 [==============================] - 1s 319us/step - loss: 0.0499 - val_loss: 0.0569\n",
      "Epoch 573/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0518 - val_loss: 0.0517\n",
      "Epoch 574/1000\n",
      "3298/3298 [==============================] - 1s 320us/step - loss: 0.0501 - val_loss: 0.0551\n",
      "Epoch 575/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0511 - val_loss: 0.0532\n",
      "Epoch 576/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0499 - val_loss: 0.0522\n",
      "Epoch 577/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0501 - val_loss: 0.0565\n",
      "Epoch 578/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0518 - val_loss: 0.0514\n",
      "Epoch 579/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0520 - val_loss: 0.0553\n",
      "Epoch 580/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0526 - val_loss: 0.0524\n",
      "Epoch 581/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0509 - val_loss: 0.0543\n",
      "Epoch 582/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0541 - val_loss: 0.0559\n",
      "Epoch 583/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0522 - val_loss: 0.0529\n",
      "Epoch 584/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0513 - val_loss: 0.0533\n",
      "Epoch 585/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0520 - val_loss: 0.0535\n",
      "Epoch 586/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0513 - val_loss: 0.0516\n",
      "Epoch 587/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0498 - val_loss: 0.0542\n",
      "Epoch 588/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0509 - val_loss: 0.0529\n",
      "Epoch 589/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0522 - val_loss: 0.0560\n",
      "Epoch 590/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0505 - val_loss: 0.0560\n",
      "Epoch 591/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0533 - val_loss: 0.0578\n",
      "Epoch 592/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0540 - val_loss: 0.0543\n",
      "Epoch 593/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0527 - val_loss: 0.0556\n",
      "Epoch 594/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0509 - val_loss: 0.0509\n",
      "Epoch 595/1000\n",
      "3298/3298 [==============================] - 1s 319us/step - loss: 0.0496 - val_loss: 0.0539\n",
      "Epoch 596/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0484 - val_loss: 0.0513\n",
      "Epoch 597/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0484 - val_loss: 0.0524\n",
      "Epoch 598/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0510 - val_loss: 0.0528\n",
      "Epoch 599/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0502 - val_loss: 0.0538\n",
      "Epoch 600/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0496 - val_loss: 0.0510\n",
      "Epoch 601/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0512 - val_loss: 0.0544\n",
      "Epoch 602/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0484 - val_loss: 0.0508\n",
      "Epoch 603/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0487 - val_loss: 0.0528\n",
      "Epoch 604/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0498 - val_loss: 0.0513\n",
      "Epoch 605/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0502 - val_loss: 0.0524\n",
      "Epoch 606/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0475 - val_loss: 0.0502\n",
      "Epoch 607/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0485 - val_loss: 0.0515\n",
      "Epoch 608/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0484 - val_loss: 0.0510\n",
      "Epoch 609/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0483 - val_loss: 0.0546\n",
      "Epoch 610/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0499 - val_loss: 0.0505\n",
      "Epoch 611/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0503 - val_loss: 0.0550\n",
      "Epoch 612/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0518 - val_loss: 0.0546\n",
      "Epoch 613/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0504 - val_loss: 0.0534\n",
      "Epoch 614/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0532 - val_loss: 0.0546\n",
      "Epoch 615/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0503 - val_loss: 0.0505\n",
      "Epoch 616/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0483 - val_loss: 0.0535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0472 - val_loss: 0.0496\n",
      "Epoch 618/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0484 - val_loss: 0.0540\n",
      "Epoch 619/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0497 - val_loss: 0.0500\n",
      "Epoch 620/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0470 - val_loss: 0.0518\n",
      "Epoch 621/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0471 - val_loss: 0.0503\n",
      "Epoch 622/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0468 - val_loss: 0.0506\n",
      "Epoch 623/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0471 - val_loss: 0.0538\n",
      "Epoch 624/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0486 - val_loss: 0.0490\n",
      "Epoch 625/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0487 - val_loss: 0.0561\n",
      "Epoch 626/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0516 - val_loss: 0.0493\n",
      "Epoch 627/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0483 - val_loss: 0.0537\n",
      "Epoch 628/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0501 - val_loss: 0.0495\n",
      "Epoch 629/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0469 - val_loss: 0.0505\n",
      "Epoch 630/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0471 - val_loss: 0.0556\n",
      "Epoch 631/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0490 - val_loss: 0.0486\n",
      "Epoch 632/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0494 - val_loss: 0.0544\n",
      "Epoch 633/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0475 - val_loss: 0.0511\n",
      "Epoch 634/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0484 - val_loss: 0.0498\n",
      "Epoch 635/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0482 - val_loss: 0.0543\n",
      "Epoch 636/1000\n",
      "3298/3298 [==============================] - 1s 320us/step - loss: 0.0495 - val_loss: 0.0510\n",
      "Epoch 637/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0499 - val_loss: 0.0520\n",
      "Epoch 638/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0465 - val_loss: 0.0511\n",
      "Epoch 639/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0471 - val_loss: 0.0503\n",
      "Epoch 640/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0463 - val_loss: 0.0524\n",
      "Epoch 641/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0469 - val_loss: 0.0532\n",
      "Epoch 642/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0492 - val_loss: 0.0543\n",
      "Epoch 643/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0523 - val_loss: 0.0581\n",
      "Epoch 644/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0516 - val_loss: 0.0499\n",
      "Epoch 645/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0480 - val_loss: 0.0528\n",
      "Epoch 646/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0473 - val_loss: 0.0494\n",
      "Epoch 647/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0467 - val_loss: 0.0481\n",
      "Epoch 648/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0472 - val_loss: 0.0550\n",
      "Epoch 649/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0485 - val_loss: 0.0514\n",
      "Epoch 650/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0509 - val_loss: 0.0544\n",
      "Epoch 651/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0492 - val_loss: 0.0527\n",
      "Epoch 652/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0475 - val_loss: 0.0480\n",
      "Epoch 653/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0463 - val_loss: 0.0542\n",
      "Epoch 654/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0481 - val_loss: 0.0482\n",
      "Epoch 655/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0464 - val_loss: 0.0495\n",
      "Epoch 656/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0446 - val_loss: 0.0528\n",
      "Epoch 657/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0481 - val_loss: 0.0490\n",
      "Epoch 658/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0487 - val_loss: 0.0529\n",
      "Epoch 659/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0470 - val_loss: 0.0484\n",
      "Epoch 660/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0452 - val_loss: 0.0510\n",
      "Epoch 661/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0475 - val_loss: 0.0552\n",
      "Epoch 662/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0508 - val_loss: 0.0556\n",
      "Epoch 663/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0491 - val_loss: 0.0490\n",
      "Epoch 664/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0486 - val_loss: 0.0530\n",
      "Epoch 665/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0459 - val_loss: 0.0560\n",
      "Epoch 666/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0500 - val_loss: 0.0512\n",
      "Epoch 667/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0512 - val_loss: 0.0539\n",
      "Epoch 668/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0477 - val_loss: 0.0502\n",
      "Epoch 669/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0462 - val_loss: 0.0487\n",
      "Epoch 670/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0458 - val_loss: 0.0519\n",
      "Epoch 671/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0468 - val_loss: 0.0516\n",
      "Epoch 672/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0469 - val_loss: 0.0520\n",
      "Epoch 673/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0456 - val_loss: 0.0482\n",
      "Epoch 674/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0443 - val_loss: 0.0509\n",
      "Epoch 675/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0444 - val_loss: 0.0495\n",
      "Epoch 676/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0444 - val_loss: 0.0503\n",
      "Epoch 677/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0461 - val_loss: 0.0528\n",
      "Epoch 678/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0477 - val_loss: 0.0505\n",
      "Epoch 679/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0452 - val_loss: 0.0514\n",
      "Epoch 680/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0470 - val_loss: 0.0498\n",
      "Epoch 681/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0456 - val_loss: 0.0498\n",
      "Epoch 682/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0436 - val_loss: 0.0465\n",
      "Epoch 683/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0430 - val_loss: 0.0487\n",
      "Epoch 684/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0423 - val_loss: 0.0479\n",
      "Epoch 685/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0421 - val_loss: 0.0486\n",
      "Epoch 686/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0422 - val_loss: 0.0469\n",
      "Epoch 687/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0447 - val_loss: 0.0531\n",
      "Epoch 688/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0449 - val_loss: 0.0473\n",
      "Epoch 689/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0456 - val_loss: 0.0505\n",
      "Epoch 690/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0437 - val_loss: 0.0479\n",
      "Epoch 691/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0442 - val_loss: 0.0473\n",
      "Epoch 692/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0430 - val_loss: 0.0529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 693/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0461 - val_loss: 0.0463\n",
      "Epoch 694/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0446 - val_loss: 0.0526\n",
      "Epoch 695/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0451 - val_loss: 0.0501\n",
      "Epoch 696/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0450 - val_loss: 0.0500\n",
      "Epoch 697/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0463 - val_loss: 0.0519\n",
      "Epoch 698/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0441 - val_loss: 0.0471\n",
      "Epoch 699/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0435 - val_loss: 0.0491\n",
      "Epoch 700/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0424 - val_loss: 0.0508\n",
      "Epoch 701/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0444 - val_loss: 0.0503\n",
      "Epoch 702/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0467 - val_loss: 0.0504\n",
      "Epoch 703/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0443 - val_loss: 0.0494\n",
      "Epoch 704/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0448 - val_loss: 0.0473\n",
      "Epoch 705/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0434 - val_loss: 0.0481\n",
      "Epoch 706/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0426 - val_loss: 0.0476\n",
      "Epoch 707/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0413 - val_loss: 0.0508\n",
      "Epoch 708/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0429 - val_loss: 0.0528\n",
      "Epoch 709/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0463 - val_loss: 0.0497\n",
      "Epoch 710/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0446 - val_loss: 0.0500\n",
      "Epoch 711/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0443 - val_loss: 0.0468\n",
      "Epoch 712/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0407 - val_loss: 0.0464\n",
      "Epoch 713/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0420 - val_loss: 0.0502\n",
      "Epoch 714/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0431 - val_loss: 0.0447\n",
      "Epoch 715/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0415 - val_loss: 0.0504\n",
      "Epoch 716/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0420 - val_loss: 0.0456\n",
      "Epoch 717/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0420 - val_loss: 0.0500\n",
      "Epoch 718/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0414 - val_loss: 0.0455\n",
      "Epoch 719/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0401 - val_loss: 0.0510\n",
      "Epoch 720/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0423 - val_loss: 0.0455\n",
      "Epoch 721/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0427 - val_loss: 0.0503\n",
      "Epoch 722/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0414 - val_loss: 0.0465\n",
      "Epoch 723/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0399 - val_loss: 0.0461\n",
      "Epoch 724/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0408 - val_loss: 0.0504\n",
      "Epoch 725/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0421 - val_loss: 0.0467\n",
      "Epoch 726/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0439 - val_loss: 0.0493\n",
      "Epoch 727/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0427 - val_loss: 0.0461\n",
      "Epoch 728/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0429 - val_loss: 0.0497\n",
      "Epoch 729/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0435 - val_loss: 0.0485\n",
      "Epoch 730/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0423 - val_loss: 0.0488\n",
      "Epoch 731/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0432 - val_loss: 0.0480\n",
      "Epoch 732/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0422 - val_loss: 0.0505\n",
      "Epoch 733/1000\n",
      "3298/3298 [==============================] - 1s 318us/step - loss: 0.0430 - val_loss: 0.0467\n",
      "Epoch 734/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0433 - val_loss: 0.0489\n",
      "Epoch 735/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0423 - val_loss: 0.0468\n",
      "Epoch 736/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0417 - val_loss: 0.0494\n",
      "Epoch 737/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0416 - val_loss: 0.0463\n",
      "Epoch 738/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0419 - val_loss: 0.0470\n",
      "Epoch 739/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0400 - val_loss: 0.0516\n",
      "Epoch 740/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0434 - val_loss: 0.0477\n",
      "Epoch 741/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0460 - val_loss: 0.0513\n",
      "Epoch 742/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0414 - val_loss: 0.0459\n",
      "Epoch 743/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0390 - val_loss: 0.0469\n",
      "Epoch 744/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0412 - val_loss: 0.0505\n",
      "Epoch 745/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0420 - val_loss: 0.0461\n",
      "Epoch 746/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0433 - val_loss: 0.0484\n",
      "Epoch 747/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0415 - val_loss: 0.0478\n",
      "Epoch 748/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0413 - val_loss: 0.0457\n",
      "Epoch 749/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0432 - val_loss: 0.0526\n",
      "Epoch 750/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0420 - val_loss: 0.0451\n",
      "Epoch 751/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0415 - val_loss: 0.0467\n",
      "Epoch 752/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0400 - val_loss: 0.0481\n",
      "Epoch 753/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0409 - val_loss: 0.0459\n",
      "Epoch 754/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0413 - val_loss: 0.0467\n",
      "Epoch 755/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0383 - val_loss: 0.0467\n",
      "Epoch 756/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0419 - val_loss: 0.0494\n",
      "Epoch 757/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0411 - val_loss: 0.0441\n",
      "Epoch 758/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0420 - val_loss: 0.0499\n",
      "Epoch 759/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0406 - val_loss: 0.0446\n",
      "Epoch 760/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0397 - val_loss: 0.0460\n",
      "Epoch 761/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0398 - val_loss: 0.0483\n",
      "Epoch 762/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0388 - val_loss: 0.0437\n",
      "Epoch 763/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0417 - val_loss: 0.0507\n",
      "Epoch 764/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0420 - val_loss: 0.0453\n",
      "Epoch 765/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0413 - val_loss: 0.0487\n",
      "Epoch 766/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0398 - val_loss: 0.0461\n",
      "Epoch 767/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0398 - val_loss: 0.0463\n",
      "Epoch 768/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0394 - val_loss: 0.0461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 769/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0390 - val_loss: 0.0474\n",
      "Epoch 770/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0416 - val_loss: 0.0524\n",
      "Epoch 771/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0437 - val_loss: 0.0472\n",
      "Epoch 772/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0412 - val_loss: 0.0476\n",
      "Epoch 773/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0410 - val_loss: 0.0472\n",
      "Epoch 774/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0387 - val_loss: 0.0434\n",
      "Epoch 775/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0399 - val_loss: 0.0512\n",
      "Epoch 776/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0401 - val_loss: 0.0442\n",
      "Epoch 777/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0415 - val_loss: 0.0485\n",
      "Epoch 778/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0413 - val_loss: 0.0458\n",
      "Epoch 779/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0391 - val_loss: 0.0439\n",
      "Epoch 780/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0392 - val_loss: 0.0511\n",
      "Epoch 781/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0406 - val_loss: 0.0443\n",
      "Epoch 782/1000\n",
      "3298/3298 [==============================] - 1s 316us/step - loss: 0.0424 - val_loss: 0.0478\n",
      "Epoch 783/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0389 - val_loss: 0.0462\n",
      "Epoch 784/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0391 - val_loss: 0.0438\n",
      "Epoch 785/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0400 - val_loss: 0.0513\n",
      "Epoch 786/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0410 - val_loss: 0.0433\n",
      "Epoch 787/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0393 - val_loss: 0.0468\n",
      "Epoch 788/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0381 - val_loss: 0.0471\n",
      "Epoch 789/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0376 - val_loss: 0.0434\n",
      "Epoch 790/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0375 - val_loss: 0.0509\n",
      "Epoch 791/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0411 - val_loss: 0.0438\n",
      "Epoch 792/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0387 - val_loss: 0.0446\n",
      "Epoch 793/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0387 - val_loss: 0.0453\n",
      "Epoch 794/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0379 - val_loss: 0.0441\n",
      "Epoch 795/1000\n",
      "3298/3298 [==============================] - 1s 315us/step - loss: 0.0389 - val_loss: 0.0471\n",
      "Epoch 796/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0381 - val_loss: 0.0436\n",
      "Epoch 797/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0386 - val_loss: 0.0452\n",
      "Epoch 798/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0368 - val_loss: 0.0480\n",
      "Epoch 799/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0370 - val_loss: 0.0435\n",
      "Epoch 800/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0384 - val_loss: 0.0510\n",
      "Epoch 801/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0396 - val_loss: 0.0438\n",
      "Epoch 802/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0401 - val_loss: 0.0476\n",
      "Epoch 803/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0383 - val_loss: 0.0466\n",
      "Epoch 804/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0393 - val_loss: 0.0458\n",
      "Epoch 805/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0410 - val_loss: 0.0509\n",
      "Epoch 806/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0413 - val_loss: 0.0453\n",
      "Epoch 807/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0381 - val_loss: 0.0442\n",
      "Epoch 808/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0377 - val_loss: 0.0500\n",
      "Epoch 809/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0389 - val_loss: 0.0423\n",
      "Epoch 810/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0403 - val_loss: 0.0486\n",
      "Epoch 811/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0391 - val_loss: 0.0449\n",
      "Epoch 812/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0364 - val_loss: 0.0430\n",
      "Epoch 813/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0399 - val_loss: 0.0516\n",
      "Epoch 814/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0405 - val_loss: 0.0436\n",
      "Epoch 815/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0385 - val_loss: 0.0463\n",
      "Epoch 816/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0377 - val_loss: 0.0480\n",
      "Epoch 817/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0362 - val_loss: 0.0448\n",
      "Epoch 818/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0417 - val_loss: 0.0477\n",
      "Epoch 819/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0370 - val_loss: 0.0452\n",
      "Epoch 820/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0393 - val_loss: 0.0475\n",
      "Epoch 821/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0402 - val_loss: 0.0493\n",
      "Epoch 822/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0408 - val_loss: 0.0476\n",
      "Epoch 823/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0404 - val_loss: 0.0461\n",
      "Epoch 824/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0384 - val_loss: 0.0518\n",
      "Epoch 825/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0400 - val_loss: 0.0430\n",
      "Epoch 826/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0382 - val_loss: 0.0450\n",
      "Epoch 827/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0369 - val_loss: 0.0480\n",
      "Epoch 828/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0393 - val_loss: 0.0440\n",
      "Epoch 829/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0395 - val_loss: 0.0484\n",
      "Epoch 830/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0386 - val_loss: 0.0436\n",
      "Epoch 831/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0364 - val_loss: 0.0459\n",
      "Epoch 832/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0384 - val_loss: 0.0476\n",
      "Epoch 833/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0377 - val_loss: 0.0419\n",
      "Epoch 834/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0379 - val_loss: 0.0491\n",
      "Epoch 835/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0384 - val_loss: 0.0438\n",
      "Epoch 836/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0369 - val_loss: 0.0448\n",
      "Epoch 837/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0365 - val_loss: 0.0485\n",
      "Epoch 838/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0377 - val_loss: 0.0427\n",
      "Epoch 839/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0373 - val_loss: 0.0464\n",
      "Epoch 840/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0374 - val_loss: 0.0454\n",
      "Epoch 841/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0364 - val_loss: 0.0433\n",
      "Epoch 842/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0374 - val_loss: 0.0490\n",
      "Epoch 843/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 844/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0354 - val_loss: 0.0461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 845/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0367 - val_loss: 0.0455\n",
      "Epoch 846/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0369 - val_loss: 0.0432\n",
      "Epoch 847/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0391 - val_loss: 0.0514\n",
      "Epoch 848/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0404 - val_loss: 0.0445\n",
      "Epoch 849/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0367 - val_loss: 0.0442\n",
      "Epoch 850/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0382 - val_loss: 0.0499\n",
      "Epoch 851/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0385 - val_loss: 0.0422\n",
      "Epoch 852/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0381 - val_loss: 0.0465\n",
      "Epoch 853/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0367 - val_loss: 0.0443\n",
      "Epoch 854/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0368 - val_loss: 0.0465\n",
      "Epoch 855/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0401 - val_loss: 0.0478\n",
      "Epoch 856/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0389 - val_loss: 0.0448\n",
      "Epoch 857/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0353 - val_loss: 0.0420\n",
      "Epoch 858/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0363 - val_loss: 0.0485\n",
      "Epoch 859/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0384 - val_loss: 0.0474\n",
      "Epoch 860/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0387 - val_loss: 0.0456\n",
      "Epoch 861/1000\n",
      "3298/3298 [==============================] - 1s 308us/step - loss: 0.0411 - val_loss: 0.0501\n",
      "Epoch 862/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0392 - val_loss: 0.0426\n",
      "Epoch 863/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0356 - val_loss: 0.0444\n",
      "Epoch 864/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0388 - val_loss: 0.0511\n",
      "Epoch 865/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0413 - val_loss: 0.0474\n",
      "Epoch 866/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0381 - val_loss: 0.0419\n",
      "Epoch 867/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0383 - val_loss: 0.0500\n",
      "Epoch 868/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0395 - val_loss: 0.0458\n",
      "Epoch 869/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0375 - val_loss: 0.0467\n",
      "Epoch 870/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0415 - val_loss: 0.0449\n",
      "Epoch 871/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0364 - val_loss: 0.0461\n",
      "Epoch 872/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0377 - val_loss: 0.0467\n",
      "Epoch 873/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0404 - val_loss: 0.0472\n",
      "Epoch 874/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0384 - val_loss: 0.0477\n",
      "Epoch 875/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0372 - val_loss: 0.0428\n",
      "Epoch 876/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0378 - val_loss: 0.0464\n",
      "Epoch 877/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0369 - val_loss: 0.0458\n",
      "Epoch 878/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0372 - val_loss: 0.0425\n",
      "Epoch 879/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0384 - val_loss: 0.0491\n",
      "Epoch 880/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0364 - val_loss: 0.0440\n",
      "Epoch 881/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0371 - val_loss: 0.0434\n",
      "Epoch 882/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0375 - val_loss: 0.0492\n",
      "Epoch 883/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0385 - val_loss: 0.0433\n",
      "Epoch 884/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0359 - val_loss: 0.0424\n",
      "Epoch 885/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0354 - val_loss: 0.0507\n",
      "Epoch 886/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0396 - val_loss: 0.0445\n",
      "Epoch 887/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0379 - val_loss: 0.0423\n",
      "Epoch 888/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0357 - val_loss: 0.0489\n",
      "Epoch 889/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0368 - val_loss: 0.0412\n",
      "Epoch 890/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0359 - val_loss: 0.0445\n",
      "Epoch 891/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0359 - val_loss: 0.0472\n",
      "Epoch 892/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0365 - val_loss: 0.0425\n",
      "Epoch 893/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0355 - val_loss: 0.0441\n",
      "Epoch 894/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0346 - val_loss: 0.0452\n",
      "Epoch 895/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0351 - val_loss: 0.0439\n",
      "Epoch 896/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0373 - val_loss: 0.0433\n",
      "Epoch 897/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0345 - val_loss: 0.0463\n",
      "Epoch 898/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0336 - val_loss: 0.0413\n",
      "Epoch 899/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0376 - val_loss: 0.0465\n",
      "Epoch 900/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0349 - val_loss: 0.0418\n",
      "Epoch 901/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0350 - val_loss: 0.0428\n",
      "Epoch 902/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0339 - val_loss: 0.0436\n",
      "Epoch 903/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0338 - val_loss: 0.0415\n",
      "Epoch 904/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0349 - val_loss: 0.0457\n",
      "Epoch 905/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0334 - val_loss: 0.0418\n",
      "Epoch 906/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0341 - val_loss: 0.0436\n",
      "Epoch 907/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0324 - val_loss: 0.0427\n",
      "Epoch 908/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0344 - val_loss: 0.0444\n",
      "Epoch 909/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0358 - val_loss: 0.0451\n",
      "Epoch 910/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0367 - val_loss: 0.0475\n",
      "Epoch 911/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0365 - val_loss: 0.0414\n",
      "Epoch 912/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0337 - val_loss: 0.0445\n",
      "Epoch 913/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0334 - val_loss: 0.0433\n",
      "Epoch 914/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0344 - val_loss: 0.0435\n",
      "Epoch 915/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0361 - val_loss: 0.0474\n",
      "Epoch 916/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0348 - val_loss: 0.0413\n",
      "Epoch 917/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0353 - val_loss: 0.0450\n",
      "Epoch 918/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0347 - val_loss: 0.0449\n",
      "Epoch 919/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0355 - val_loss: 0.0438\n",
      "Epoch 920/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0369 - val_loss: 0.0463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 921/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0359 - val_loss: 0.0410\n",
      "Epoch 922/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0343 - val_loss: 0.0440\n",
      "Epoch 923/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0351 - val_loss: 0.0452\n",
      "Epoch 924/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0350 - val_loss: 0.0412\n",
      "Epoch 925/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0355 - val_loss: 0.0477\n",
      "Epoch 926/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0346 - val_loss: 0.0415\n",
      "Epoch 927/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0345 - val_loss: 0.0427\n",
      "Epoch 928/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0331 - val_loss: 0.0450\n",
      "Epoch 929/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0344 - val_loss: 0.0408\n",
      "Epoch 930/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0360 - val_loss: 0.0456\n",
      "Epoch 931/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0352 - val_loss: 0.0422\n",
      "Epoch 932/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0334 - val_loss: 0.0411\n",
      "Epoch 933/1000\n",
      "3298/3298 [==============================] - 1s 317us/step - loss: 0.0342 - val_loss: 0.0470\n",
      "Epoch 934/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0344 - val_loss: 0.0408\n",
      "Epoch 935/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0332 - val_loss: 0.0433\n",
      "Epoch 936/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0340 - val_loss: 0.0436\n",
      "Epoch 937/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0347 - val_loss: 0.0453\n",
      "Epoch 938/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0362 - val_loss: 0.0438\n",
      "Epoch 939/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0354 - val_loss: 0.0451\n",
      "Epoch 940/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0335 - val_loss: 0.0419\n",
      "Epoch 941/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0319 - val_loss: 0.0421\n",
      "Epoch 942/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0321 - val_loss: 0.0433\n",
      "Epoch 943/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0325 - val_loss: 0.0419\n",
      "Epoch 944/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0334 - val_loss: 0.0457\n",
      "Epoch 945/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0340 - val_loss: 0.0439\n",
      "Epoch 946/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0347 - val_loss: 0.0423\n",
      "Epoch 947/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0327 - val_loss: 0.0423\n",
      "Epoch 948/1000\n",
      "3298/3298 [==============================] - 1s 314us/step - loss: 0.0326 - val_loss: 0.0417\n",
      "Epoch 949/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0324 - val_loss: 0.0455\n",
      "Epoch 950/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0358 - val_loss: 0.0448\n",
      "Epoch 951/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0368 - val_loss: 0.0449\n",
      "Epoch 952/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0355 - val_loss: 0.0451\n",
      "Epoch 953/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0334 - val_loss: 0.0400\n",
      "Epoch 954/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0337 - val_loss: 0.0464\n",
      "Epoch 955/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0342 - val_loss: 0.0442\n",
      "Epoch 956/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0351 - val_loss: 0.0428\n",
      "Epoch 957/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0332 - val_loss: 0.0445\n",
      "Epoch 958/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0328 - val_loss: 0.0405\n",
      "Epoch 959/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0335 - val_loss: 0.0441\n",
      "Epoch 960/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0333 - val_loss: 0.0417\n",
      "Epoch 961/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0330 - val_loss: 0.0432\n",
      "Epoch 962/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0335 - val_loss: 0.0420\n",
      "Epoch 963/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0350 - val_loss: 0.0441\n",
      "Epoch 964/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0339 - val_loss: 0.0407\n",
      "Epoch 965/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0325 - val_loss: 0.0409\n",
      "Epoch 966/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0307 - val_loss: 0.0425\n",
      "Epoch 967/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0310 - val_loss: 0.0407\n",
      "Epoch 968/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0318 - val_loss: 0.0442\n",
      "Epoch 969/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0336 - val_loss: 0.0411\n",
      "Epoch 970/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0328 - val_loss: 0.0437\n",
      "Epoch 971/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0333 - val_loss: 0.0412\n",
      "Epoch 972/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0310 - val_loss: 0.0427\n",
      "Epoch 973/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0323 - val_loss: 0.0419\n",
      "Epoch 974/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0316 - val_loss: 0.0418\n",
      "Epoch 975/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0326 - val_loss: 0.0425\n",
      "Epoch 976/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0335 - val_loss: 0.0422\n",
      "Epoch 977/1000\n",
      "3298/3298 [==============================] - 1s 309us/step - loss: 0.0311 - val_loss: 0.0433\n",
      "Epoch 978/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0316 - val_loss: 0.0411\n",
      "Epoch 979/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0320 - val_loss: 0.0430\n",
      "Epoch 980/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0316 - val_loss: 0.0411\n",
      "Epoch 981/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0322 - val_loss: 0.0455\n",
      "Epoch 982/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0336 - val_loss: 0.0413\n",
      "Epoch 983/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0345 - val_loss: 0.0452\n",
      "Epoch 984/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0344 - val_loss: 0.0407\n",
      "Epoch 985/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0332 - val_loss: 0.0424\n",
      "Epoch 986/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0316 - val_loss: 0.0435\n",
      "Epoch 987/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0318 - val_loss: 0.0401\n",
      "Epoch 988/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0324 - val_loss: 0.0461\n",
      "Epoch 989/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0332 - val_loss: 0.0408\n",
      "Epoch 990/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0327 - val_loss: 0.0435\n",
      "Epoch 991/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0322 - val_loss: 0.0407\n",
      "Epoch 992/1000\n",
      "3298/3298 [==============================] - 1s 313us/step - loss: 0.0316 - val_loss: 0.0428\n",
      "Epoch 993/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0324 - val_loss: 0.0423\n",
      "Epoch 994/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0326 - val_loss: 0.0428\n",
      "Epoch 995/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0334 - val_loss: 0.0432\n",
      "Epoch 996/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0326 - val_loss: 0.0417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 997/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0320 - val_loss: 0.0420\n",
      "Epoch 998/1000\n",
      "3298/3298 [==============================] - 1s 310us/step - loss: 0.0313 - val_loss: 0.0421\n",
      "Epoch 999/1000\n",
      "3298/3298 [==============================] - 1s 311us/step - loss: 0.0310 - val_loss: 0.0402\n",
      "Epoch 1000/1000\n",
      "3298/3298 [==============================] - 1s 312us/step - loss: 0.0305 - val_loss: 0.0429\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import RepeatVector\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Create our cross validation data structure\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(processed_data['x'], processed_data['y'],\n",
    "                                                                    test_size = 0.2)\n",
    "\n",
    "# Train for wind intensity\n",
    "y_train_wind = np.array([[[features[2]] for features in y] for y in y_train], dtype = np.float64)\n",
    "y_test_wind = np.array([[[features[2]] for features in y] for y in y_test], dtype = np.float64)\n",
    "\n",
    "# Train for latitude and longitude location\n",
    "y_train_lat = np.array([[[features[0]] for features in y] for y in y_train], dtype = np.float64)\n",
    "y_test_lat = np.array([[[features[0]] for features in y] for y in y_test], dtype = np.float64)\n",
    "y_train_long = np.array([[[features[1]] for features in y] for y in y_train], dtype = np.float64)\n",
    "y_test_long = np.array([[[features[1]] for features in y] for y in y_test], dtype = np.float64)\n",
    "\n",
    "\n",
    "def bd_lstm_td(X_train, y_train, X_test, y_test, n_epochs = 500) :    \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units = 1024, return_sequences = True, dropout = 0.05),\n",
    "                            input_shape = (X_train.shape[1],X_train.shape[2])))\n",
    "    model.add(LSTM(units = 512, return_sequences = True, dropout = 0.05))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, y_train, batch_size = len(X_train), epochs = n_epochs,\n",
    "                        validation_data = (X_test, y_test))\n",
    "    return model, history\n",
    "\n",
    "def lstm_td(X_train, X_test, y_train, y_test) :\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = 1024, input_shape = (5,8), return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(8)))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    model.fit(X_train, y_train, batch_size = len(X_train), epochs = 300)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_wind, model_wind_history = bd_lstm_td(X_train, y_train_wind, X_test, y_test_wind)\n",
    "model_lat, model_lat_history = bd_lstm_td(X_train, y_train_lat, X_test, y_test_lat, n_epochs = 1000)\n",
    "model_long, model_long_history = bd_lstm_td(X_train, y_train_long, X_test, y_test_long, n_epochs = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection<a id=\"Selection\"></a>\n",
    "The following models were compared\n",
    "- Bidirectional LSTM with Time Distributed(Best performance)\n",
    "- LSTM with Time Distributed\n",
    "- MLP\n",
    "- Bidirectional GRU with Time Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e7fe0da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e7fe446d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.378664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.050457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-96.378191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.270789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.296344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.697134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>91.645017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  4125.000000\n",
       "mean     -0.378664\n",
       "std       8.050457\n",
       "min     -96.378191\n",
       "25%      -3.270789\n",
       "50%      -0.296344\n",
       "75%       2.697134\n",
       "max      91.645017"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ai_errors(predictions, observations, history = None) :\n",
    "    '''\n",
    "    PURPOSE: Provide descriptive statistics on the predicted output versus the observed measurments\n",
    "    METHOD:  Take the errors of the predictions and answers and then calculate standard descriptive statistics\n",
    "    INPUT:   predictions - 2D array of predictions of observed output\n",
    "             observations - 2D array measurements of observed output\n",
    "             history - Keras history model for displaying model loss, default is None if not available\n",
    "    OUTPUT:\n",
    "    '''\n",
    "    errors = []\n",
    "    for i in range(len(predictions)) :\n",
    "        for j in range(len(predictions[i])) :\n",
    "            # Calculate errors\n",
    "            error = predictions[i][j] - observations[i][j]\n",
    "            errors.append(error)\n",
    "    \n",
    "    # Display history and erros\n",
    "    plt.figure(1)\n",
    "    plt.hist(errors, bins = 50)\n",
    "    plt.title('error histogram')\n",
    "    plt.xlabel('error')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    return pd.DataFrame(errors)\n",
    "\n",
    "# Predict values\n",
    "wind_predictions = model_wind.predict(X_test)\n",
    "lat_predictions = model_lat.predict(X_test)\n",
    "long_predictions = model_long.predict(X_test)\n",
    "\n",
    "# Scale back our predictions\n",
    "# Wind\n",
    "wind_predictions_scaled = [scaler.inverse_transform([[0,0,winds[0],0,0,0,0,0,0,0,0] for winds in prediction])\n",
    "                           for prediction in wind_predictions]\n",
    "y_wind_test_scaled = [scaler.inverse_transform([[0,0,winds[0],0,0,0,0,0,0,0,0] for winds in observation])\n",
    "                      for observation in y_test_wind]\n",
    "# Latitude\n",
    "lat_predictions_scaled = [scaler.inverse_transform([[lat[0],0,0,0,0,0,0,0,0,0,0] for lat in prediction])\n",
    "                          for prediction in lat_predictions]\n",
    "y_lat_test_scaled = [scaler.inverse_transform([[lat[0],0,0,0,0,0,0,0,0,0,0] for lat in observation])\n",
    "                     for observation in y_test_lat]\n",
    "# Longitude\n",
    "long_predictions_scaled = [scaler.inverse_transform([[0,long[0],0,0,0,0,0,0,0,0,0] for long in prediction])\n",
    "                           for prediction in long_predictions]\n",
    "y_long_test_scaled = [scaler.inverse_transform([[0,long[0],0,0,0,0,0,0,0,0,0] for long in observation])\n",
    "                      for observation in y_test_long]\n",
    "\n",
    "# Record wind predictions and observations\n",
    "print(\"Wind\")\n",
    "wind_predictions = [[pred[2] for pred in hurricanes_pred] for hurricanes_pred in wind_predictions_scaled]\n",
    "wind_observations = [[obsrv[2] for obsrv in hurricanes_obsrv] for hurricanes_obsrv in y_wind_test_scaled]\n",
    "\n",
    "# Present Errors\n",
    "ai_errors(wind_predictions, wind_observations, model_wind_history).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lat\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e8930d828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e893d86a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.019501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.351134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-20.295284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.325692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.053342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.437728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.411461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  4125.000000\n",
       "mean      0.019501\n",
       "std       1.351134\n",
       "min     -20.295284\n",
       "25%      -0.325692\n",
       "50%       0.053342\n",
       "75%       0.437728\n",
       "max      12.411461"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Lat\")\n",
    "lat_predictions = [[pred[0] for pred in hurricanes_pred] for hurricanes_pred in lat_predictions_scaled]\n",
    "lat_observations = [[obsrv[0] for obsrv in hurricanes_obsrv] for hurricanes_obsrv in y_lat_test_scaled]\n",
    "ai_errors(lat_predictions, lat_observations, model_lat_history).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e7fbce748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e7fbff358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.135158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.300875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-30.939977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.659186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.123691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.839209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42.489526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  4125.000000\n",
       "mean      0.135158\n",
       "std       2.300875\n",
       "min     -30.939977\n",
       "25%      -0.659186\n",
       "50%       0.123691\n",
       "75%       0.839209\n",
       "max      42.489526"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Long\")\n",
    "long_predictions = [[pred[1] for pred in hurricanes_pred] for hurricanes_pred in long_predictions_scaled]\n",
    "long_observations = [[obsrv[1] for obsrv in hurricanes_obsrv] for hurricanes_obsrv in y_long_test_scaled]\n",
    "ai_errors(long_predictions, long_observations, model_long_history).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation & Benchmarks<a id=\"Benchmarks\"></a>\n",
    "\n",
    "The machine learning neural network will use 2 main methods of applied evaluation. The first will be evaluated compared to the other models that predict Atlantic hurricanes. The forecast errors have been loaded into each hurricane object corresponding to their forecast model; both the OFCL (official track, or the truth) and the BCD5 (model using multivariate regression). The BCD5 model is \"the CLP5 (track) and DSF5 (intensity) models merged\" that uses the best track as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
